{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[KU]BERT 한국어 데이터_수정본.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c7b0c1f057748e9b049f3ecd30d0d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_75440f5aaee44efe93fdc440e99d491d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5ee4f783536940f0a6fb3112a623506a",
              "IPY_MODEL_0f12fdfa296b49dfb0b017465a3df458"
            ]
          }
        },
        "75440f5aaee44efe93fdc440e99d491d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ee4f783536940f0a6fb3112a623506a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0cfca4089d67462ca36aee009cd1cc08",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_120b18c71aad4eb69c6d361a558cdc14"
          }
        },
        "0f12fdfa296b49dfb0b017465a3df458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_49a5bcd79bc141c9ab67715ab4ddd398",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 996k/996k [00:00&lt;00:00, 5.32MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_42853a8feb294303b913821a951184fd"
          }
        },
        "0cfca4089d67462ca36aee009cd1cc08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "120b18c71aad4eb69c6d361a558cdc14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49a5bcd79bc141c9ab67715ab4ddd398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "42853a8feb294303b913821a951184fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80514765c70e4b40bc7b630498aa429f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_80e16dae3b014cc58cb01ef1fdd8453e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b86678f9783b4144b3c3c2a817c217c1",
              "IPY_MODEL_efbe1b8ad9bc4b59a435acc325133d01"
            ]
          }
        },
        "80e16dae3b014cc58cb01ef1fdd8453e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b86678f9783b4144b3c3c2a817c217c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0bc13fb4c4084447a1a50b13c64c66a4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f375bef452b4c39b5c8bae3af9f7c10"
          }
        },
        "efbe1b8ad9bc4b59a435acc325133d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_38d6b5d77d5f40a186339a303cc80f82",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 625/625 [00:00&lt;00:00, 719B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45140abd9ec44d9180f94464de55dce6"
          }
        },
        "0bc13fb4c4084447a1a50b13c64c66a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f375bef452b4c39b5c8bae3af9f7c10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38d6b5d77d5f40a186339a303cc80f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "45140abd9ec44d9180f94464de55dce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a1b651cadae4a9d9f4b2a0c673b5f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0af7e4caaa8e4e7db4dc0d9aad3e82a4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6a80b91d0c1d43e082bf54fe43c7c86d",
              "IPY_MODEL_798b8edab0394c78922bb00b44f055fd"
            ]
          }
        },
        "0af7e4caaa8e4e7db4dc0d9aad3e82a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a80b91d0c1d43e082bf54fe43c7c86d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1242ac84dc8944538f9756fd995618c5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70275676b951487699514335c5c2ec29"
          }
        },
        "798b8edab0394c78922bb00b44f055fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_727e5cb85f2543faa024d5ebdf60fcb0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 714M/714M [00:14&lt;00:00, 48.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c50509416c17403497b73002b56a8667"
          }
        },
        "1242ac84dc8944538f9756fd995618c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70275676b951487699514335c5c2ec29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "727e5cb85f2543faa024d5ebdf60fcb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c50509416c17403497b73002b56a8667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeongyeon-Myeong/Python-Project/blob/master/BERT_%ED%95%9C%EA%B5%AD%EC%96%B4_Naver%20Movie_%EB%8D%B0%EC%9D%B4%ED%84%B0_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfPYyPLERU-H"
      },
      "source": [
        "# **Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT4PIM2xRgBp"
      },
      "source": [
        "- Edit > Notebook settings > Hardward accelerators > GPU > SAVE\n",
        "- Download the Friends dataset in EmotionLines website:\n",
        "http://doraemon.iis.sinica.edu.tw/emotionlines/download.html\n",
        "- Download the unlabeled json file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idM2mzIOxygV"
      },
      "source": [
        "# **Tutorials**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NWok7mf5xZk"
      },
      "source": [
        "##### **Settings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZewOX-D9BLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ce21b9-1314-464b-fbdd-8eaefef7bea9"
      },
      "source": [
        "!pip install transformers --quiet # package installer for python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5MB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 40.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 41.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6idZebToz_Wa"
      },
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKYIuzDw6mCq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167,
          "referenced_widgets": [
            "7c7b0c1f057748e9b049f3ecd30d0d82",
            "75440f5aaee44efe93fdc440e99d491d",
            "5ee4f783536940f0a6fb3112a623506a",
            "0f12fdfa296b49dfb0b017465a3df458",
            "0cfca4089d67462ca36aee009cd1cc08",
            "120b18c71aad4eb69c6d361a558cdc14",
            "49a5bcd79bc141c9ab67715ab4ddd398",
            "42853a8feb294303b913821a951184fd",
            "80514765c70e4b40bc7b630498aa429f",
            "80e16dae3b014cc58cb01ef1fdd8453e",
            "b86678f9783b4144b3c3c2a817c217c1",
            "efbe1b8ad9bc4b59a435acc325133d01",
            "0bc13fb4c4084447a1a50b13c64c66a4",
            "6f375bef452b4c39b5c8bae3af9f7c10",
            "38d6b5d77d5f40a186339a303cc80f82",
            "45140abd9ec44d9180f94464de55dce6",
            "5a1b651cadae4a9d9f4b2a0c673b5f62",
            "0af7e4caaa8e4e7db4dc0d9aad3e82a4",
            "6a80b91d0c1d43e082bf54fe43c7c86d",
            "798b8edab0394c78922bb00b44f055fd",
            "1242ac84dc8944538f9756fd995618c5",
            "70275676b951487699514335c5c2ec29",
            "727e5cb85f2543faa024d5ebdf60fcb0",
            "c50509416c17403497b73002b56a8667"
          ]
        },
        "outputId": "a1c9caa1-b4d2-4db5-dbe5-3e646f1d4db6"
      },
      "source": [
        "pretrained_weights = 'bert-base-multilingual-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(pretrained_weights) #bert tokenizer\n",
        "model = BertModel.from_pretrained(pretrained_weights) # bert model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c7b0c1f057748e9b049f3ecd30d0d82",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80514765c70e4b40bc7b630498aa429f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a1b651cadae4a9d9f4b2a0c673b5f62",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCpSWlYV7ja0"
      },
      "source": [
        "##### **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shaQM8GE9RIR"
      },
      "source": [
        "#예시 문장\n",
        "sentence = '완전 감동적인 영화네'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAxXbZXW3ueJ",
        "outputId": "81a022e8-3251-4131-b6bf-aeb2598b5205"
      },
      "source": [
        " #bert tokenizer로 토큰화\n",
        "tokens = tokenizer.tokenize(sentence) #['[CLS]', 'all', 'the', 'classes', 'are', 'provided', '.', '[SEP]']\n",
        "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', '완', '##전', '감', '##동', '##적인', '영화', '##네', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wcGm1oJ4Df7",
        "outputId": "548b50cd-0468-4a41-d68f-7bf4a09fc7c6"
      },
      "source": [
        "# token하여 id로 바꾼다.[[101, 2035, 1996, 4280, 2024, 3024, 1012, 102]]\n",
        "#Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the vocabulary.\n",
        "ids = [tokenizer.convert_tokens_to_ids(tokens)]\n",
        "print(ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[101, 9591, 16617, 8848, 18778, 15387, 42428, 77884, 102]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGVa_OCH-31F"
      },
      "source": [
        "#tensor로 변경, tensor([[ 101, 2035, 1996, 4280, 2024, 3024, 1012,  102]])\n",
        "input_tensor = torch.tensor(ids)\n",
        "#masks_tensor = torch.tensor(attention_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThFxkpIMZUz3",
        "outputId": "f88193de-3608-4937-eb14-64c86cddd504"
      },
      "source": [
        "print(masks_tensor.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 19])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOLOtWOatAH9"
      },
      "source": [
        "# 추가 고려사항!! \r\n",
        "# 입력토큰의 최대 시퀀스 길이에 대해 max length를 구하고 모자른 부분을 0으로 채워준다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns0LjLOd7Ry5"
      },
      "source": [
        "##### **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBGZFEn08vbW",
        "outputId": "b0835b72-206e-4e09-b324-4ce1732635b3"
      },
      "source": [
        "# id값을 bert모델에 넣기\n",
        "hidden_tensor = model(input_tensor, attention_mask = masks_tensor)[0]\n",
        "print(hidden_tensor.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 19, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrYgst9d1AJA",
        "outputId": "797b8b26-88f7-4de6-8273-129ab9f3ca6b"
      },
      "source": [
        "hidden_tensor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3332,  0.3654,  0.1161,  ..., -0.1596,  0.2420,  0.5352],\n",
              "         [ 0.5816,  0.3318,  0.3934,  ...,  0.2164,  0.3406, -0.2485],\n",
              "         [-0.0190,  0.4895,  0.4740,  ..., -0.3352,  0.3113,  0.3196],\n",
              "         ...,\n",
              "         [ 0.1850,  0.4164,  0.4646,  ...,  0.0479, -0.0294, -0.1171],\n",
              "         [ 0.3209,  0.3451,  0.4535,  ...,  0.0429, -0.0533, -0.0560],\n",
              "         [ 0.3667,  0.3243,  0.4432,  ...,  0.0751, -0.0630, -0.0889]]],\n",
              "       grad_fn=<NativeLayerNormBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3yB6A3dCN6T",
        "outputId": "d405c616-b923-4c2d-fc75-8fa38575931c"
      },
      "source": [
        "#torch.Size([1, 8, 768])\n",
        "logit = torch.nn.Linear(768, 2)(hidden_tensor)\n",
        "print(logit.size())\n",
        "print(logit.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 19, 2])\n",
            "tensor([[[-0.0394, -0.6272],\n",
            "         [ 0.2332, -0.5167],\n",
            "         [ 0.0976,  0.0890],\n",
            "         [-0.2424, -0.4108],\n",
            "         [-0.0234, -0.1241],\n",
            "         [-0.0410, -0.3244],\n",
            "         [ 0.0504, -0.0833],\n",
            "         [-0.0383,  0.0512],\n",
            "         [ 0.2636, -0.4400],\n",
            "         [ 0.4942, -0.0908],\n",
            "         [ 0.3496, -0.3235],\n",
            "         [ 0.5387, -0.3592],\n",
            "         [ 0.2215, -0.2635],\n",
            "         [ 0.6073, -0.4124],\n",
            "         [ 0.4209, -0.2905],\n",
            "         [ 0.1868, -0.2592],\n",
            "         [ 0.2577, -0.2580],\n",
            "         [ 0.1927, -0.2361],\n",
            "         [ 0.1884, -0.2572]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd5X-Id3DERC",
        "outputId": "b4f56a87-4fab-49c4-c735-053e14962382"
      },
      "source": [
        "prediction = torch.nn.Softmax(dim=-1)(logit)\n",
        "print(prediction.size())\n",
        "print(prediction.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 19, 2])\n",
            "tensor([[[0.6429, 0.3571],\n",
            "         [0.6792, 0.3208],\n",
            "         [0.5021, 0.4979],\n",
            "         [0.5420, 0.4580],\n",
            "         [0.5252, 0.4748],\n",
            "         [0.5704, 0.4296],\n",
            "         [0.5334, 0.4666],\n",
            "         [0.4777, 0.5223],\n",
            "         [0.6690, 0.3310],\n",
            "         [0.6422, 0.3578],\n",
            "         [0.6622, 0.3378],\n",
            "         [0.7105, 0.2895],\n",
            "         [0.6189, 0.3811],\n",
            "         [0.7349, 0.2651],\n",
            "         [0.6707, 0.3293],\n",
            "         [0.6097, 0.3903],\n",
            "         [0.6262, 0.3738],\n",
            "         [0.6056, 0.3944],\n",
            "         [0.6096, 0.3904]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPKnl3-7yCnJ"
      },
      "source": [
        "# **Emotion Recognition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk8Fa9xrzndp"
      },
      "source": [
        "**Train and Dev Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFSp-U4HqheI",
        "outputId": "97f1b74b-be14-4a3b-dfd6-61f8c89f83e1"
      },
      "source": [
        "# 네이버 영화리뷰 감정분석 데이터 다운로드\r\n",
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nsmc'...\n",
            "remote: Enumerating objects: 14763, done.\u001b[K\n",
            "remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n",
            "Receiving objects: 100% (14763/14763), 56.19 MiB | 22.63 MiB/s, done.\n",
            "Resolving deltas: 100% (1749/1749), done.\n",
            "Checking out files: 100% (14737/14737), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFhz5l0qmqB",
        "outputId": "a7e49fc3-ad49-4e53-da4e-957bc9d8d32a"
      },
      "source": [
        "# 디렉토리의 파일 목록\r\n",
        "!ls nsmc -la"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 38628\n",
            "drwxr-xr-x 5 root root     4096 Dec 23 12:43 .\n",
            "drwxr-xr-x 1 root root     4096 Dec 23 12:43 ..\n",
            "drwxr-xr-x 2 root root     4096 Dec 23 12:43 code\n",
            "drwxr-xr-x 8 root root     4096 Dec 23 12:43 .git\n",
            "-rw-r--r-- 1 root root  4893335 Dec 23 12:43 ratings_test.txt\n",
            "-rw-r--r-- 1 root root 14628807 Dec 23 12:43 ratings_train.txt\n",
            "-rw-r--r-- 1 root root 19515078 Dec 23 12:43 ratings.txt\n",
            "drwxr-xr-x 2 root root   450560 Dec 23 12:43 raw\n",
            "-rw-r--r-- 1 root root     2596 Dec 23 12:43 README.md\n",
            "-rw-r--r-- 1 root root    36746 Dec 23 12:43 synopses.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j24kf78qpcB",
        "outputId": "f920c0e6-166c-4037-b5a2-85f16e60b3e7"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "# 판다스로 훈련셋과 테스트셋 데이터 로드\r\n",
        "train = pd.read_csv(\"nsmc/ratings_train.txt\", sep='\\t')\r\n",
        "dev = pd.read_csv(\"nsmc/ratings_test.txt\", sep='\\t') # test data를 dev dataset으로 사용\r\n",
        "\r\n",
        "print(train.shape)\r\n",
        "print(dev.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150000, 3)\n",
            "(50000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "AgCGDWK6qy-_",
        "outputId": "28a31bf1-8637-402b-cd07-f97fab6d865b"
      },
      "source": [
        "# 훈련셋의 앞부분 출력\r\n",
        "dev.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6270596</td>\n",
              "      <td>굳 ㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9274899</td>\n",
              "      <td>GDNTOPCLASSINTHECLUB</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8544678</td>\n",
              "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6825595</td>\n",
              "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6723715</td>\n",
              "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7898805</td>\n",
              "      <td>음악이 주가 된, 최고의 음악영화</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6315043</td>\n",
              "      <td>진정한 쓰레기</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6097171</td>\n",
              "      <td>마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8932678</td>\n",
              "      <td>갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6242223</td>\n",
              "      <td>이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 But, 모든 사람이 그렇지는 않네..</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                           document  label\n",
              "0  6270596                                                굳 ㅋ      1\n",
              "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
              "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
              "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
              "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0\n",
              "5  7898805                                 음악이 주가 된, 최고의 음악영화      1\n",
              "6  6315043                                            진정한 쓰레기      0\n",
              "7  6097171           마치 미국애니에서 튀어나온듯한 창의력없는 로봇디자인부터가,고개를 젖게한다      0\n",
              "8  8932678  갈수록 개판되가는 중국영화 유치하고 내용없음 폼잡다 끝남 말도안되는 무기에 유치한c...      0\n",
              "9  6242223     이별의 아픔뒤에 찾아오는 새로운 인연의 기쁨 But, 모든 사람이 그렇지는 않네..      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0EbJ5v2lnmH"
      },
      "source": [
        "# dataframe을 list형태로 변경함.\r\n",
        "train_utterance = train['document'].values.tolist()\r\n",
        "dev_utterance = dev['document'].values.tolist()\r\n",
        "train_emotion = train['label'].values.tolist()\r\n",
        "dev_emotion = dev['label'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDf31ERrZiFW",
        "outputId": "2a12e0e8-3382-4feb-87d4-a9727ef487d3"
      },
      "source": [
        " train['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    75173\n",
              "1    74827\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VX25EphOBZu"
      },
      "source": [
        "def token_input(utterance):\r\n",
        "  for i in range(len(utterance)):\r\n",
        "      utterance[i] = '[CLS] ' + str(utterance[i]) + ' [SEP] [PAD]' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3oT6jDnZTvY"
      },
      "source": [
        "token_input(train_utterance)\r\n",
        "token_input(dev_utterance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n0Q-qv3KYN4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1c84795f-9f2e-464f-abb1-5c13068b29fc"
      },
      "source": [
        "# train과 dev dataset \r\n",
        "\"\"\"\r\n",
        "for i in range(len(train_utterance)):\r\n",
        "   train_utterance[i] = train_speaker[i] + ', ' + train_utterance[i]\r\n",
        "for i in range(len(dev_utterance)):\r\n",
        "   dev_utterance[i] = dev_speaker[i] + ', ' + dev_utterance[i]   \r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor i in range(len(train_utterance)):\\n   train_utterance[i] = train_speaker[i] + ', ' + train_utterance[i]\\nfor i in range(len(dev_utterance)):\\n   dev_utterance[i] = dev_speaker[i] + ', ' + dev_utterance[i]   \\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "8tFzFPAekFfr",
        "outputId": "a0b38feb-16c3-4fd4-e2e5-72359371d1ab"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in train_utterance))\r\n",
        "print('리뷰의 평균 길이 :',sum(map(len, train_utterance))/len(train_utterance))\r\n",
        "plt.hist([len(s) for s in train_utterance], bins=50)\r\n",
        "plt.xlabel('length of samples')\r\n",
        "plt.ylabel('number of samples')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "리뷰의 최대 길이 : 164\n",
            "리뷰의 평균 길이 : 53.203453333333336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siNMXKsWl4sT"
      },
      "source": [
        "# padding의 max length 설정\r\n",
        "MAX_LEN = 160"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIJAheccwqUJ"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from transformers import BertModel, BertTokenizer\r\n",
        "def tokenize(dataset):\r\n",
        "  # 입력 토큰의 최대 시퀀스 길이\r\n",
        "  tokenized_utterance = [ str(sentence) for sentence in dataset ]\r\n",
        "  # token화 하기\r\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\r\n",
        "  tokenized_utterance = [tokenizer.tokenize(sent) for sent in tokenized_utterance]\r\n",
        "  # 인덱스로 변환하기 \r\n",
        "  ids_utterance = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_utterance] \r\n",
        "  # 패딩 채우기\r\n",
        "  ids_utterance = pad_sequences(ids_utterance, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\r\n",
        "  return ids_utterance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ7FzYQkEj9x"
      },
      "source": [
        "token_train_utterance = tokenize(train_utterance)\r\n",
        "token_dev_utterance = tokenize(dev_utterance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl_5Uqhfi09U"
      },
      "source": [
        "# 어텐션 마스크 \r\n",
        "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\r\n",
        "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\r\n",
        "def mask_func(masklist, targetdata):\r\n",
        "  for seq in targetdata:\r\n",
        "    seq_mask = [float(i>0) for i in seq]\r\n",
        "    masklist.append(seq_mask)\r\n",
        "  print(masklist[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poavcWuFAR1z",
        "outputId": "90447266-7930-4a72-916f-a17f46934266"
      },
      "source": [
        "# train과 dev data\r\n",
        "masks_train_utterance = []\r\n",
        "masks_dev_utterance = []\r\n",
        "mask_func(masks_train_utterance, token_train_utterance) \r\n",
        "mask_func(masks_dev_utterance, token_dev_utterance)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06f_kyszEpdM"
      },
      "source": [
        "\"\"\"\r\n",
        "import pandas as pd\r\n",
        "df = pd.DataFrame(friendsdata_train_dict, columns= ['utterance'])\r\n",
        "df_list = df.values.tolist()\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rnpPG-zqEtq"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from transformers import BertModel, BertTokenizer\r\n",
        "\r\n",
        "class Dataset():\r\n",
        "  def __init__(self, x, y, z):\r\n",
        "    self.utterance = x\r\n",
        "    self.emotion = y\r\n",
        "    self.mask = z\r\n",
        " \r\n",
        "  def __len__(self): \r\n",
        "    return len(self.utterance)\r\n",
        "\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    input_tensor = torch.tensor(self.utterance[idx])\r\n",
        "    labels_tensor = torch.tensor(self.emotion[idx])\r\n",
        "    mask_tensor = torch.tensor(self.mask[idx])\r\n",
        "    return input_tensor,labels_tensor, mask_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBWMuEb6_5hz"
      },
      "source": [
        "train_dataset = Dataset(token_train_utterance, train_emotion, masks_train_utterance)\r\n",
        "dev_dataset = Dataset(token_dev_utterance, dev_emotion, masks_dev_utterance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZBpxRRHACi2"
      },
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\r\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwn8v4Pry_h9"
      },
      "source": [
        "**Test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEDaXsoCzDP_"
      },
      "source": [
        "# test dataset load\r\n",
        "import csv\r\n",
        "dialogs = []\r\n",
        "\r\n",
        "with open('ko_data.csv', newline='', encoding='cp949') as csvfile:\r\n",
        "  dataread = csv.reader(csvfile)\r\n",
        "  for i,row in enumerate(dataread):\r\n",
        "    if i!=0:\r\n",
        "      dialogs.append([row[0], row[1]])\r\n",
        "\r\n",
        "# test data frame load\r\n",
        "df_test = pd.DataFrame(dialogs, columns=['Id','Sentence'])\r\n",
        "test_Id = df_test['Id'].values.tolist()\r\n",
        "test_utterance = df_test['Sentence'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0FybwcvamHK"
      },
      "source": [
        "# tag cls, sep, pad 넣기\r\n",
        "token_input(test_utterance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzM42JtJUpcC",
        "outputId": "9e2b5c58-ad8c-4c41-c36b-1d8341e8162d"
      },
      "source": [
        "test_utterance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] 정말 많이 울었던 영화입니다. [SEP] [PAD]',\n",
              " '[CLS] 시간 낭비예요. [SEP] [PAD]',\n",
              " '[CLS] 포스터를 저렇게밖에 만들지 못했던 제작자의 소심함에 침을 뱉고 싶다. [SEP] [PAD]',\n",
              " '[CLS] 지금 봐도 재미있는 영화!!! 코믹과 감동!!! 그리고 요리!!! [SEP] [PAD]',\n",
              " '[CLS] 이걸 영화로 만드는 거야?얼마나 가는지 보자. [SEP] [PAD]',\n",
              " '[CLS] 잔잔한 감동을 주는 영화가 좋은 영화다. 줄리안 무어의 매력! [SEP] [PAD]',\n",
              " '[CLS] 프랑스 영화, 정말 재미없다는 말밖에... [SEP] [PAD]',\n",
              " '[CLS] 이보다 더 자연스러울 수는 없다. [SEP] [PAD]',\n",
              " '[CLS] 잠만 자고 있었는데~~ [SEP] [PAD]',\n",
              " '[CLS] 오프닝 씬이... (이 영화와 젊음에 대해 말한다.) [SEP] [PAD]',\n",
              " '[CLS] 내용도 재미있고~ 유덕화가 가장 멋지게 나온 영화들~ [SEP] [PAD]',\n",
              " '[CLS] 영화는 없고 색깔만 있다 [SEP] [PAD]',\n",
              " '[CLS] 홍성진 영화해설자님우뢰매를 욕하다니... [SEP] [PAD]',\n",
              " '[CLS] 이게 뭐라고... [SEP] [PAD]',\n",
              " '[CLS] 우울한 작가주의인 척 [SEP] [PAD]',\n",
              " '[CLS] 재밌는데... [SEP] [PAD]',\n",
              " '[CLS] 최고...녹화 찍어놓은 걸 볼 수 있어서 정말 다행이야.. [SEP] [PAD]',\n",
              " '[CLS] 보면서 동감을 많이 해요. 보면 남의 일이 아닌 것 같아요. 수고하십시요 [SEP] [PAD]',\n",
              " '[CLS] 역시 강 씨. [SEP] [PAD]',\n",
              " '[CLS] 오오~ 정말 좋아해요!! [SEP] [PAD]',\n",
              " '[CLS] 진저얼굴짱 몸짱!! 정말 이뻐! [SEP] [PAD]',\n",
              " '[CLS] 감동과 함께 감성을 자극해 줍니다. [SEP] [PAD]',\n",
              " '[CLS] 힘과 강렬함...정말로 산뜻한 영화다.. [SEP] [PAD]',\n",
              " '[CLS] 우리 은별이 언니 b [SEP] [PAD]',\n",
              " '[CLS] 분위기만 나쁘게 해서 끝난다.... [SEP] [PAD]',\n",
              " '[CLS] 소설을 보는 듯한 지루한 줄거리와 볼썽사나운 액션 [SEP] [PAD]',\n",
              " '[CLS] 충훈이가 나왔으니까 많이 줘~! [SEP] [PAD]',\n",
              " '[CLS] 보고나서는 좀 허전하다 [SEP] [PAD]',\n",
              " '[CLS] 와~ [SEP] [PAD]',\n",
              " '[CLS] 말도 안돼! PS2 간접광고하는거 아니야? 이러니저러니 보양생 [SEP] [PAD]',\n",
              " '[CLS] 예술영화라고 하기엔 너무 탄탄한 스토리와 스케일... [SEP] [PAD]',\n",
              " '[CLS] 언어가 필요없는 최고의 애니메이션입니다.♡ [SEP] [PAD]',\n",
              " '[CLS] 사랑 앞에서는 돈도 필요없어!! 못 보신 분은 꼭 보세요. 추천! [SEP] [PAD]',\n",
              " '[CLS] 보시면 별루임을 알 수 있습니다. [SEP] [PAD]',\n",
              " '[CLS] 최고의 영화 [SEP] [PAD]',\n",
              " '[CLS] 보지 말아주세요.--; [SEP] [PAD]',\n",
              " '[CLS] 이 영화의 지루함은 우리에게 말로 표현할 수 없는 뭔가를 느끼게 하는 것이지 결코 지루하지 않다. [SEP] [PAD]',\n",
              " '[CLS] TV에서 우연히 접했다면 추천! 합니다. 아주 경쾌한 해프닝 사건. [SEP] [PAD]',\n",
              " '[CLS] 전혀 다르네요. [SEP] [PAD]',\n",
              " '[CLS] OOO~1점도 아쉽다~ [SEP] [PAD]',\n",
              " '[CLS] 완전히 빗나가 오히려 관객이 돈을 받아야 하는 영화! [SEP] [PAD]',\n",
              " '[CLS] 이런 에로쓰레기 영화를 극장판에서 보는 돈이 얼마나 아까운지 알 수 있다. [SEP] [PAD]',\n",
              " '[CLS] 정말 지루한 영화.. [SEP] [PAD]',\n",
              " '[CLS] 재미있는 작품 [SEP] [PAD]',\n",
              " '[CLS] 혼자 세상을 바꿀 수는 없지만 변화시킬 수는 있다. [SEP] [PAD]',\n",
              " '[CLS] 감독시마이카이라OO [SEP] [PAD]',\n",
              " '[CLS] 포스터만 보고 기대했다가 실패했어. [SEP] [PAD]',\n",
              " '[CLS] 저는 루트 영화를 못 받지만 저는 책을 읽었어요. [SEP] [PAD]',\n",
              " '[CLS] 이건 뭐냐? 영화 받아봤는데 재미없어서 진짜 허무하게 시작해서 허무하게 끝나. [SEP] [PAD]',\n",
              " '[CLS] 현대 중국의 모습, 그 속에서 사는 지금의 젊은이들. [SEP] [PAD]',\n",
              " '[CLS] 너무 야하다 [SEP] [PAD]',\n",
              " '[CLS] 더 이상 숨어서는 안 된다.그냥 지나갈 뿐... [SEP] [PAD]',\n",
              " '[CLS] 로맨스이긴 하지만 내용의 흐림이 어쩐지 어색한 것 [SEP] [PAD]',\n",
              " '[CLS] 글쎄요, 너무 평범해서 시간낭비하고 싶어요. [SEP] [PAD]',\n",
              " '[CLS] 허버 굳이 미야이즈미 죽이지마 -- 죽일 때가 기억에 남지 마 [SEP] [PAD]',\n",
              " '[CLS] 10점이 아깝지 않다 [SEP] [PAD]',\n",
              " '[CLS] 인권이라는 주제를 잘 표현한 영화 [SEP] [PAD]',\n",
              " '[CLS] 천재적인 아이들이 마음에 끌렸다고 생각합니다.~^-^!! [SEP] [PAD]',\n",
              " '[CLS] 이거는 거의 플루시맨을 보는 것 같은... 무슨 영화가 뭐야, 그래픽도. [SEP] [PAD]',\n",
              " '[CLS] 가족호러에 에디머피가 나오면 재미도 없고 호러도 아니라는 결론이 나온다 [SEP] [PAD]',\n",
              " '[CLS] 희망을 버리지 않고, 결국은 쟁취한다. 그러나 암울하다 [SEP] [PAD]',\n",
              " '[CLS] ㅋㅋㅋ 혹평이라니 ㅋㅋㅋ그때 당시 미국 개봉 때 유행했는데... [SEP] [PAD]',\n",
              " '[CLS] 요즘도 반히포크라테스가 시행되고 있는지 궁금합니다. [SEP] [PAD]',\n",
              " '[CLS] 이것은 왜 19살입니까? 공포때문에?? [SEP] [PAD]',\n",
              " '[CLS] 비디오로 보는것도 시간과 돈이 아깝다... [SEP] [PAD]',\n",
              " '[CLS] 금년 최악의 영화다 [SEP] [PAD]',\n",
              " '[CLS] 캐스팅만 화려해.정말... 다른게 남는게 없어 [SEP] [PAD]',\n",
              " '[CLS] 음악을 아는 사람에게는 1000점을 줘도 모자란 영화. [SEP] [PAD]',\n",
              " '[CLS] 정말 못 참겠네. -_-+ [SEP] [PAD]',\n",
              " '[CLS] 주인공들의 비극적인 종말과 그들의 관계... 도저히 걸을 수 없는 현실 [SEP] [PAD]',\n",
              " '[CLS] 미사키 너무 귀여워~~ [SEP] [PAD]',\n",
              " '[CLS] 아..속편은 왜 안나오는거죠? 보고싶지만 붉은매 사랑합니다 ^. [SEP] [PAD]',\n",
              " '[CLS] 내 인생에서 가장 중요한 영화!! [SEP] [PAD]',\n",
              " '[CLS] 예수님을 많이 알게되는 계기가 되네요 [SEP] [PAD]',\n",
              " '[CLS] 드디어 1퍼구!! 이 영화점이 정말 재미있어.. [SEP] [PAD]',\n",
              " '[CLS] 시간이 너무 남아서 어떻게 할 수가 없을 때 그때 봐도 화나게 [SEP] [PAD]',\n",
              " '[CLS] 이 영화는 반드시 재평가를 받아야 한다. [SEP] [PAD]',\n",
              " '[CLS] 들어도 재미있다. 빨리 만나고 싶다. [SEP] [PAD]',\n",
              " '[CLS] 이영걸의 소림사 시리즈 이후 오랜만의 작품이었다. [SEP] [PAD]',\n",
              " '[CLS] 선생님, 그 진지함에 비해서 영상미가 아까워요. [SEP] [PAD]',\n",
              " '[CLS] 아름다운 음악과 이야기.. [SEP] [PAD]',\n",
              " '[CLS] 영상이 정말 멋있었다.아레스랑 데이비드도 멋있어... 마지막이 우울해~ㅜㅜ [SEP] [PAD]',\n",
              " '[CLS] 영화 완전 재미없어...(책임질게요) 완전히 감동하지 않는 영화, 하류 감독작 [SEP] [PAD]',\n",
              " '[CLS] 좋은 느낌의... 최고의 영화 [SEP] [PAD]',\n",
              " '[CLS] 저처럼 딸도 영화관에서 멋진 추억을 만들 수 있어서 정말 좋았어요. [SEP] [PAD]',\n",
              " '[CLS] 1이 훨씬 더 좋은 것 같네요 2는 별로... [SEP] [PAD]',\n",
              " '[CLS] 좋은 의도로 만들어졌지만 영화 내에서는 의도가 느껴지지 않는다. [SEP] [PAD]',\n",
              " '[CLS] 족구같은 영화다 - 위에 평온한 녀석들은 뭐하는 너야? ...아니야 [SEP] [PAD]',\n",
              " '[CLS] 낭만 자객이 낫다. [SEP] [PAD]',\n",
              " '[CLS] 카피와 캐릭터를 좋아합니다. [SEP] [PAD]',\n",
              " '[CLS] 별 다섯 개! 세 번 봐야 하는 영화. [SEP] [PAD]',\n",
              " '[CLS] 별로 재미없었는데... ㅎㅎ 그래도 좀 웃긴 것 같기도 하고. 21세기에 [SEP] [PAD]',\n",
              " '[CLS] 너무 재미있었어요. [SEP] [PAD]',\n",
              " '[CLS] 복싱은 애술이다 누구나 복싱은 할 수 있지만 진정한 복싱인이 되려면 직접 쏘아야 복싱의 묘미 [SEP] [PAD]',\n",
              " '[CLS] ㄱ [SEP] [PAD]',\n",
              " '[CLS] ...정말 별로였다. . [SEP] [PAD]',\n",
              " '[CLS] 15년전부터 지금까지.. 곽재용의 영화는 모두 \"소나기\"다... [SEP] [PAD]',\n",
              " '[CLS] 너무 웃겼어. 전에 보고 최근에 봤는데 재밌어. [SEP] [PAD]',\n",
              " '[CLS] 쓰레기~ [SEP] [PAD]',\n",
              " '[CLS] 솔직히 기대 했었는데 주인공인 성우에게 실망했어. [SEP] [PAD]',\n",
              " '[CLS] 이런 싸구려 애니메이션은 초등학생을 제외하고는 보지 않는다. [SEP] [PAD]',\n",
              " '[CLS] 0점은 왜 안 했냐 [SEP] [PAD]',\n",
              " '[CLS] 캐스팅 미스, 스토리 구성 미흡 [SEP] [PAD]',\n",
              " '[CLS] 위에 평가한 사람 직원인가? 좀 솔직하게 평합시다!! 지루함 [SEP] [PAD]',\n",
              " '[CLS] 하지만 별로 재미없어요..(심심해) [SEP] [PAD]',\n",
              " '[CLS] 재미없는 드라마를 여행하고 [SEP] [PAD]',\n",
              " '[CLS] 남들이 뭐라고 할지는 모르겠지만 저는 최고의 영화라고 생각해요. [SEP] [PAD]',\n",
              " '[CLS] 신고 [SEP] [PAD]',\n",
              " '[CLS] 너희들이여, 더 열심히 해보세요. 강추! [SEP] [PAD]',\n",
              " '[CLS] 희미하게 보네...즐겁게 봐♪ [SEP] [PAD]',\n",
              " '[CLS] 내용이 어려워서 이해할 수 없는 영화가 아니라 설정자체가 부족하여 이해안감. [SEP] [PAD]',\n",
              " '[CLS] 나의 어릴때 가족들과 이불속에 들어가서 이 영화를 해준시간이 제일 좋았어. [SEP] [PAD]',\n",
              " '[CLS] 정말 감동적! 중학교 때 봤는데 아직도 가슴에 남아요.^ㅁ^ [SEP] [PAD]',\n",
              " '[CLS] 작지만 크다. 임승래의 첫 작품. 개인적으로 최초공개 첫회를 본... [SEP] [PAD]',\n",
              " '[CLS] 볼 만한 영화 [SEP] [PAD]',\n",
              " '[CLS] 웃긴.. [SEP] [PAD]',\n",
              " '[CLS] 너무 재밌어요.비전으로라도 사고 싶은데 중국은 없네요.구할 수 없나요? [SEP] [PAD]',\n",
              " '[CLS] 좋은 영화입니다. [SEP] [PAD]',\n",
              " '[CLS] 제이슨 vs 프레디처럼 재미없어 [SEP] [PAD]',\n",
              " '[CLS] 이것도 영화야!!!!절대 보지마라. [SEP] [PAD]',\n",
              " '[CLS] 진짜 재미없어 [SEP] [PAD]',\n",
              " '[CLS] 아프리카의 현실을 담은 영화, 그의 영화를 존경한다. [SEP] [PAD]',\n",
              " '[CLS] 마지막 엔딩곡은 박지윤의 성인식 리메이크곡이네요 [SEP] [PAD]',\n",
              " '[CLS] 진짜 어이없다. [SEP] [PAD]',\n",
              " '[CLS] 내 인생 최고의 영화를 보지 않으면 후회한다.. 정말 추천... 강추, 강추, 강추 [SEP] [PAD]',\n",
              " '[CLS] 한 점도 아까운 올해 최악의 미국 영화 [SEP] [PAD]',\n",
              " '[CLS] 68권까지 나온 것 같은데... [SEP] [PAD]',\n",
              " '[CLS] 보구...NOT REMMBER [SEP] [PAD]',\n",
              " '[CLS] 천재니 뭐니 황당하다.말도 안되는 상황 던져놓고 관객에게 맡길래? [SEP] [PAD]',\n",
              " '[CLS] 요즘은 파워레인저 하나에요? [SEP] [PAD]',\n",
              " '[CLS] 이병헌의 가장 못생긴 모습을 볼 수 있는 영화www [SEP] [PAD]',\n",
              " '[CLS] 너무 슬펐다.. [SEP] [PAD]',\n",
              " '[CLS] 정말 재밌어요. [SEP] [PAD]',\n",
              " '[CLS] 뭐.. [SEP] [PAD]',\n",
              " '[CLS] 너무 감동♡ [SEP] [PAD]',\n",
              " '[CLS] 오늘 봤는데... 옛날 영화로는 볼 만했네요. [SEP] [PAD]',\n",
              " '[CLS] 어떻게 된 거예요, 도대체...#;; [SEP] [PAD]',\n",
              " '[CLS] 때로는 진지하게, 때로는 코믹하게 연출하지만 여전히 재미없다! [SEP] [PAD]',\n",
              " '[CLS] 너무 심심해 [SEP] [PAD]',\n",
              " '[CLS] 장철의 3대 걸작 독비도 십삼태보 그리고 이 영화의 복수 [SEP] [PAD]',\n",
              " '[CLS] 어린 신부와 사건 사고들은 거의 똑같지만 구성은 허접.. [SEP] [PAD]',\n",
              " '[CLS] 나는 감독이 이걸 왜 만들었는지 정말 궁금하다.정말.. [SEP] [PAD]',\n",
              " '[CLS] 정말 재미있는 영화다. [SEP] [PAD]',\n",
              " '[CLS] 할리우드가 아깝다.아까... [SEP] [PAD]',\n",
              " '[CLS] 이런 영화를 수입하는 데 50억이나 쓰다니. [SEP] [PAD]',\n",
              " '[CLS] 제임스 우즈 [SEP] [PAD]',\n",
              " '[CLS] 이것도 영화라고 만들었니?기가 막힌 초등학생도 이보다는 잘 만든다. 푸핫핫. [SEP] [PAD]',\n",
              " '[CLS] 내용은 없어보이지만 흔히 우리 주변에서 볼 수 있는 평범한 이야기에 흥미진진한 [SEP] [PAD]',\n",
              " '[CLS] 주성치유덕화 킷츠씬이 왜 그렇게 재미있는지ㅋㅋㅋ [SEP] [PAD]',\n",
              " '[CLS] 그냥 드라마 같은 영화. [SEP] [PAD]',\n",
              " '[CLS] gg [SEP] [PAD]',\n",
              " '[CLS] 어렸을때 테이프가 처지도록 봤어~~ㅎㅎㅎ [SEP] [PAD]',\n",
              " '[CLS] 미국의 SF드라마를 다크엔젤 빼고 말할 수 있을까. [SEP] [PAD]',\n",
              " '[CLS] 내 인생영화~ 더스틴과 존의 연기도 최고~ [SEP] [PAD]',\n",
              " '[CLS] 지금 보고 있는데 뭔가 조용한 영화 같아요. [SEP] [PAD]',\n",
              " '[CLS] 호호 웃겨서 눈물나는 방송 [SEP] [PAD]',\n",
              " '[CLS] 잘 알려져 있지는 않지만 재미있습니다. [SEP] [PAD]',\n",
              " '[CLS] 어떤 감동도 재미없는 것을, 그냥 곰이 되자구. [SEP] [PAD]',\n",
              " '[CLS] 공포물을 좋아하는데 이거 최저... [SEP] [PAD]',\n",
              " '[CLS] 완전 로맨틱 코미디! [SEP] [PAD]',\n",
              " '[CLS] 존나 재미없는 영화..졸려 죽는 줄 알았어. [SEP] [PAD]',\n",
              " '[CLS] 20분만 보면 그냥 잔다. 이것을 DVD로 소장하는 사람은 크레용 신짱. [SEP] [PAD]',\n",
              " '[CLS] 이걸 보면 김나은씨의 애切ない한 모습을 볼 수 있을 것 같아.;; [SEP] [PAD]',\n",
              " '[CLS] 공포영화인 줄 알고 보다가 혼났어. 졸려. 귀신도 졸라 웃겨. [SEP] [PAD]',\n",
              " '[CLS] 금발이 지긋지긋한 태양같은 상큼함을 기대했는데 정말 어이없을 듯한 그런 영화 [SEP] [PAD]',\n",
              " '[CLS] 이거, 오시엔에서 밤에 자주 해 에로비안 나이트랑 비슷해. [SEP] [PAD]',\n",
              " '[CLS] 이것도 영화라고 만든 건가... [SEP] [PAD]',\n",
              " '[CLS] ........................................ [SEP] [PAD]',\n",
              " '[CLS] 난생처음 내 돈주고 본 영화중 극장에서 졸았던 영화다. [SEP] [PAD]',\n",
              " '[CLS] 왜 케빈 스미스의 뉴저지 연작 시리즈 완결편에 만점을 주지 않을까. [SEP] [PAD]',\n",
              " '[CLS] 대체 배신 자학 집착 이런 느낌인데 멀게 느끼란 말이야? 권하지 않는 껍질 조아하심보샘 [SEP] [PAD]',\n",
              " '[CLS] 헨리 폰다의 일당백총격신은 지금 봐도 훌륭해... [SEP] [PAD]',\n",
              " '[CLS] 너무나 오랜만에 감동의 잔잔함을 느꼈습니다....추천해요~!!! 이런 영화 [SEP] [PAD]',\n",
              " '[CLS] #재미 [SEP] [PAD]',\n",
              " '[CLS] 109분짜리 단편영화를 본 것 같다.영상미에별2 [SEP] [PAD]',\n",
              " '[CLS] 90년대 영화에서만 느낄 수 있었던 향기 [SEP] [PAD]',\n",
              " '[CLS] 우연히 TV에서 보게 되었는데 정말 아름다운 영화였어요. [SEP] [PAD]',\n",
              " '[CLS] 앨범에 담겨있지 않은 창문에서 보는 세상.. [SEP] [PAD]',\n",
              " '[CLS] 가슴 어딘가 한곳이 아파. 울고 싶은 영화 [SEP] [PAD]',\n",
              " '[CLS] 그냥... 1000원짜리 에로영화... [SEP] [PAD]',\n",
              " '[CLS] 정말 재미없어 [SEP] [PAD]',\n",
              " '[CLS] 강렬한 감독 [SEP] [PAD]',\n",
              " '[CLS] 소재가 너무 진부해서 또 심심해서... [SEP] [PAD]',\n",
              " '[CLS] 재미있다. 나름대로 식스 센스도 아닌 꽤 괜찮은 내용의 전개이고, 꽤 짜여져 있다. [SEP] [PAD]',\n",
              " '[CLS] good [SEP] [PAD]',\n",
              " '[CLS] 버피보다 재미만 있었는데.... 버피도 물론 재밌다 [SEP] [PAD]',\n",
              " '[CLS] 당시에는 이런 재미를 주는 코믹 액션이 없었어요. [SEP] [PAD]',\n",
              " '[CLS] 마지막이 인상깊었다 [SEP] [PAD]',\n",
              " '[CLS] 어설픈 시도 [SEP] [PAD]',\n",
              " '[CLS] 경찰 조쉬 하트넷과 해리슨포드의 하루 일과를 영화화했다. [SEP] [PAD]',\n",
              " '[CLS] 박중훈은 역시 최고야! [SEP] [PAD]',\n",
              " '[CLS] 오르가슴에 집착.잼은 없었다. 정직 [SEP] [PAD]',\n",
              " '[CLS] 불쌍하다... [SEP] [PAD]',\n",
              " '[CLS] I wang give this movie abour 0 point.!! [SEP] [PAD]',\n",
              " '[CLS] 쓰레기 [SEP] [PAD]',\n",
              " '[CLS] (어릴때 TV에서 재밌게 본 영화.. [SEP] [PAD]',\n",
              " '[CLS] 6살때 본 기억이...(웃음) [SEP] [PAD]',\n",
              " '[CLS] 이해할 수 없었던 영화 [SEP] [PAD]',\n",
              " '[CLS] 포스터에 바깥 스릴러물하고 넣을까? 얄팍한 흥행을 목적으로 자멸한 영화 [SEP] [PAD]',\n",
              " '[CLS] 꼭 보세요. [SEP] [PAD]',\n",
              " '[CLS] 정말로 이상한 영화네요.(웃음) [SEP] [PAD]',\n",
              " '[CLS] 3점 준 애가 누구지? [SEP] [PAD]',\n",
              " '[CLS] 유치하다고? 웃겨! [SEP] [PAD]',\n",
              " '[CLS] 도대체 무슨 내용인지 모르겠어. [SEP] [PAD]',\n",
              " '[CLS] 정말 팬덤한 영화네~ [SEP] [PAD]',\n",
              " '[CLS] 마이너스 점수가 그리울 줄 몰랐어. [SEP] [PAD]',\n",
              " '[CLS] 10년도 훨씬 전에 본 영화 아직 내 가슴에 추억으로 남아있어...(웃음) [SEP] [PAD]',\n",
              " '[CLS] 완전한 사랑의 의미를 알고 만든것인가.. [SEP] [PAD]',\n",
              " '[CLS] 깡패지만 인생의 정취를 아는 김민정의 캐릭터를 좋아했다. [SEP] [PAD]',\n",
              " '[CLS] 나는 너무 재미있었어. 아자피 코믹 멜로 아닌가? [SEP] [PAD]',\n",
              " '[CLS] 주사로 좀비를 만든다니... 좀 신기하기도 했지만 기대에 못 미칩니다... [SEP] [PAD]',\n",
              " '[CLS] 한국영화의 미래와 발전을 위해 영화계를 떠나야 할 쓰레기 감독 여균동 [SEP] [PAD]',\n",
              " '[CLS] 아프다 [SEP] [PAD]',\n",
              " '[CLS] 쓰레기 같은 현대 문명의 잔혹성을 비판. 홀로코스트의 뜻을 모르나 [SEP] [PAD]',\n",
              " '[CLS] 너무 멋있어요.쿠쿠후쿠 [SEP] [PAD]',\n",
              " '[CLS] 정말 치밀한 구성시나리오... 그냥 영화 보러 오신 분들은 못 보시는 것처럼... [SEP] [PAD]',\n",
              " '[CLS] 뭔가 로맨틱할거라고 생각해본다면 no~ [SEP] [PAD]',\n",
              " '[CLS] 바이오맨이고싶다. [SEP] [PAD]',\n",
              " '[CLS] 재미는 X [SEP] [PAD]',\n",
              " '[CLS] 1점 준 거 이해 못하겠네.눈이 부셔졌나? [SEP] [PAD]',\n",
              " '[CLS] 전부 조용해.... 가끔 총소리만 귀에 들어와... [SEP] [PAD]',\n",
              " '[CLS] 나는 잭키한 영화 같은 느낌을 받았다 [SEP] [PAD]',\n",
              " '[CLS] 스토리도 액션도 잘 못하는데 여주인공까지 예쁘지 않아. [SEP] [PAD]',\n",
              " '[CLS] 꽤 인기있었잖아 [SEP] [PAD]',\n",
              " '[CLS] 엄청난 캐릭터의 힘! [SEP] [PAD]',\n",
              " '[CLS] Let The River Run.. 가슴 벅찬 BGM [SEP] [PAD]',\n",
              " '[CLS] 삶과 죽음, 삶의 무게를 느낄 수 있는 길을 알려준 영화 [SEP] [PAD]',\n",
              " '[CLS] 정말 멋진 영화다 [SEP] [PAD]',\n",
              " \"[CLS] '준'을 빙자한 일탈 야, 그냥 솔직하게 말하는 게 어때? [SEP] [PAD]\",\n",
              " '[CLS] 원래 이런 영화는 기대다. [SEP] [PAD]',\n",
              " '[CLS] 키아누는 정말 작품 고르는 눈이 낮다. [SEP] [PAD]',\n",
              " '[CLS] 일본에서 다그치면 한국만화일까? [SEP] [PAD]',\n",
              " '[CLS] 여자는 행복하다 [SEP] [PAD]',\n",
              " '[CLS] 모델들이 배우라는 이름을 갖게 하기 위해 만들어졌다. [SEP] [PAD]',\n",
              " '[CLS] 쓰레기... 권해효에게 2점 [SEP] [PAD]',\n",
              " '[CLS] 내가 가장 좋아하는 영화 제목중 하나 [SEP] [PAD]',\n",
              " '[CLS] 감독은 발이 넓다.쓰레기 시나리오에서 배우를 캐스팅 하는 것은 [SEP] [PAD]',\n",
              " '[CLS] 존내 재미있는 일 [SEP] [PAD]',\n",
              " '[CLS] 내 인생 최고의 영화 [SEP] [PAD]',\n",
              " '[CLS] 감동이 없다면 당신은 순수 0% [SEP] [PAD]',\n",
              " '[CLS] 쓰레기영화 어른3류영화관서 신음소리 듣는다 [SEP] [PAD]',\n",
              " '[CLS] 상당한 재미균형을 잡기 위해서. [SEP] [PAD]',\n",
              " '[CLS] 이 영화 보고 10점 준 놈들 달려라 [SEP] [PAD]',\n",
              " '[CLS] 역시...코미디구나...최악 [SEP] [PAD]',\n",
              " '[CLS] 우연이 만드는 운명이란 올챙이가 병아리가 되는 그런 것일까? [SEP] [PAD]',\n",
              " '[CLS] 외계인, 너무 바보같이 나온다. 기술은 좋은데 머리는 바보야. [SEP] [PAD]',\n",
              " '[CLS] 비주~ 보면 후회 쓰레기 [SEP] [PAD]',\n",
              " '[CLS] 불확실한 미래에 대한...노력...하지만 냉정한 현실.... [SEP] [PAD]',\n",
              " \"[CLS] 18세 영화? 영화 완전 초등학생용'코딱지 싸움, 트램 싸움' 초등생에게 권한다 [SEP] [PAD]\",\n",
              " '[CLS] 전형적인 할리우드 버디 영화. 재미는 꽤 좋다. [SEP] [PAD]',\n",
              " '[CLS] 재밌어. [SEP] [PAD]',\n",
              " '[CLS] 흥행에 성공한다면...아마도... 나는 일반 대중이 아닐 것이다. [SEP] [PAD]',\n",
              " '[CLS] 전작을 능가하는 지루함 역시 속편은 강도가 있다. [SEP] [PAD]',\n",
              " '[CLS] 웃음을 원한다면 보지말아라 진실을 알고싶으면 보지말아라!! [SEP] [PAD]',\n",
              " '[CLS] 이재은 신이 내렸다? 응응...울나라 정서를 잘 표현한 영화... [SEP] [PAD]',\n",
              " '[CLS] 시사회 같이 간 친구에게 미안할 정도 [SEP] [PAD]',\n",
              " '[CLS] 화장실 유머...이해하기 힘들다... [SEP] [PAD]',\n",
              " '[CLS] 실화라면 더 실감할 수 있는 [SEP] [PAD]',\n",
              " '[CLS] 영화가 아니라 쓰레기다. [SEP] [PAD]',\n",
              " '[CLS] 여배우는 정말 마음에 안 들어. 샤무엘 잭슨-헨리 연쇄살인자도 등장 [SEP] [PAD]',\n",
              " '[CLS] 초등학교때 학교에서 본 기억이...ㅋㅋㅋ재미있었어~ [SEP] [PAD]',\n",
              " '[CLS] 재미있었다. [SEP] [PAD]',\n",
              " '[CLS] 주연배우 이름만 기억에 남아. [SEP] [PAD]',\n",
              " '[CLS] 중국의 유치 짬뽕 3류 드라마도 이보다 좋다. [SEP] [PAD]',\n",
              " '[CLS] 재은아... 왜 그래? [SEP] [PAD]',\n",
              " '[CLS] 많이 느끼게 하다 [SEP] [PAD]',\n",
              " '[CLS] 그 나름대로 여러 가지 생각하게 한 영화 [SEP] [PAD]',\n",
              " '[CLS] 마지막, 짜증이 나는 여동생의 탈이 벗겨진 것은 이상하지만 평범하다 [SEP] [PAD]',\n",
              " '[CLS] 참 재미있다 [SEP] [PAD]',\n",
              " '[CLS] 네, 아첨 더 던질게요. [SEP] [PAD]',\n",
              " '[CLS] 연기를 잘해요. [SEP] [PAD]',\n",
              " '[CLS] 0점 있으면 주고 싶은데 캐서린 제타 존스 팬이라서 3점 줄게. [SEP] [PAD]',\n",
              " '[CLS] 우~~♥ [SEP] [PAD]',\n",
              " '[CLS] 재미있습니다 [SEP] [PAD]',\n",
              " '[CLS] 재미있습니다. [SEP] [PAD]',\n",
              " '[CLS] 정말 재밌어요! [SEP] [PAD]',\n",
              " '[CLS] 감상문을 쓰기엔 너무 어려운 영화~ [SEP] [PAD]',\n",
              " '[CLS] 결국 후반부에 혼자 싸우는 할리우드의 법칙 [SEP] [PAD]',\n",
              " '[CLS] 크리스는 입이 클수록 시끄럽다. 영화도 시끄럽다 [SEP] [PAD]',\n",
              " '[CLS] 왜 이런 영화를 만들었을까. [SEP] [PAD]',\n",
              " '[CLS] 차라리 책을 보았으면 좋겠다구 [SEP] [PAD]',\n",
              " '[CLS] 평이 별로 난 꽤 무서워서 재미있게 봤는데; [SEP] [PAD]',\n",
              " '[CLS] 그냥 좀 황당하다. [SEP] [PAD]',\n",
              " '[CLS] 읽지도 않았는데 스캔된 장면이 압권. 뭐 영화니까 말이야. (웃음) [SEP] [PAD]',\n",
              " '[CLS] 수면영화의 최고봉! 불면증에 직빵! And napping goes on. [SEP] [PAD]',\n",
              " '[CLS] 용환아, 분명히 꽝 근데 여균동은 쓰레기가 아니에요 [SEP] [PAD]',\n",
              " '[CLS] 재미가 없다.일본의 문화(특히 사무라이는) 아직 위화감을 느낀다. [SEP] [PAD]',\n",
              " '[CLS] 썰렁~ [SEP] [PAD]',\n",
              " '[CLS] 황당한 스토리에 어디선가 많이 본 캐릭터들. [SEP] [PAD]',\n",
              " '[CLS] 박중훈 혼자 오버 너무 많이 해 모든 점에서 만족감을 줄 수 없다 [SEP] [PAD]',\n",
              " '[CLS] 이 배우들은 왜 이 영화에 나왔을까? [SEP] [PAD]',\n",
              " '[CLS] 거짓말로 만든 길리엄의 또 다른 저주받은 SF 걸작. [SEP] [PAD]',\n",
              " '[CLS] 가슴이 아파 --;; 내.. [SEP] [PAD]',\n",
              " '[CLS] 상우야, 그만 좀 해.매일 질질 드라마에만 출연한다. [SEP] [PAD]',\n",
              " '[CLS] 좀더 사실을 세상에 알려 주십시오. [SEP] [PAD]',\n",
              " '[CLS] 어이없는 설정부터 짜증났던 초반 건물 폭파 장면은 볼 만 [SEP] [PAD]',\n",
              " '[CLS] 재즈의 음색과 영화의 색채가 동시에 망각에 들어간다. [SEP] [PAD]',\n",
              " '[CLS] 깊은 감동이 오래 남는 좋은 영화.. [SEP] [PAD]',\n",
              " \"[CLS] 영어 제목처럼 '죽지 않는다'고 보는 동안 울화 지수는 높아졌다 [SEP] [PAD]\",\n",
              " '[CLS] 해리스 써서 만든 비겁한 상업 에로 영화 [SEP] [PAD]',\n",
              " '[CLS] 최고야~ 최고! [SEP] [PAD]',\n",
              " '[CLS] 보지 않는 편이 좋습니다. 재미도 없고 안무도 무섭고 [SEP] [PAD]',\n",
              " '[CLS] 한때 유명했던 영화 포스터, 하지만 너무나 뻔한 스토리. [SEP] [PAD]',\n",
              " '[CLS] 별루 [SEP] [PAD]',\n",
              " '[CLS] 음. [SEP] [PAD]',\n",
              " \"[CLS] 따뜻한 새벽에 혼자 봤는데 정말 느낌이 좋은 영화였다. ' [SEP] [PAD]\",\n",
              " '[CLS] 나이틀리 역의 제러미 노덤... 딱이다! [SEP] [PAD]',\n",
              " '[CLS] 도대체...우리를 어떻게 하라는건지..좋은점수를 줄수가 없어. [SEP] [PAD]',\n",
              " '[CLS] 고잉호잉만 봤어요 아무튼 울었으니까 9점~ [SEP] [PAD]',\n",
              " '[CLS] 아름다운 곡 [SEP] [PAD]',\n",
              " '[CLS] 전작과의 차별성을 갖고 싶다 [SEP] [PAD]',\n",
              " '[CLS] 너무 좋았다.항상 다시 보고 싶은 영화 1위 [SEP] [PAD]',\n",
              " '[CLS] 밑에서 평가하신 분들은 모두 스텝인가요? [SEP] [PAD]',\n",
              " '[CLS] 공포 다큐멘터리라고 잔뜩 기대 했었는데. 호러는 아니었어 [SEP] [PAD]',\n",
              " '[CLS] 제임스 딘 그는 상처받기 쉬운 섬세한 영혼의 소유자다~~ [SEP] [PAD]',\n",
              " '[CLS] 심심해.. 재미없어.. 시간만큼 아깝다.. [SEP] [PAD]',\n",
              " '[CLS] 파괴적이지 않은 간절한 사랑 이야기 [SEP] [PAD]',\n",
              " '[CLS] 정치는 피 흘리지 않는 전쟁이고, 전쟁은 피 흘리는 정치다. [SEP] [PAD]',\n",
              " '[CLS] 이런 멋진 영화를... 점수가 너무 낮아! 로그인 시킬께. [SEP] [PAD]',\n",
              " '[CLS] 우리 나라의 정서에 맞지 않는 영화 [SEP] [PAD]',\n",
              " '[CLS] 일본인은 영화 만들지 마 [SEP] [PAD]',\n",
              " '[CLS] 보지마 절대!! 최악이 무엇인지 보여준 영화 [SEP] [PAD]',\n",
              " '[CLS] 초반에는 재밌는데 중반으로 갈수록 조금... [SEP] [PAD]',\n",
              " '[CLS] OO데 마이신다일류 배우를 둔 삼류 드라마다. . [SEP] [PAD]',\n",
              " '[CLS] 어째서 신기전이 너희들 대신에 당당하게 쓰는거야!! [SEP] [PAD]',\n",
              " '[CLS] 이게 2003년작이라니 ...끝까지 보는 건 임파서블이야 [SEP] [PAD]',\n",
              " '[CLS] 내가 올린 사진이 있어 [SEP] [PAD]',\n",
              " '[CLS] 야한 장면도 없지만...약한 영화....하다 [SEP] [PAD]',\n",
              " '[CLS] 음.. 이제 시걸도 은퇴할 때가 된 것 같아. [SEP] [PAD]',\n",
              " '[CLS] 이 정도의 유머는 허영만 만들 수 있고 슈퍼보드에도 있다!! [SEP] [PAD]',\n",
              " '[CLS] 로버트 드니로도 이제 끝인가... 좋아하는 배우인데 말야. [SEP] [PAD]',\n",
              " '[CLS] 단순한 스토리지만, 계속 떨어져간다 그리고 아름다워 [SEP] [PAD]',\n",
              " '[CLS] 재미있다! 한마디면 좋을 것 같다. 배우좋아 [SEP] [PAD]',\n",
              " '[CLS] 정말 최고야~~~~ [SEP] [PAD]',\n",
              " '[CLS] 독특한데 개뿔...시간, 돈이 모두 아까운 영화 [SEP] [PAD]',\n",
              " '[CLS] 한국 애니메이션의 가능성을 한단계 높인 영화.. 굿 잡! [SEP] [PAD]',\n",
              " '[CLS] 아 공포영화였나보네. [SEP] [PAD]',\n",
              " '[CLS] 잔잔하게 이야기를 풀어가며 감동을 주는 영화 [SEP] [PAD]',\n",
              " '[CLS] 정말 말이 필요없네요. [SEP] [PAD]',\n",
              " '[CLS] K-1 선수 레이세포와 게리 굿리지 카메오 출연합니다. 찾아보게 [SEP] [PAD]',\n",
              " '[CLS] 스티븐 킹 원작에서 두번째로 별로 [SEP] [PAD]',\n",
              " '[CLS] 기어의 청춘 [SEP] [PAD]',\n",
              " '[CLS] 몇 번을 봐도 재미있는 영화.. [SEP] [PAD]',\n",
              " '[CLS] 솔직히 기대이하 두배우에게 기대 했었는데 [SEP] [PAD]',\n",
              " '[CLS] 피눈물과 닮은 섬마을의 위험함 [SEP] [PAD]',\n",
              " '[CLS] 형편없는 쓰레기 영화 [SEP] [PAD]',\n",
              " '[CLS] 재미있었어. [SEP] [PAD]',\n",
              " '[CLS] 섹스 신 대작 [SEP] [PAD]',\n",
              " '[CLS] 이대근의 묵직한 김두한의 연기 [SEP] [PAD]',\n",
              " '[CLS] 꽤 재미있었어요.(웃음) [SEP] [PAD]',\n",
              " '[CLS] 최고의 웃음!! [SEP] [PAD]',\n",
              " '[CLS] 북한 소녀들이 소재라는 점만 빼면 다큐멘터리 같은 거... [SEP] [PAD]',\n",
              " '[CLS] 캐스팅은 화려하지만 주인공의 연기를 별로 좋아하지 않아요. [SEP] [PAD]',\n",
              " '[CLS] 핵꿀잼 [SEP] [PAD]',\n",
              " '[CLS] 보면 볼수록 감동이 밀려옵니다! 많이 좋아해요 [SEP] [PAD]',\n",
              " '[CLS] 인투 더 헬이다.그게 바로... [SEP] [PAD]',\n",
              " '[CLS] 재미없지만 뭔가 있어. [SEP] [PAD]',\n",
              " '[CLS] 잔잔한 드라마 고두심씨의 연기가 정말 좋아요!! [SEP] [PAD]',\n",
              " '[CLS] 효도 심청의 이야기와 비슷하지만(웃음) 짱! [SEP] [PAD]',\n",
              " '[CLS] 어렸을 때 무서워서 재미있게 봤다..ㅋㅋ 마지막 날으는 씬 감동 [SEP] [PAD]',\n",
              " '[CLS] 재밌어. [SEP] [PAD]',\n",
              " '[CLS] 이시구로 노보루 감독의 TV 시리즈가 진정한 마크로스다. [SEP] [PAD]',\n",
              " '[CLS] 신랄한 조롱으로 정치를 풍자하다. 드니로의 연기는 단연 최고 [SEP] [PAD]',\n",
              " '[CLS] 최고의 데뷔작 중 하나. 실내를 조이는 미장센은 정말 멋지다 [SEP] [PAD]',\n",
              " '[CLS] 풀하우스 [SEP] [PAD]',\n",
              " '[CLS] 최고 최고 *`* [SEP] [PAD]',\n",
              " '[CLS] 불면증 치료 ok~ [SEP] [PAD]',\n",
              " '[CLS] 바보선언의 영화적 기법은 당시는 물론 현대까지도 생소할 정도다. [SEP] [PAD]',\n",
              " '[CLS] 따뜻하고 즐거운 영화 [SEP] [PAD]',\n",
              " '[CLS] 한국어 더빙판은 OOO기 임원판 추천 [SEP] [PAD]',\n",
              " '[CLS] 이것이 영웅의 본색보다 나은 것인가. 잔인하기 짝이 없는 쓰레기 영화 [SEP] [PAD]',\n",
              " '[CLS] 엔딩만 충격적, 가본적 있는 촬영장소라서 신선하다. [SEP] [PAD]',\n",
              " '[CLS] 시도만 좋았다 [SEP] [PAD]',\n",
              " '[CLS] 이런 삼류 영화인줄은 모르고 봐버렸다. [SEP] [PAD]',\n",
              " '[CLS] 사상 최악의 드라마 영화도 아니다. 역겹다. [SEP] [PAD]',\n",
              " '[CLS] Passion flows again... [SEP] [PAD]',\n",
              " '[CLS] 연애의 몸짓이나 하는 드라마보다는 낮다. [SEP] [PAD]',\n",
              " '[CLS] 초등학교 때 본 기억이 나요. 생각나면 감동받는 영화.. [SEP] [PAD]',\n",
              " '[CLS] 3점 들어갑니다. [SEP] [PAD]',\n",
              " '[CLS] 한길수, 전국민이 알아야 할 영화 [SEP] [PAD]',\n",
              " '[CLS] 엔딩 크레딧이 올라가는 순간 영화는 다시 시작된다. [SEP] [PAD]',\n",
              " '[CLS] 잔혹, 강간, 살인만이 등장하는 X같은 영화 [SEP] [PAD]',\n",
              " '[CLS] 잔인함보다 르네의 매력이 잘 드러난 영화! 수작이다. [SEP] [PAD]',\n",
              " '[CLS] 일단 서양정서를 감안하더라도 이것은 OOO기 영화다. [SEP] [PAD]',\n",
              " '[CLS] 역시 1편이 제일 재미있었어. [SEP] [PAD]',\n",
              " '[CLS] 눈물 젖은 감동적인 영화 [SEP] [PAD]',\n",
              " '[CLS] 별로 기대 안 했는데 배꼽이 빠진 줄 알았어 [SEP] [PAD]',\n",
              " '[CLS] 드라마를 보고 울어본적은 처음이야. [SEP] [PAD]',\n",
              " '[CLS] 역사는 승자만의 기록이라지만 일본이 미래의 승자라는 것은 좀 끔찍하다. [SEP] [PAD]',\n",
              " '[CLS] 이 영화의 원제는 Nowhere in Africa(아프리카 어디에도) [SEP] [PAD]',\n",
              " '[CLS] 복잡한 생각은 필요없다 차라리 로코를 봐 버리자 [SEP] [PAD]',\n",
              " '[CLS] 킥킥거리다 보면 러닝타임이 바로... Joseph Locke.. [SEP] [PAD]',\n",
              " '[CLS] 좋네요^_^ [SEP] [PAD]',\n",
              " '[CLS] 진짜 짱짱! [SEP] [PAD]',\n",
              " '[CLS] 쥐를 너무 맛있게 먹어서... 신고 [SEP] [PAD]',\n",
              " '[CLS] 정말 재밌었다. 아직도 생각나다 [SEP] [PAD]',\n",
              " '[CLS] 피같이 붉은 사랑이야기 [SEP] [PAD]',\n",
              " '[CLS] 글쎄...... [SEP] [PAD]',\n",
              " '[CLS] 최고의 블랙코미디와 패러디, 최고의 카메오들 그리고 FUCK. [SEP] [PAD]',\n",
              " '[CLS] 좋아요. 가족영화로는 작품이 좋아요. [SEP] [PAD]',\n",
              " '[CLS] 잔잔한 행복이 담겨있는 영화.. 금성무 너무 좋아 [SEP] [PAD]',\n",
              " '[CLS] 평점 풀터보니 아르바이트가 있다 [SEP] [PAD]',\n",
              " '[CLS] 초반에만 괜찮았다.그 다음은 감독이 손을 뗀 듯한 공허한 전개. [SEP] [PAD]',\n",
              " '[CLS] 그런 영화... [SEP] [PAD]',\n",
              " '[CLS] 누가 이 영화를 보고 카펜터 영화라고 할 수 있을까. [SEP] [PAD]',\n",
              " '[CLS] 오리엔탈리즘에 한 표. 마지막으로 양희와 양 키스는 역겨웠다. [SEP] [PAD]',\n",
              " '[CLS] 초반의 긴 반감이 마지막으로 무너진 용두사미가 된 영화 [SEP] [PAD]',\n",
              " '[CLS] 기대 많이 해주시고요. 감동의 물결이 밀려옵니다. [SEP] [PAD]',\n",
              " '[CLS] 반전이 있는 멜로 영화? [SEP] [PAD]',\n",
              " '[CLS] 최고로 웃겼어 !! 펭귄 한마리 키우고싶어 !! [SEP] [PAD]',\n",
              " '[CLS] 원작을 흉내내도 중요한 부분은 드물게 고치는 센스 [SEP] [PAD]',\n",
              " '[CLS] 더러운 스토리와 영상, 지루한 시간의 평점이 너무 높다. [SEP] [PAD]',\n",
              " '[CLS] 재미있네요 [SEP] [PAD]',\n",
              " '[CLS] 지루해.. 뻔한 스토리.. 전편이 훨씬 좋은 것 같아.. [SEP] [PAD]',\n",
              " '[CLS] 60년대 블록버스터 영화, 전쟁보다 전쟁 스릴러라는 게 맞을 것이다. [SEP] [PAD]',\n",
              " '[CLS] 이걸 정말 멋져요.ㅜㅜ [SEP] [PAD]',\n",
              " '[CLS] 영화의 불감증은 조금이라도 해소된 수작 [SEP] [PAD]',\n",
              " '[CLS] 그들의 순수함이 묻어나는 유혹에 빠져보면? [SEP] [PAD]',\n",
              " '[CLS] 반전영화!! 기가 막혀! [SEP] [PAD]',\n",
              " '[CLS] 3개의 시리즈중에서 가장 멋있는 반장의 카리스마!! [SEP] [PAD]',\n",
              " '[CLS] 다 좋은데? 너무 잔잔하다. [SEP] [PAD]',\n",
              " '[CLS] 꼭 봐주세요 [SEP] [PAD]',\n",
              " '[CLS] 이 영화를 보고 사랑이란 명목 아래 불륜 바람이 불긴 할까. [SEP] [PAD]',\n",
              " '[CLS] 신경숙은 원작 가격으로 얼마를 받았을까. [SEP] [PAD]',\n",
              " '[CLS] 그냥 밴덤 영화. 더 이상 발전할 수 없다. [SEP] [PAD]',\n",
              " '[CLS] 케이블 xtm 에서 봤는데 정말 재밌더라구요. [SEP] [PAD]',\n",
              " '[CLS] 아! 희망이란... 미래란.... [SEP] [PAD]',\n",
              " '[CLS] 나름대로 배역에 맞는 적절한 캐스팅이 좋다. [SEP] [PAD]',\n",
              " '[CLS] 챈들러한테는 미안하지만... [SEP] [PAD]',\n",
              " '[CLS] 돌을 던진 자 스스로는 깨끗한가. 나에게 들어라 [SEP] [PAD]',\n",
              " '[CLS] 담담하게 서술한 기적 같은 스포츠사의 한 변. [SEP] [PAD]',\n",
              " '[CLS] 존재조차 몰랐던 영화!! noraism한테 맞아서 나한테 와. [SEP] [PAD]',\n",
              " '[CLS] 순덕이! 순덕이! 순덕이! 순덕이! [SEP] [PAD]',\n",
              " '[CLS] 돈으로 보는 개콘 개그콘서트를 볼 수 없어. [SEP] [PAD]',\n",
              " '[CLS] 점수를 더 줄 수는 없을까?그 시대에 경험할 수 있는 모든 것 [SEP] [PAD]',\n",
              " '[CLS] 그렇게 좋았어.. [SEP] [PAD]',\n",
              " '[CLS] 독립영화의 승리 재밌었어요. [SEP] [PAD]',\n",
              " '[CLS] 이 영화의 변명은 가족영화라는 것이다. [SEP] [PAD]',\n",
              " '[CLS] SF에서는 절대 아니다. 단지 심리, 미스터리, 드라마 장르의 영화. [SEP] [PAD]',\n",
              " '[CLS] 정말 재밌었던... [SEP] [PAD]',\n",
              " '[CLS] 우와! [SEP] [PAD]',\n",
              " '[CLS] 반전을 기억이 안 나는데 [SEP] [PAD]',\n",
              " '[CLS] 조은숙 대박이다 [SEP] [PAD]',\n",
              " '[CLS] ...이 영화는 한국 SF영화의 자존심입니다.정말 훌륭한 영화입니다. [SEP] [PAD]',\n",
              " '[CLS] 시대적 성장물 [SEP] [PAD]',\n",
              " '[CLS] 뭐야 이건? 삼류범죄물☆ [SEP] [PAD]',\n",
              " '[CLS] 멜로를 블렌딩한 시리즈. [SEP] [PAD]',\n",
              " '[CLS] 비트박스 못잊어~ [SEP] [PAD]',\n",
              " '[CLS] 배창호 최고 걸작 [SEP] [PAD]',\n",
              " '[CLS] 최악의 졸작이다 [SEP] [PAD]',\n",
              " '[CLS] 극히 폭력적이었던 작품 [SEP] [PAD]',\n",
              " '[CLS] 좋은 소재를 엉망으로 만든 영화 [SEP] [PAD]',\n",
              " '[CLS] 볼 게 없어서 봤다. [SEP] [PAD]',\n",
              " '[CLS] 답답한 드라마 [SEP] [PAD]',\n",
              " '[CLS] OOO전에 만들어졌다고는 도저히 믿을수없을정도의 화려함. [SEP] [PAD]',\n",
              " '[CLS] 프랑스인이 즐길 수 있는 황당한 액션물 [SEP] [PAD]',\n",
              " '[CLS] 스타 특락을 배경으로 한 비보 SF물. [SEP] [PAD]',\n",
              " '[CLS] 몇 번을 봐도 마음에 와닿는 영화.. 바로 그 영화.바구시 [SEP] [PAD]',\n",
              " '[CLS] 일단 봐라 [SEP] [PAD]',\n",
              " '[CLS] 기존의 드라마와는 다른 풋풋한 드라마는 정말 재미있었습니다. [SEP] [PAD]',\n",
              " '[CLS] 이상아가 괜찮게 나왔는데. [SEP] [PAD]',\n",
              " \"[CLS] '성룡' 주연인 줄 알고 속인 영화, 쓰레기 확정! [SEP] [PAD]\",\n",
              " '[CLS] 좋네요. 제 여동생도 좋고요~ [SEP] [PAD]',\n",
              " '[CLS] 내용을 도무지 알 수 없다. 보험사 직원과 잘 먹고 잘 살 수 있나 [SEP] [PAD]',\n",
              " '[CLS] 뉴욕을 배경으로 한 영화여서 끌렸지만 내용이나 엔딩이 많이 [SEP] [PAD]',\n",
              " '[CLS] 웃음밖에 나오지 않다 [SEP] [PAD]',\n",
              " '[CLS] 정태우였구나 [SEP] [PAD]',\n",
              " '[CLS] 스토리는 별로 없는데. 화려함 굉장하네요.ww [SEP] [PAD]',\n",
              " '[CLS] 너무 재미있어서.. 올 시즌 4기까지 계속 보고 있는 중.. [SEP] [PAD]',\n",
              " '[CLS] 이런.. [SEP] [PAD]',\n",
              " '[CLS] -_-// [SEP] [PAD]',\n",
              " '[CLS] 재미있는 건가... [SEP] [PAD]',\n",
              " '[CLS] 재미뿐 아니라 교훈도 얻을 수 있는 영화 [SEP] [PAD]',\n",
              " '[CLS] 늘 자극적인 것만 찾아다니는 임상수 [SEP] [PAD]',\n",
              " '[CLS] 서머셋 몸의 반전이 완벽하게 나온 영화 [SEP] [PAD]',\n",
              " '[CLS] Q. Q 좋은 영화 추천 [SEP] [PAD]',\n",
              " '[CLS] 이 정도 잠니 아래 헌터에 맞았나? [SEP] [PAD]',\n",
              " '[CLS] 바퀴벌레...ㅎㅎㅎ [SEP] [PAD]',\n",
              " '[CLS] 동요 너무 재미있어요.찬이는 무왕이었으면 좋겠습니다서동요 만세! [SEP] [PAD]',\n",
              " '[CLS] 평점을 개처럼 쓴 인간들 뭐야? 정말 추천... 재밌어 [SEP] [PAD]',\n",
              " '[CLS] 옹졸하면서도 성숙한 그의 일상 [SEP] [PAD]',\n",
              " '[CLS] 너무 유치했어요. 배우의 이미지만이 타격이죠. [SEP] [PAD]',\n",
              " '[CLS] 사실 여자는 자상한 남자가 좋다. [SEP] [PAD]',\n",
              " '[CLS] 주성치의 연기도 일품이지만 귀여운 사람들의 연기를 보는것도 즐겁습니다. [SEP] [PAD]',\n",
              " '[CLS] 막문 위의 머리는 왜 벗겨졌지? 난해한 내용이 가득한 영화 [SEP] [PAD]',\n",
              " '[CLS] 연산군의 해학을 잘 드러낸 이야기를 영화화한 것. [SEP] [PAD]',\n",
              " '[CLS] 스릴감이 너무 없다 내용은 좋았다. [SEP] [PAD]',\n",
              " '[CLS] 정리는 한국인~ [SEP] [PAD]',\n",
              " '[CLS] 지금까지 본 한국영화중 최악이야..현실은 녹록치 않다. [SEP] [PAD]',\n",
              " '[CLS] 칠 점인가 [SEP] [PAD]',\n",
              " '[CLS] 뭔가 추억이 있는 용구형님 영화.. [SEP] [PAD]',\n",
              " '[CLS] 역시 브래드 피트! 나를 미치게 한 당 호호... [SEP] [PAD]',\n",
              " '[CLS] 두 남매가 주는 인생의 감동.. [SEP] [PAD]',\n",
              " '[CLS] 신파잖아. [SEP] [PAD]',\n",
              " '[CLS] 하나도 아깝다. [SEP] [PAD]',\n",
              " '[CLS] 내 의지로 인생은 계속될까, 아니면 그 반대 일까, [SEP] [PAD]',\n",
              " '[CLS] 여자친구랑 만났는데...혼자 봐도 0점... 둘이 보면 마이너스 [SEP] [PAD]',\n",
              " '[CLS] 로리타와는 비교가 안 되는 졸작 [SEP] [PAD]',\n",
              " '[CLS] 한국인의 정서에 맞지 않는 영화랄까, [SEP] [PAD]',\n",
              " '[CLS] 하나도 심심해서 정말 재밌게 봤어요. [SEP] [PAD]',\n",
              " '[CLS] 슬픈 영상, 아름다운 음악.좋다 재밌다~ [SEP] [PAD]',\n",
              " '[CLS] ...말이 안 나오다. [SEP] [PAD]',\n",
              " '[CLS] 네이버의 무개념인지 그 반전 줄거리로 써놓는 게 어때. [SEP] [PAD]',\n",
              " '[CLS] 이게 시청률이 35%를 달리고 있다니 말도 안 돼. [SEP] [PAD]',\n",
              " '[CLS] 킬링 타임용으로는 대히트를 친 영화 진부한 스토리 [SEP] [PAD]',\n",
              " '[CLS] 와~나는 정말 좋았어요~비주얼도 강했고~ [SEP] [PAD]',\n",
              " '[CLS] 더 큰 세계를 향한 엔딩. [SEP] [PAD]',\n",
              " '[CLS] 하나의 미술 작품을 보는 듯한 아름다운 영상물의 감동적인 영화 [SEP] [PAD]',\n",
              " '[CLS] 이를 만든 뒤 감독은 현재 실종 상태다. [SEP] [PAD]',\n",
              " '[CLS] 쓰레기도 이런 쓰레기도 없다 이런 것은 재활용도 안 된다. [SEP] [PAD]',\n",
              " '[CLS] 정말 95년 영화라는 게 믿기지 않을 만큼 탄탄한 스토리입니다. [SEP] [PAD]',\n",
              " '[CLS] 괜찮은데? [SEP] [PAD]',\n",
              " '[CLS] 왜 18살이야? 조제는 15살인데? [SEP] [PAD]',\n",
              " '[CLS] 재미있는 시나리오와 음악의 환상적인 조합! [SEP] [PAD]',\n",
              " '[CLS] 멀리서 시사회를 보러 달려갔는데? 조금 실망했습니다. ㅠ [SEP] [PAD]',\n",
              " '[CLS] 그룹닐바나 얘기인줄 알았는데... [SEP] [PAD]',\n",
              " '[CLS] 기분 좋은 영화 [SEP] [PAD]',\n",
              " '[CLS] 이거 왜 그래? [SEP] [PAD]',\n",
              " '[CLS] 좋은 영화 [SEP] [PAD]',\n",
              " '[CLS] 웃음을 자아내면서 마음에 뭔가 촘촘하게 남기는 작품 [SEP] [PAD]',\n",
              " '[CLS] 자연히 스며드는 절제된 감정 우리는 큰 걸 바라지 않아 [SEP] [PAD]',\n",
              " '[CLS] 장롱 밑에 숨어있던 만 원을 찾아낸 기분이랄까? 재미있어요. [SEP] [PAD]',\n",
              " '[CLS] 꽤 재밌게 봤어 ㅋㅋ 재미있다.. [SEP] [PAD]',\n",
              " '[CLS] 오래전에 인상깊게 봤던 기억이... [SEP] [PAD]',\n",
              " '[CLS] 영상이 너무 마음에 들어 [SEP] [PAD]',\n",
              " '[CLS] 아주 재미없는 작품 [SEP] [PAD]',\n",
              " '[CLS] 너무나도 우리에게 익숙해졌다. 지나친 환상에 빠진 영화 [SEP] [PAD]',\n",
              " '[CLS] 여주인공이..꽤 괜찮은 얼굴_♪ [SEP] [PAD]',\n",
              " '[CLS] 안봐도 전혀 후회없는 영화 [SEP] [PAD]',\n",
              " '[CLS] 2편이 1편보다 훨씬 재미있다 [SEP] [PAD]',\n",
              " '[CLS] 칠레의 격동기 속에서 한 가족의 삶과 애환을 그린 영화. [SEP] [PAD]',\n",
              " '[CLS] 하하, 나에게도 두렵지 않은 공포물이 있었다니. [SEP] [PAD]',\n",
              " '[CLS] 화려한 색체가 돋보인다. [SEP] [PAD]',\n",
              " '[CLS] 그냥 아무 생각없이 보려고 했는데.. 긴장시킨 영화 [SEP] [PAD]',\n",
              " '[CLS] 우웨볼작품중 최고의 재미를 자랑한다... [SEP] [PAD]',\n",
              " '[CLS] 호화로운 화면과 배경음악, 탄탄한 시나리오로... [SEP] [PAD]',\n",
              " '[CLS] 돈을 들인 조잡한 외화보다 훨씬 낫다. 여주인공 섹시함의 극치... [SEP] [PAD]',\n",
              " '[CLS] 하하하...유쾌의 극치. [SEP] [PAD]',\n",
              " '[CLS] 네이버 평점 70% 믿을 수 있다 [SEP] [PAD]',\n",
              " '[CLS] 전에 보니까 기억이 안 나는데 임팩트 없었어. [SEP] [PAD]',\n",
              " '[CLS] 이슬람을 너무 나쁜 쪽으로 몰아넣다. 이런 영화 없어져야죠. [SEP] [PAD]',\n",
              " '[CLS] 너무 재미있어요 [SEP] [PAD]',\n",
              " '[CLS] 이영걸과 아들 ~ 둘이 잘어울린다~~ 액션양!! [SEP] [PAD]',\n",
              " '[CLS] 리얼한 제니퍼의 매력! 귀여운 여자아류중 최고! [SEP] [PAD]',\n",
              " '[CLS] 내가 나이를 먹어도 현실에서는 있을 수 없는 공감대가 형성되지 않는다. [SEP] [PAD]',\n",
              " '[CLS] 어이없어..; [SEP] [PAD]',\n",
              " '[CLS] 차태현과 김민희 덕분에 볼만한 영화 예지원은 에이~ [SEP] [PAD]',\n",
              " '[CLS] 값싼 공포영화와는 다른 의미를 찾기 쉽지 않은 영화 [SEP] [PAD]',\n",
              " '[CLS] 모르겠어. 감독님은 무슨 말을 하고 싶었는지. [SEP] [PAD]',\n",
              " '[CLS] 안 봐도 될 정도예요. [SEP] [PAD]',\n",
              " '[CLS] 그 당시 최고의 무술 영화 [SEP] [PAD]',\n",
              " '[CLS] 최고 !! [SEP] [PAD]',\n",
              " '[CLS] 아, 왜이렇게 심심해~! 나한텐 허무해~~~~ [SEP] [PAD]',\n",
              " '[CLS] 중간중간에 본 순간도 눈을 뗄 수 없었던 매력적인 영화 [SEP] [PAD]',\n",
              " '[CLS] 영화배우가 아까운 영화 같네요. [SEP] [PAD]',\n",
              " '[CLS] 이 엄청난 졸작이 이렇게 길 줄이야. [SEP] [PAD]',\n",
              " '[CLS] 다수와 소수의 일단이 섞여...우리도 이 정도면 살맛인데. [SEP] [PAD]',\n",
              " '[CLS] 탄탄한 성장영화 탄탄한 연기력 [SEP] [PAD]',\n",
              " '[CLS] 조금만 다듬으면 그래도 가슴이 아프다 [SEP] [PAD]',\n",
              " '[CLS] 지루하고 답답하다 특히 잘 맞지 않는 영어 더빙이 마음에 걸린다 [SEP] [PAD]',\n",
              " '[CLS] 양조위 시점을 두다. 참내 [SEP] [PAD]',\n",
              " '[CLS] 시간이 아깝다. [SEP] [PAD]',\n",
              " '[CLS] 어렸을때 재미있게 봤다.www. [SEP] [PAD]',\n",
              " '[CLS] 내 생애 최고의 드라마 [SEP] [PAD]',\n",
              " '[CLS] 가족의 정을 느낄 수 있는 영화 [SEP] [PAD]',\n",
              " '[CLS] 아무리 비디오용 영화라지만 이건 너무하잖아. [SEP] [PAD]',\n",
              " '[CLS] 반전견 억지 [SEP] [PAD]',\n",
              " '[CLS] 요즘 시대에는 진부하지만 비디오 시대 최고의 액션 영화 [SEP] [PAD]',\n",
              " '[CLS] 왜 이런 속편을 만들었을까. 도대체 왜! 왜! [SEP] [PAD]',\n",
              " '[CLS] 돈 내고 보기가 아깝다. [SEP] [PAD]',\n",
              " '[CLS] 어른은 NO. 아이는 나름대로 YES [SEP] [PAD]',\n",
              " '[CLS] 배우들의 연기도, 내용도 좋았어 숀빈때문에 봤어. [SEP] [PAD]',\n",
              " '[CLS] 예전에는 TV에서 많이 했었는데... 또 보고싶다~ [SEP] [PAD]',\n",
              " '[CLS] 롱테이크의 미학 빠져들다 그리고 정웅인의 재발견. [SEP] [PAD]',\n",
              " '[CLS] 비록 평작이지만 싫증나지 않는 영화 영상과 음악이 일품 [SEP] [PAD]',\n",
              " '[CLS] 뭔가 깊은 의미가 있을것 같다...그러나 재미는 없다 [SEP] [PAD]',\n",
              " '[CLS] 아, 만난지 오래돼서 잘 기억은 안나지만, 어중간한 SF랑 멜로사이;; [SEP] [PAD]',\n",
              " '[CLS] 나 예전에 이 드라마 봤는데 최진실과 송현주 연기 최고야. [SEP] [PAD]',\n",
              " '[CLS] 이가훈 씨 사랑해요. [SEP] [PAD]',\n",
              " '[CLS] 나는 재밌었지만 마지막에 감동(?)정도.. [SEP] [PAD]',\n",
              " '[CLS] 0점은 없어?~~~ [SEP] [PAD]',\n",
              " '[CLS] 기존의 가치관을 뒤흔드는 드라마 [SEP] [PAD]',\n",
              " '[CLS] 최고 [SEP] [PAD]',\n",
              " '[CLS] 왜 평점이 높은지 이해가 안되네.난 좀 거역하더라. [SEP] [PAD]',\n",
              " '[CLS] 생각하려다 중심을 잃은 영화 [SEP] [PAD]',\n",
              " '[CLS] 경악을 금치 못했다. 이런 빌어먹을... [SEP] [PAD]',\n",
              " '[CLS] 팀 로빈스가 왜 이런 영화에 나오는지. 네 생각이나 해. [SEP] [PAD]',\n",
              " '[CLS] 망외의 즐거움 마지막 반전이 대단했어요. [SEP] [PAD]',\n",
              " '[CLS] 매직키드 마슬리 성인 버전? [SEP] [PAD]',\n",
              " '[CLS] 서울 공략 +2 이것뿐이다. [SEP] [PAD]',\n",
              " '[CLS] 브랜든리의 죽음을 헛되게 만든 속편들 [SEP] [PAD]',\n",
              " '[CLS] 브랜든리의 죽음을 헛되게 만든 속편들 [SEP] [PAD]',\n",
              " '[CLS] 재밌어요~! [SEP] [PAD]',\n",
              " '[CLS] 당시 최고 인기스타 이승희 이름 하나로 만든 영화 [SEP] [PAD]',\n",
              " '[CLS] 지루하게 느껴질 수도 있지만, 많은 것을 시사하는 영화 [SEP] [PAD]',\n",
              " '[CLS] 쓰레기가 무엇인지 보여 주는 영화 [SEP] [PAD]',\n",
              " '[CLS] 사랑하는 것은 분명 무모한 일이다. 그러나 사랑한다. [SEP] [PAD]',\n",
              " '[CLS] 어이없어.. 칸리나가 아까워 [SEP] [PAD]',\n",
              " '[CLS] 이해하기 어려운 정서 [SEP] [PAD]',\n",
              " '[CLS] 나이가 드니 이상한 영화만 만드네.진짜 비출 것 같아! [SEP] [PAD]',\n",
              " '[CLS] 감독 스타일 독특한 히로인에 기발한 발상 식상 전개법 [SEP] [PAD]',\n",
              " '[CLS] 그냥 아무렇지도않고 최고야 [SEP] [PAD]',\n",
              " '[CLS] 오 재밌어요. (주) [SEP] [PAD]',\n",
              " '[CLS] 돌프룬드그렌 영화치고는 수작;; 그래도 스토리가 있는 액션... [SEP] [PAD]',\n",
              " '[CLS] 1점 감점은 연기자 안보가 부족한 스태프들 때문에-나머지는 엑설런트 [SEP] [PAD]',\n",
              " '[CLS] 하지만 더 좋다. [SEP] [PAD]',\n",
              " '[CLS] 이자벨아자니는증말연기파~! [SEP] [PAD]',\n",
              " '[CLS] 마을 사람들의 집단 세뇌를 그린 영화 이게 정말 그녀의 순수한 결정? [SEP] [PAD]',\n",
              " '[CLS] 잔잔한 여운과 함께 아름다움까지. 언제 봐도 멋진 영화 [SEP] [PAD]',\n",
              " '[CLS] 재기가 번득이는 개의 뿔 [SEP] [PAD]',\n",
              " '[CLS] 어떻게 생겼어? 말대루 1점도 아까워? DVD로 샀는데, 장르가 너무 에메하네 [SEP] [PAD]',\n",
              " '[CLS] 열강의 시선, 명분 쌓기, 조드슨(캐릭터)의 모호함, 위선, 졸작. [SEP] [PAD]',\n",
              " '[CLS] 임달화의 매력을 발산한 절정 액션 느와르. [SEP] [PAD]',\n",
              " '[CLS] 허접 [SEP] [PAD]',\n",
              " '[CLS] 아, 정말 재미없어요. 비유 [SEP] [PAD]',\n",
              " '[CLS] 별루... [SEP] [PAD]',\n",
              " '[CLS] 한국영화 암흑기의 얼굴 전영록. [SEP] [PAD]',\n",
              " '[CLS] 영화보다 줄거리 읽는게 더 재밌으니까.... [SEP] [PAD]',\n",
              " '[CLS] 3부작 중에서 가장 떨어진다.맞아. [SEP] [PAD]',\n",
              " '[CLS] 너무 좋아요! [SEP] [PAD]',\n",
              " '[CLS] 눈물나라고요 호호 눈물이 나오래요 [SEP] [PAD]',\n",
              " '[CLS] 이전 작품보다 훨씬 더 멋진 최고급 코미디 영화 [SEP] [PAD]',\n",
              " '[CLS] 소재가 새로 샀어... [SEP] [PAD]',\n",
              " '[CLS] 이걸 보고나서 다른 영화보면 다 재밌게 볼 수 있을 것 같아 -----↓ [SEP] [PAD]',\n",
              " '[CLS] 잔잔하게 재는군요. [SEP] [PAD]',\n",
              " '[CLS] 왜 0점은 없을까? [SEP] [PAD]',\n",
              " '[CLS] 한국 영화의 한계 아래 シェル 위 댄스를 보는 듯한 느낌. [SEP] [PAD]',\n",
              " '[CLS] 진정한 흥분과 감동을 주는 영화.. [SEP] [PAD]',\n",
              " '[CLS] 축차 식상 [SEP] [PAD]',\n",
              " '[CLS] 역시 34번가보다 8번가의 기적이다. [SEP] [PAD]',\n",
              " '[CLS] 미국 아랍 모두 패배자가 될 것이다.나쁜 놈들(영환 보통 수준) [SEP] [PAD]',\n",
              " '[CLS] 흥분한 황소 진항생 [SEP] [PAD]',\n",
              " '[CLS] 1부에서 끝냈어야 했다. [SEP] [PAD]',\n",
              " '[CLS] 유치한걸 좋아해서 10점인데 10점! 근데 딸아이는 무슨일이야? [SEP] [PAD]',\n",
              " '[CLS] 아, 벌써 시간 버렸어. [SEP] [PAD]',\n",
              " '[CLS] 잠-ㅁ-다. 보고나서 다른 일을 했다-ㅅ-다 [SEP] [PAD]',\n",
              " '[CLS] 이 영화 볼 시간에 자자. [SEP] [PAD]',\n",
              " '[CLS] it might be you [SEP] [PAD]',\n",
              " '[CLS] 별난 줄 알았더니 흑백이네~공돌이와 공순이 아무 내용이 없다. [SEP] [PAD]',\n",
              " '[CLS] 됐어. [SEP] [PAD]',\n",
              " '[CLS] 너무 재미있네요. [SEP] [PAD]',\n",
              " '[CLS] 통근버스에서 본 영화 아침부터 기분 나빠~ [SEP] [PAD]',\n",
              " '[CLS] 처음에는 조금 긴장감이 있고, 처절하게 무너지는 후반. [SEP] [PAD]',\n",
              " '[CLS] 무슨 영화일까 [SEP] [PAD]',\n",
              " '[CLS] 진짜 재미없어. [SEP] [PAD]',\n",
              " '[CLS] 영화란 매체가 존재하는 한 영원히 기억될 진정한 걸작. [SEP] [PAD]',\n",
              " '[CLS] 재미있게 봤는데 평점이 왜 이러니? [SEP] [PAD]',\n",
              " '[CLS] 1, 2, 3편에 비해 뚝 떨어지는 재미! 정식 4편 인정을 망설이게 한다. [SEP] [PAD]',\n",
              " '[CLS] 처키 대단하네요. [SEP] [PAD]',\n",
              " '[CLS] 영화관을 일주일에 세 번 드나들며 본 영화 [SEP] [PAD]',\n",
              " '[CLS] 인생을 정말로 행복하게 하는 것은 세상의 모든 것과 따뜻한 관계에 있다 [SEP] [PAD]',\n",
              " '[CLS] 주인공의 절박한 심정이 무엇보다 몸에 밴 영화 [SEP] [PAD]',\n",
              " '[CLS] 이해 못하는 사람이 너무 많아. [SEP] [PAD]',\n",
              " '[CLS] 감동적인 저예산 영화. 울면서 웃으면서 가슴에 전해져온 영화. [SEP] [PAD]',\n",
              " '[CLS] 바람난 가족+조용한 가족... [SEP] [PAD]',\n",
              " '[CLS] 그냥 링의 내용을 설명하는 그런 스토리... [SEP] [PAD]',\n",
              " '[CLS] 아! 이 영화 찾고있었어요.재미있게 본 영화 [SEP] [PAD]',\n",
              " '[CLS] 에로영화일 뿐 평점이 너무 높다. [SEP] [PAD]',\n",
              " '[CLS] 영심아!! 또 만나고 싶어요. ㅜㅜ [SEP] [PAD]',\n",
              " '[CLS] 말론 브랜도 같은 명배우들의 출연은 힘든 졸작 [SEP] [PAD]',\n",
              " '[CLS] 이런 느낌은 별로... [SEP] [PAD]',\n",
              " '[CLS] 조금만 집중해서 보면 굉장히 심오한 영화다..! [SEP] [PAD]',\n",
              " '[CLS] 속편보다 못한 최고의 영화 [SEP] [PAD]',\n",
              " '[CLS] 자유로움이 느껴지는 멋진 영화... [SEP] [PAD]',\n",
              " '[CLS] 더러워서 못 볼 영화 [SEP] [PAD]',\n",
              " '[CLS] 앗!! 결말을 못봤어!!! [SEP] [PAD]',\n",
              " '[CLS] 기대하지 않았는데 감동♡로리페티 너무 좋아! [SEP] [PAD]',\n",
              " '[CLS] 유치하네... [SEP] [PAD]',\n",
              " '[CLS] 재미로는 크림슨 리버2를 능가 [SEP] [PAD]',\n",
              " '[CLS] 아직 신데렐라? [SEP] [PAD]',\n",
              " '[CLS] 1편과 크게 다른 내용은 없다. 역시 별로야 [SEP] [PAD]',\n",
              " '[CLS] 멋진 영화다..두근두근 타카코 역시 예쁘다 -_-; [SEP] [PAD]',\n",
              " '[CLS] 당시에는 특수효과가 멋져 보였지만 다시 보니 이건 아니다라는 생각이 든다. [SEP] [PAD]',\n",
              " '[CLS] 영상이 너무 아름다웠어요탱고의 선율도 아주 좋았다. [SEP] [PAD]',\n",
              " '[CLS] 서세원은 사망자 명단 이름 가운데 하나라고 한다. [SEP] [PAD]',\n",
              " '[CLS] 항상 기다리던 내 중3시절을 생각나게 해. [SEP] [PAD]',\n",
              " '[CLS] 이럴 수 밖에 없었던 것일까...꼭 이래야만 했던 걸까...( ?;ω;`) [SEP] [PAD]',\n",
              " '[CLS] 뻔한 스토리에 뻔한 결말을 보는 내내 너무 지루했다. [SEP] [PAD]',\n",
              " '[CLS] TV에 나왔는데 고민이야 [SEP] [PAD]',\n",
              " '[CLS] 영상 누드집 [SEP] [PAD]',\n",
              " '[CLS] 저스틴 그만해. [SEP] [PAD]',\n",
              " '[CLS] 전원일기가 좋다 [SEP] [PAD]',\n",
              " '[CLS] 오오..전혀 흥미진진하구요.. 흐름도 뭔가.. [SEP] [PAD]',\n",
              " '[CLS] 테러리스트 제2탄은 3점씩....... [SEP] [PAD]',\n",
              " '[CLS] 나나코, 완전 내 이상형. @@소리마치..나쁜 녀석.. [SEP] [PAD]',\n",
              " '[CLS] 장난치는 듯이 [SEP] [PAD]',\n",
              " '[CLS] 공포영화를 보면서 존망은 처음이에요. [SEP] [PAD]',\n",
              " '[CLS] 노래 좋다. [SEP] [PAD]',\n",
              " '[CLS] TV로 보기엔 너무 아까운 영화..다시 한번 보고 싶어.. [SEP] [PAD]',\n",
              " '[CLS] 국민배우 박중훈!!! [SEP] [PAD]',\n",
              " '[CLS] 9점 시대를 앞지른 영화 [SEP] [PAD]',\n",
              " '[CLS] 영화, 가족의 소중함을 느끼다. [SEP] [PAD]',\n",
              " '[CLS] 이거는 동화책만 봐도 알 수 있는데... [SEP] [PAD]',\n",
              " '[CLS] 정말 재미있네요. [SEP] [PAD]',\n",
              " '[CLS] 시간이 다 되었다... [SEP] [PAD]',\n",
              " '[CLS] 볼 것이 많을수록 흉내를 내는 영화의 발전은 없다. [SEP] [PAD]',\n",
              " '[CLS] 내이름은 김삼순이야. [SEP] [PAD]',\n",
              " '[CLS] 우연히 봤지만 눈과 코가 찡했다 특히 노모와 아들의... [SEP] [PAD]',\n",
              " '[CLS] 다큐멘터리를 입김하는 힘은 진실 [SEP] [PAD]',\n",
              " '[CLS] 이지현의 몸만 볼 게 없었다. 내용이뭐냐? [SEP] [PAD]',\n",
              " '[CLS] 여주인공 때문에 10점 [SEP] [PAD]',\n",
              " '[CLS] 종교적인 위대한 애관을 보여줬다. 내 생에서 가장 아름다운 작품 [SEP] [PAD]',\n",
              " '[CLS] 다세포보다는 괜찮아. 하지만 최악. [SEP] [PAD]',\n",
              " '[CLS] 무슨 병인가 [SEP] [PAD]',\n",
              " '[CLS] 너무 웃겼어. 어디서 다운 받지? dvd라도 갖고 싶다 [SEP] [PAD]',\n",
              " '[CLS] 후후후 [SEP] [PAD]',\n",
              " '[CLS] 조니デ 기다려 주세요. [SEP] [PAD]',\n",
              " '[CLS] 짜증나... [SEP] [PAD]',\n",
              " '[CLS] 이건 또 뭐야? [SEP] [PAD]',\n",
              " '[CLS] 새벽에 눈을 깜박여 주는 영화 [SEP] [PAD]',\n",
              " '[CLS] 이런 영화... 제발! 안 돼요 [SEP] [PAD]',\n",
              " '[CLS] 지나 데이비스 굉장하다 [SEP] [PAD]',\n",
              " '[CLS] 10점을 안 줄 수 없는 드라마 [SEP] [PAD]',\n",
              " '[CLS] 냉기가 느껴지는 영화 [SEP] [PAD]',\n",
              " '[CLS] 이것이 영화라고 할까.다세포선생 [SEP] [PAD]',\n",
              " '[CLS] 어렸을때 감명받았던 영화로 얼마전 소장하게 되어 기쁘다... [SEP] [PAD]',\n",
              " '[CLS] 다세포를 생각하세요.제군 [SEP] [PAD]',\n",
              " '[CLS] 미사와 비교가 되지 않는다.내용이 엉망이여서 배우들의 감정몰입이... [SEP] [PAD]',\n",
              " '[CLS] 눈물겨운 감동적 영화 최고 [SEP] [PAD]',\n",
              " '[CLS] 다가갈때까지 미국아이들만 보고 천사를 보는것같은... [SEP] [PAD]',\n",
              " '[CLS] 정말 재밌어 good! [SEP] [PAD]',\n",
              " '[CLS] 아무튼 진짜 쓰레기. [SEP] [PAD]',\n",
              " '[CLS] 아름다운 영상과 슬픈 음악 모두가 좋았던 영화. [SEP] [PAD]',\n",
              " '[CLS] 원작을 철저히 망쳐버린 영화.. [SEP] [PAD]',\n",
              " '[CLS] 정말 재미있었던 드라마 [SEP] [PAD]',\n",
              " '[CLS] 좋군 [SEP] [PAD]',\n",
              " '[CLS] 록산느 메스키다에서만 평점 7은 줄 영화 [SEP] [PAD]',\n",
              " '[CLS] 거짓말이지만 애당초 먹고 산다. [SEP] [PAD]',\n",
              " '[CLS] 아줌마가 된 최진실!! 최고 !! [SEP] [PAD]',\n",
              " '[CLS] 수많은 여로와 처절한 바이올린 선율이 심금을 울리는 영화 [SEP] [PAD]',\n",
              " '[CLS] 영화를 보니 나도 날고 싶었다. 예쁜 주인공 같은 여자랑 [SEP] [PAD]',\n",
              " '[CLS] 웃겨~ [SEP] [PAD]',\n",
              " '[CLS] 너무 재미있어요. [SEP] [PAD]',\n",
              " '[CLS] 관계를 맺는 여성적인 상처. 그럼에도 불구하고 인생은 계속된다. [SEP] [PAD]',\n",
              " '[CLS] 요즘 공포영화 고전으로 벤치마킹해~! [SEP] [PAD]',\n",
              " '[CLS] 이용, 죽여버리고 싶다. [SEP] [PAD]',\n",
              " '[CLS] 이런 영화를 위해 진지하게 연기하는 배우들이 불쌍하다. [SEP] [PAD]',\n",
              " '[CLS] 착취당하는 구조와 그것을 망각하는 인간들. [SEP] [PAD]',\n",
              " '[CLS] 초절정 순수 [SEP] [PAD]',\n",
              " '[CLS] 정말로 아내... 봐버렸다... [SEP] [PAD]',\n",
              " '[CLS] 여배우 연기 최악 [SEP] [PAD]',\n",
              " '[CLS] 갑자기 끝나서 아쉽지만... [SEP] [PAD]',\n",
              " '[CLS] 처음에는 재밌으려나?? 했지만... 너무 재밌게 봤어요 슌!!!! [SEP] [PAD]',\n",
              " '[CLS] 연기 너무 잘하는 거 아니야? [SEP] [PAD]',\n",
              " '[CLS] 교묘히 전개하는 영화테마가 좋아서 가능. [SEP] [PAD]',\n",
              " '[CLS] 많이 좋아해요!! 앞으로 당분간 이보다 좋은 드라마는 못 볼 것 같아 [SEP] [PAD]',\n",
              " '[CLS] 정말 너무 재밌어요.문정혁이 실감나는 연기는 너무 재미있어요. [SEP] [PAD]',\n",
              " '[CLS] 브라보! 흐흐흐 웃음이 나와... [SEP] [PAD]',\n",
              " '[CLS] 우연히 TV에서 봤는데 너무 재밌었어요.진광희씨 정말좋아~>ㅁㅁ [SEP] [PAD]',\n",
              " '[CLS] 드라마같이... [SEP] [PAD]',\n",
              " '[CLS] 아래쪽 코멘트에 동감↓ [SEP] [PAD]',\n",
              " '[CLS] 짝사랑의 느낌은 좋았지만, 붙잡을 것은 없다. [SEP] [PAD]',\n",
              " '[CLS] 멋지다! 박중훈의 매직막 미소가 최고야! [SEP] [PAD]',\n",
              " '[CLS] 너무 짜증나.카메라맨 거. 영화를 망쳤어. [SEP] [PAD]',\n",
              " '[CLS] 밴덤이 찍은 액션 영화 중 최고 수준인데? [SEP] [PAD]',\n",
              " '[CLS] 나는 펠레가 언제 축구를 하는지 궁금해서 본 것.... [SEP] [PAD]',\n",
              " '[CLS] 전매특허인 코믹 연기도 아니고 진지한 것도 아니고 중간도 아니고. [SEP] [PAD]',\n",
              " '[CLS] 요즘 영화 고르는 기술 최악이네. 재미없는 영화였어요. [SEP] [PAD]',\n",
              " '[CLS] 아쉽게 묻혀버린 한국형 서스펜스 스릴러 [SEP] [PAD]',\n",
              " '[CLS] 양심.. 신념.. 지키면서 사는게 얼마나 어려운지.. [SEP] [PAD]',\n",
              " '[CLS] 제작비 5백만 달러로 무엇을 만드나요. 아 포스터에 낚였어 [SEP] [PAD]',\n",
              " '[CLS] 전설의 드라마다. [SEP] [PAD]',\n",
              " '[CLS] 늘 그렇듯이... [SEP] [PAD]',\n",
              " '[CLS] 평점이 높아지니 죄송합니다! ㅋ [SEP] [PAD]',\n",
              " '[CLS] 아주 멋진 영화로 기억한다. 옛날에 봤는데 [SEP] [PAD]',\n",
              " '[CLS] 내가 본 영화 중 최악의 영화 [SEP] [PAD]',\n",
              " '[CLS] 소설을 읽었다면 결코 좋은 평가가 나올 수 없다. [SEP] [PAD]',\n",
              " '[CLS] 위대한 투사의 비극적 결말 ~ 슬퍼도 가슴 벅차! [SEP] [PAD]',\n",
              " '[CLS] 사람을 재우기에 충분했습니다 [SEP] [PAD]',\n",
              " '[CLS] 이거 기억하시는 분 없나요? 옛날에 많이 해줬었는데? [SEP] [PAD]',\n",
              " '[CLS] 진부하지만 여배우를 보는 재미가 있다 [SEP] [PAD]',\n",
              " '[CLS] 아무것도 모르는 상태에서 보았지만 몰입해 버리는 영화였다.반전도 굿 [SEP] [PAD]',\n",
              " '[CLS] 쟈무나이~~ [SEP] [PAD]',\n",
              " '[CLS] 결론은 아방근에서 인육을 사용한 거예요. 이건 진짜 최고야! [SEP] [PAD]',\n",
              " '[CLS] 안성기, 수많은 그의 작품 중 이 영화에서 가장 빛났다 [SEP] [PAD]',\n",
              " '[CLS] 차라리 무적의 낙하산 요원 황진이가 낫다. [SEP] [PAD]',\n",
              " '[CLS] 대단한 급반전 [SEP] [PAD]',\n",
              " '[CLS] 이거 뭐야..-_- 안만들었으면 더 좋았을텐데.. [SEP] [PAD]',\n",
              " '[CLS] 운명의 장난어쩔 수 없는 불가항력 같다. [SEP] [PAD]',\n",
              " '[CLS] 재미없어 [SEP] [PAD]',\n",
              " '[CLS] 한 점이나 속다 [SEP] [PAD]',\n",
              " '[CLS] 미국이 우리를 보는 시각을 안다면, 거기에 맞추어 행동하자 [SEP] [PAD]',\n",
              " '[CLS] 이것은 공포 영화가 아니다. [SEP] [PAD]',\n",
              " '[CLS] 원작 망친 쓰레기 어쩔 수 없는 할리우드는 한심하다 [SEP] [PAD]',\n",
              " '[CLS] 맷 딜런이 수용소에서 죽는 장면을 잊을 수가 없습니다. [SEP] [PAD]',\n",
              " '[CLS] 감동적이긴 한데 너무나도 뻔한 스토리 [SEP] [PAD]',\n",
              " '[CLS] 시사회에서 보고 반해버린 영화였어요. [SEP] [PAD]',\n",
              " '[CLS] 뭐야 이거--다세포보다 평이 더 낮아ㅋㅋㅋ [SEP] [PAD]',\n",
              " '[CLS] ㅋㅋㅋ 비아그라 어디있어? 장난하는 거야? [SEP] [PAD]',\n",
              " '[CLS] 잘되니 점점 이상해지네요. [SEP] [PAD]',\n",
              " '[CLS] 캐스팅 민망할수록 뭔가 부족한 영화 [SEP] [PAD]',\n",
              " '[CLS] 최고의 드라마...설공찬 너무 멋있어요 [SEP] [PAD]',\n",
              " '[CLS] 아주 실패적인 리메이크 주연 배우들이 아깝다 [SEP] [PAD]',\n",
              " '[CLS] 유머러스함과 감동의 조합 [SEP] [PAD]',\n",
              " '[CLS] 많은 생각을 가지고 있던 뜻깊고 슬픈 아름다운 만화 [SEP] [PAD]',\n",
              " '[CLS] 실화인 줄 모르고 봤는데 실화였군요. [SEP] [PAD]',\n",
              " '[CLS] 당대의 수작 [SEP] [PAD]',\n",
              " '[CLS] 이것을 영화라고 포스터부터 압박적이다. [SEP] [PAD]',\n",
              " '[CLS] 처음봤을때 충격 굉장했어요... [SEP] [PAD]',\n",
              " '[CLS] 진부한 신데렐라 영화 평점이 높은게 아무래도 이해가... [SEP] [PAD]',\n",
              " '[CLS] 알고 싶지도 않다.이런 쓸데없는 비행은... [SEP] [PAD]',\n",
              " '[CLS] 닉슨을 죽이기 전에 사람을 재우는 영화다. [SEP] [PAD]',\n",
              " '[CLS] 걸작이다 [SEP] [PAD]',\n",
              " '[CLS] 심심해서 재미가 없다. [SEP] [PAD]',\n",
              " '[CLS] 깡패 세계만큼 욕만 하는 곳이 정치판이 아닐까. [SEP] [PAD]',\n",
              " '[CLS] 네~ [SEP] [PAD]',\n",
              " '[CLS] 나에게 wake up! 이라고 외쳐준 영화 [SEP] [PAD]',\n",
              " '[CLS] 민정씨가 나온것만으로도 충분하다!! [SEP] [PAD]',\n",
              " '[CLS] 네이버 별점 꽤 신뢰도가 높네요.5점 이하는 안 보는 게 좋을 것 같아. [SEP] [PAD]',\n",
              " '[CLS] 대단하다...감독의 능력...이렇게도 만들수 있구나...감탄하다... [SEP] [PAD]',\n",
              " '[CLS] 실화라는 이유로 어설픈 남성 판타지즘을 낳다. [SEP] [PAD]',\n",
              " '[CLS] 삼색 시리즈 중 가장 웰메이드.진정한 자유와 슬픔, 그리고 해방 [SEP] [PAD]',\n",
              " '[CLS] 재미있다♡ [SEP] [PAD]',\n",
              " '[CLS] 가슴이 울렁거려, 하지만 눈을 뗄수없는 중독성... [SEP] [PAD]',\n",
              " '[CLS] 나한테는 싸이코 영화로만 보여... [SEP] [PAD]',\n",
              " '[CLS] 이 영화로 주성치 아시아태평양영화제 남우주연상을 수상했다. [SEP] [PAD]',\n",
              " '[CLS] 꽤 재미있네~ 웃기는 애니메이션~ [SEP] [PAD]',\n",
              " '[CLS] 아키.. [SEP] [PAD]',\n",
              " '[CLS] 어떻게떨어지느냐는중요하지않다. [SEP] [PAD]',\n",
              " '[CLS] 마이크 마이어스의 1인 4역은 훌륭했지만, 그 뿐이야. [SEP] [PAD]',\n",
              " '[CLS] 꺼져 [SEP] [PAD]',\n",
              " '[CLS] 역시 돈으로 괜찮아! [SEP] [PAD]',\n",
              " '[CLS] 이거 일본영화 러브레터가 아니래. [SEP] [PAD]',\n",
              " '[CLS] 아, XX내의 아까운 롯데포인트. 포인트도 아쉽다. [SEP] [PAD]',\n",
              " '[CLS] 지루하고 교훈적인 데다가 결말까지 모호하게 해서는 곤란하다. [SEP] [PAD]',\n",
              " '[CLS] 굳이 베리 강 [SEP] [PAD]',\n",
              " '[CLS] 정말 최고의 댄스 영화! 짝! 짝! 짝! 신고 [SEP] [PAD]',\n",
              " '[CLS] 마음에 와닿는 영화 [SEP] [PAD]',\n",
              " '[CLS] 최고 [SEP] [PAD]',\n",
              " '[CLS] 꿈에 대해 다시 생각해 본 영화 [SEP] [PAD]',\n",
              " '[CLS] 난 이거 넘쳐봤는데... 송룡아 씨, 70세까지 영화 찍어요! [SEP] [PAD]',\n",
              " '[CLS] 감동적이고 전혀 지루하지 않고 감독의 재발견이죠. [SEP] [PAD]',\n",
              " '[CLS] 재미있어요. 카루조가 이 영화에 스타로 마이애미도 나왔어요. [SEP] [PAD]',\n",
              " '[CLS] 그들의 삶만큼만 집착하는 방식 [SEP] [PAD]',\n",
              " '[CLS] 어린이뿐만 아니라 어른도 즐길 수 있는 가족영화 [SEP] [PAD]',\n",
              " '[CLS] 거품처럼 커졌을 뿐 로맨틱 코미디 영화의 매력은 하나도 없다. [SEP] [PAD]',\n",
              " '[CLS] 민주주의, 자유, 그리고 자본. [SEP] [PAD]',\n",
              " '[CLS] 원작에 충실하긴 했지만...이토준지씨 화낼것같다 [SEP] [PAD]',\n",
              " '[CLS] 뭐.. [SEP] [PAD]',\n",
              " '[CLS] 실소와 함께 따뜻한 미소가 떠오른다. [SEP] [PAD]',\n",
              " '[CLS] 어휴, 곤란한데. [SEP] [PAD]',\n",
              " '[CLS] 마치 탐정영화를 보는 것 같은 이야기 구성이 좋군요. [SEP] [PAD]',\n",
              " '[CLS] 맞아, 이런 영화도 있었지. [SEP] [PAD]',\n",
              " '[CLS] 볼 만하지만 홍콩영화의 한계다. [SEP] [PAD]',\n",
              " '[CLS] ##심은하가 왜 저러는 거야? [SEP] [PAD]',\n",
              " '[CLS] 설경구의 다른 매력 [SEP] [PAD]',\n",
              " '[CLS] 볼 만한 영화 [SEP] [PAD]',\n",
              " '[CLS] 어렸을 때 읽었던 책 내용을 떠올리면서 정말 재밌게 봤어요.^^* [SEP] [PAD]',\n",
              " '[CLS] 진짜 영화는 개잖아 없음--사람 아무도 없고 재미없으니까 그대로 나오다 [SEP] [PAD]',\n",
              " '[CLS] 여자가 보면 재미있을지도... [SEP] [PAD]',\n",
              " '[CLS] 내 인생 30년 최고의 영화예요. 역시 시걸영화.. [SEP] [PAD]',\n",
              " '[CLS] 내가 최고로 꼽히는 영화 중 하나. [SEP] [PAD]',\n",
              " '[CLS] 후하게 연주자 [SEP] [PAD]',\n",
              " '[CLS] 지금 재봉해도 통용되는 영화 [SEP] [PAD]',\n",
              " '[CLS] 영화에 비해 너무 높은 평점이다 [SEP] [PAD]',\n",
              " '[CLS] 홀딱 반한 영화 제레미는 나이가 들어도 너무 섹시해. [SEP] [PAD]',\n",
              " '[CLS] 1점 줄 거면 평가를 해야 되니까 재밌네. [SEP] [PAD]',\n",
              " '[CLS] 오웬윌슨꺼라면 무조건 최고!!! [SEP] [PAD]',\n",
              " '[CLS] 소재는 좋지만 스릴의 밀도는 현저히 떨어진다 [SEP] [PAD]',\n",
              " '[CLS] 아픔을 겪지만 굴하지 않고 새로운 삶을 살아간다. [SEP] [PAD]',\n",
              " '[CLS] 뻔한 이야기인 줄 알았던 영화지만 그 감동은 영원하다. [SEP] [PAD]',\n",
              " '[CLS] 원작의 전율은커녕 평범한 스릴러에도 못 미치는 작품. [SEP] [PAD]',\n",
              " '[CLS] 은행털이가 장난이라고 생각해라 [SEP] [PAD]',\n",
              " '[CLS] 깨닫는 사람, 깨닫지 못하는 사람. 현실 속의 이야기 [SEP] [PAD]',\n",
              " '[CLS] 과감한 노출 장면, 그러나 어설픈 메시지는 모냐 [SEP] [PAD]',\n",
              " '[CLS] 엔딩 하나 멋있네. [SEP] [PAD]',\n",
              " '[CLS] 배드씬이 꽤 웃기게 되었네. [SEP] [PAD]',\n",
              " '[CLS] 액션 제목만 빼면 되는데 [SEP] [PAD]',\n",
              " '[CLS] 잘 만들어졌지만 공포가 두렵지 않다면.... [SEP] [PAD]',\n",
              " '[CLS] 왜 이걸 비디오로 빌려봤을까? [SEP] [PAD]',\n",
              " '[CLS] 좋은 줄거리에 좋은 배우임에도 불구하고, 보잘것 같다 [SEP] [PAD]',\n",
              " '[CLS] 알맞을 것 같은 [SEP] [PAD]',\n",
              " '[CLS] 먼저 보라고 느낄 것이다. [SEP] [PAD]',\n",
              " '[CLS] 홍길동 찍으려면 판타지적으로 가야지 [SEP] [PAD]',\n",
              " '[CLS] 이 영화도 어렸을 때 굉장히 감동적이었어. [SEP] [PAD]',\n",
              " '[CLS] 쟈밈 [SEP] [PAD]',\n",
              " '[CLS] 유치해 [SEP] [PAD]',\n",
              " '[CLS] 꽤 지루하다... 특히 마키 라이언이... 보다가 消した다. [SEP] [PAD]',\n",
              " '[CLS] 기대해봤지만 기대이하에요. [SEP] [PAD]',\n",
              " '[CLS] 襲. [SEP] [PAD]',\n",
              " '[CLS] 영화가 끝나면 눈물을 흘리며 기립박수를 했던 기억이 난다.ㅅㅂ [SEP] [PAD]',\n",
              " '[CLS] >_ [SEP] [PAD]',\n",
              " '[CLS] 페미니즘이 아니라 노골적인 남자 깎아내리기... [SEP] [PAD]',\n",
              " '[CLS] 어른들은 이해못하지만 십대인 나는 이해공감했어... [SEP] [PAD]',\n",
              " '[CLS] 자기 신념, 자기 신앙이 얼마나 아름다운지를 보여 주는 영화 [SEP] [PAD]',\n",
              " '[CLS] 이거 1리터에 밀리다니 말도 안 돼. [SEP] [PAD]',\n",
              " '[CLS] 재밌기만 했어요 저는 [SEP] [PAD]',\n",
              " '[CLS] 매번 변하지 않는 스토리 멜깁슨표 액션 작품 팬들은 봐도 좋을 것 같아. [SEP] [PAD]',\n",
              " '[CLS] 옥빈이 니코틴 부족하니까 촬영하기 전에 섭취하고 와. [SEP] [PAD]',\n",
              " '[CLS] 영화 잘 만들었어-ㅅ-a [SEP] [PAD]',\n",
              " '[CLS] 짱이다 [SEP] [PAD]',\n",
              " '[CLS] 만화보단 이 영화로 로빈훗을 알았어 ㅋㅋ [SEP] [PAD]',\n",
              " '[CLS] 왜이렇게 유치한거지... [SEP] [PAD]',\n",
              " '[CLS] 최고의 캐스팅과 최고의 지루함 [SEP] [PAD]',\n",
              " '[CLS] 지친 가슴과 몹시 불쾌하게 망가진 얼굴, 온데간데없이 늙어버렸다. [SEP] [PAD]',\n",
              " '[CLS] 토쿄 나의 돈 [SEP] [PAD]',\n",
              " '[CLS] 재밌네요. 부그 목소리 마틴 좋아요. [SEP] [PAD]',\n",
              " '[CLS] 아무리 이유가 그럴듯해도 불륜은 불륜이다. [SEP] [PAD]',\n",
              " '[CLS] 굿도 [SEP] [PAD]',\n",
              " '[CLS] 스타의 이름이 아까운 영화 [SEP] [PAD]',\n",
              " '[CLS] 오래간만에 몰입해서 본 영화 스토리도 배우도 연기도 만족. [SEP] [PAD]',\n",
              " '[CLS] 로맨틱하지도 않고 코미디도 아니고 중간에 나오고 싶었던 영화 [SEP] [PAD]',\n",
              " '[CLS] 옛날에 봤을 때도 재미없었어. [SEP] [PAD]',\n",
              " '[CLS] 이거 보고 에어플레인 찾아서 보고 더 울었다. 너무 웃겨서. [SEP] [PAD]',\n",
              " '[CLS] 카메라 앵글을 마음대로 잡느냐?눈이 아프고 머리가 아파.제크일... [SEP] [PAD]',\n",
              " '[CLS] 영화보다 나온건 처음이야( ?;ω;`) [SEP] [PAD]',\n",
              " '[CLS] 제목을 만든 사람을 죽여라. [SEP] [PAD]',\n",
              " '[CLS] 고개를 끄덕이자 [SEP] [PAD]',\n",
              " '[CLS] 야호! 1등 나 [SEP] [PAD]',\n",
              " '[CLS] 18세 관람가라고. [SEP] [PAD]',\n",
              " '[CLS] 나는 해피엔딩을 좋아하지만 아주 슬픈 영화이다 [SEP] [PAD]',\n",
              " '[CLS] 마약중독으로 폐인이 되어 암환자로 가난하게 산대요. [SEP] [PAD]',\n",
              " '[CLS] 어떻게 이런 애니메이션이 50만 관객을 동원할 수 있었지? [SEP] [PAD]',\n",
              " '[CLS] good [SEP] [PAD]',\n",
              " '[CLS] 감독의 연출이 부재중이니 좋은 소재를 못쓰게 되어 버리다. [SEP] [PAD]',\n",
              " '[CLS] 화렐, 머피를 너무 좋아하는 배우인데..영화는 정말로 [SEP] [PAD]',\n",
              " '[CLS] 잭 블랙은 재미있었어. [SEP] [PAD]',\n",
              " '[CLS] 어렸을 때 극장에서 보던 영화네요. 18세 이상 관람 가.; [SEP] [PAD]',\n",
              " '[CLS] 신난다. 유쾌한 노랫소리. 주인공들의 코믹한 연기 굿 굿 굿 [SEP] [PAD]',\n",
              " '[CLS] 괜찮았어. [SEP] [PAD]',\n",
              " '[CLS] [오피니언] 칠득이와 만득이... 지금의 투カップ스의 원조 [SEP] [PAD]',\n",
              " '[CLS] OOOO들이 다른 영화를 만들면, 춘념!!!!!!!!! [SEP] [PAD]',\n",
              " '[CLS] 애슐리!! 연기력 좋은 아이가 왜 잘 벗니?(겉모습은 좋지만..) [SEP] [PAD]',\n",
              " '[CLS] 정신 병자가 있다는 것을 알리기 위한 영화. [SEP] [PAD]',\n",
              " '[CLS] 감독과 배우에게 반한 영화 [SEP] [PAD]',\n",
              " '[CLS] ㅉㅉㅉ [SEP] [PAD]',\n",
              " '[CLS] 좋다 [SEP] [PAD]',\n",
              " '[CLS] 이게 영화인가... 영화계의.. [SEP] [PAD]',\n",
              " '[CLS] 히바루... 보다가 잠든 최초의 공포영화...--. [SEP] [PAD]',\n",
              " '[CLS] 절대 잊을 수 없다 애니 [SEP] [PAD]',\n",
              " '[CLS] 뭔가 대단한 맛? 피해망상증이 있네. [SEP] [PAD]',\n",
              " '[CLS] 최고의 코미디! 다세포 소녀를 능가하는 영화의 훌륭함 [SEP] [PAD]',\n",
              " '[CLS] 오늘 즐겨보자. [SEP] [PAD]',\n",
              " '[CLS] 영화, 속아 버린 영화...www [SEP] [PAD]',\n",
              " '[CLS] 확실히 뭔가 좀 어색해... 제시카 알바의 연기가... [SEP] [PAD]',\n",
              " '[CLS] 가족..가족이 많으면 힘든 면도 있지만..좋은 일도 있다 [SEP] [PAD]',\n",
              " '[CLS] 완전히 기대 이하였다.전혀 재미도 재미도 없는 영화였다. [SEP] [PAD]',\n",
              " '[CLS] 이것은 초등학교 때 가족과 함께 영화관에 가서 봤다. OOO기 영화 [SEP] [PAD]',\n",
              " '[CLS] 또 한사람의 초인영웅 [SEP] [PAD]',\n",
              " '[CLS] 무슨 소리 007 시리즈는 홍콩 할매에 비하면 하수구 쓰레기다. [SEP] [PAD]',\n",
              " '[CLS] 볼만함 [SEP] [PAD]',\n",
              " '[CLS] 정말로 할말 없다. 영화였다 공포영화치고는 부끄럽다 (영화) [SEP] [PAD]',\n",
              " '[CLS] 초등학교때 봤는데.. [SEP] [PAD]',\n",
              " '[CLS] 아르바이트에 끌렸다 감동도 스타일도 내용도 없다 [SEP] [PAD]',\n",
              " '[CLS] 주인공의 직업은 OO이고, 하는 일은 초순수야.그리고 이렇게까지 해야 되나? [SEP] [PAD]',\n",
              " '[CLS] 다시 떠올리고 싶지 않아. 제일 민망한 한국 어린이 드라마 [SEP] [PAD]',\n",
              " '[CLS] 시대적 고통을 이용한 선정적 상업 영화 [SEP] [PAD]',\n",
              " '[CLS] 재미도 재미도 없는 마카로니 웨스턴 [SEP] [PAD]',\n",
              " '[CLS] 조난 잠타... [SEP] [PAD]',\n",
              " '[CLS] 한마디로 쓰레기 [SEP] [PAD]',\n",
              " '[CLS] 영화가 정말 소박하고 신중하고 지혜롭다.그윽하고 깊은 울림을 주다. [SEP] [PAD]',\n",
              " '[CLS] 이걸 내가 왜 봤는지... [SEP] [PAD]',\n",
              " '[CLS] 잔잔하면서도 재즈 음악이 인상 깊은 일본판 러브 액추얼리. [SEP] [PAD]',\n",
              " '[CLS] 오랜만에 접한 로맨스입니다강력히 추천합니다. [SEP] [PAD]',\n",
              " '[CLS] 할 말이 없다. [SEP] [PAD]',\n",
              " '[CLS] 적어도 한반도보다는 재미있는 영화(MSG무첨가영화) [SEP] [PAD]',\n",
              " '[CLS] 대충 어이가 없어... 최강의 반전이 당신을 기다리고 있습니다. [SEP] [PAD]',\n",
              " '[CLS] 7편과 내용이 거의 같다. [SEP] [PAD]',\n",
              " '[CLS] 일생에 한번뿐인 사랑... 영원한 갈증속에 죽을 수 밖에 없어 [SEP] [PAD]',\n",
              " '[CLS] 어이없음 [SEP] [PAD]',\n",
              " '[CLS] 솔직히 재미없어...... [SEP] [PAD]',\n",
              " '[CLS] 생각보다 재밌어요.추천... [SEP] [PAD]',\n",
              " '[CLS] 스카팅 [SEP] [PAD]',\n",
              " '[CLS] 어린 시절의 추억을 간직하고 있는 또 하나의 시리즈물 [SEP] [PAD]',\n",
              " '[CLS] 소설을 써야지 [SEP] [PAD]',\n",
              " '[CLS] 공포물이 없는 공포영화가 줄줄 늘어선 넋두리 같다. [SEP] [PAD]',\n",
              " '[CLS] 내가 만들어도 이것보다는 재밌을 것 같아. [SEP] [PAD]',\n",
              " '[CLS] 지나의 자손 [SEP] [PAD]',\n",
              " '[CLS] 마지막에 나오는 한편의 시가 가슴에 와 닿습니다. [SEP] [PAD]',\n",
              " '[CLS] 잘 모르겠다..영화라.. [SEP] [PAD]',\n",
              " '[CLS] 누리꾼의 평점대로 드라마가 너무 좋았다. 위노나 라이더의 귀여운 매력! [SEP] [PAD]',\n",
              " '[CLS] 또 보고싶다. sbs에서 재밌게 봤던 기억이.. 분위기 짱. [SEP] [PAD]',\n",
              " '[CLS] 진정한 웃음을 원하는 사람은 거들떠보지도 않는 그런 영화 [SEP] [PAD]',\n",
              " '[CLS] 쓰레기 영화-점수는 없나 예수쟁이 보아라 천박한 유대인 영화 [SEP] [PAD]',\n",
              " '[CLS] 내러티브와의 커뮤니케이션 재구성 [SEP] [PAD]',\n",
              " '[CLS] 유치장의 합창 장면 최고의 명장면이다! [SEP] [PAD]',\n",
              " '[CLS] 재미없어...내실력이 부족한걸까..? [SEP] [PAD]',\n",
              " '[CLS] 정말 걸작인... [SEP] [PAD]',\n",
              " '[CLS] 오다죠랑 같이 출발한 여행엔딩이 맘에 들어서 아저씨 느낌이 많이 나ㅋ [SEP] [PAD]',\n",
              " '[CLS] 감독의 종교적 사유의 깊이가 그대로 드러난다. [SEP] [PAD]',\n",
              " '[CLS] 여행가고 싶다멋지다! [SEP] [PAD]',\n",
              " '[CLS] Babyface가 만든 최고의 OST가 빛나는 영화~~ [SEP] [PAD]',\n",
              " '[CLS] 내용은 7점이지만 배우 아오이 유우의 연기 소화력은 10점. [SEP] [PAD]',\n",
              " '[CLS] 원작 망치 교본 [SEP] [PAD]',\n",
              " '[CLS] 재미있었던 드라마.. [SEP] [PAD]',\n",
              " '[CLS] 정말 조스는 한 편으로 끝났어야 했는데. [SEP] [PAD]',\n",
              " '[CLS] 억지 이야기에 인물별로 배경별로 노는 c.g오마이갓. [SEP] [PAD]',\n",
              " '[CLS] 발상은 좋다. 하지만 단편영화라는 게 발상만 좋다고 다는 아니다. [SEP] [PAD]',\n",
              " '[CLS] 파멜라를 앞세운 어른 취향의 재미없는 액션 [SEP] [PAD]',\n",
              " '[CLS] 재밌어, 결말도 귀엽고. [SEP] [PAD]',\n",
              " '[CLS] 이거 본 사람들 다 해외파인가? [SEP] [PAD]',\n",
              " '[CLS] 에리카이프네 [SEP] [PAD]',\n",
              " '[CLS] 너무한다.. [SEP] [PAD]',\n",
              " '[CLS] 정말 공감한다. [SEP] [PAD]',\n",
              " '[CLS] 박건형 좋아해요! [SEP] [PAD]',\n",
              " '[CLS] 재밌어 ! ㅎㅎ [SEP] [PAD]',\n",
              " '[CLS] 정말 좋은 영화네요.여주인공 연기가 좀 오버액션 [SEP] [PAD]',\n",
              " '[CLS] 재미있었어. [SEP] [PAD]',\n",
              " '[CLS] 고증을 함부로 따분해하다 한반도보다 재미없다. [SEP] [PAD]',\n",
              " '[CLS] 악과 악의 대결 [SEP] [PAD]',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pqw0edFTqs4"
      },
      "source": [
        "token_test_utterance = tokenize(test_utterance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIJxKQsZb02C",
        "outputId": "2d8ce22d-437b-4ae3-c68d-f1ea0f83100c"
      },
      "source": [
        " # test mask id 생성\r\n",
        "masks_test_utterance = []\r\n",
        "mask_func(masks_test_utterance, token_test_utterance) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2HMRASLZ0GE",
        "outputId": "bdfa4282-2fd7-41f1-e10b-3cf7db0d0665"
      },
      "source": [
        "# test 데이터를 텐서로 변환\r\n",
        "test_inputs = torch.tensor(token_test_utterance)\r\n",
        "test_masks = torch.tensor(masks_test_utterance)\r\n",
        "\r\n",
        "print(test_inputs[0])\r\n",
        "print(test_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  101,  9670, 89523, 47058,  9607, 61439, 42428, 58303, 48345,   119,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6gxWKi11OHV"
      },
      "source": [
        "##### **Hyper-parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUY7pdLAyEDq"
      },
      "source": [
        "pretrained_weights = 'bert-base-multilingual-cased'\n",
        "learning_rate = 2e-5 # 1e-5\n",
        "n_epoch = 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC1cs8JC3cYA"
      },
      "source": [
        "##### **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVU9HqHc3Tsz"
      },
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.bert_model = BertModel.from_pretrained(pretrained_weights, num_labels=2)\n",
        "    self.linear = torch.nn.Linear(768, 2) \n",
        "\n",
        "  def forward(self, input_tensor, input_mask):\n",
        "    hidden_tensor = self.bert_model(input_tensor,attention_mask=input_mask)[0] # (bat, len, hid)\n",
        "    hidden_tensor = hidden_tensor[:, 0, :] # (bat, hid)\n",
        "    logit = self.linear(hidden_tensor)\n",
        "    return logit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rztPJu1DJI_G"
      },
      "source": [
        "##### **Evaluation Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "273RBAszJMGd"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate(true_list, pred_list):\n",
        "  print(pred_list)\n",
        "  print(true_list)\n",
        "  precision = precision_score(true_list, pred_list, average=None)\n",
        "  recall = recall_score(true_list, pred_list, average=None)\n",
        "  micro_f1 = f1_score(true_list, pred_list, average='micro')\n",
        "\n",
        "  # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "  eval_accuracy = flat_accuracy(pred_list, true_list)\n",
        "  print('precision: ', ['%.4f' % v for v in precision])\n",
        "  print('recall:\\t\\t', ['%.4f' % v for v in recall])\n",
        "  print('micro_f1: %.6f' % micro_f1)\n",
        "  print(\"Accuracy: {0:.2f}\".format(eval_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9RcP6vr1Whw"
      },
      "source": [
        "##### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh4bqgfSP6Wm",
        "outputId": "bc65e8dd-e299-4279-916a-0ebeb28776c9"
      },
      "source": [
        "# GPU 디바이스 이름 구함\r\n",
        "import tensorflow as tf\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "# GPU 디바이스 이름 검사\r\n",
        "if device_name == '/device:GPU:0':\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "else:\r\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxxDICLbP727",
        "outputId": "69a671ca-8920-443d-d1e0-e308feb04b3f"
      },
      "source": [
        "# 디바이스 설정\r\n",
        "if torch.cuda.is_available():    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "else:\r\n",
        "    device = torch.device(\"cpu\")\r\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kutNRuQScPQY",
        "outputId": "8013ef01-6247-4bda-a693-db84a958f0a9"
      },
      "source": [
        "\r\n",
        "n_epoch = 2\r\n",
        "for i_epoch in range(0, n_epoch):\r\n",
        "  for batch_idx, batch_data in enumerate(dev_dataloader): \r\n",
        "    b_input_ids, b_labels, b_input_mask = batch_data\r\n",
        "    #print(b_input_ids.size()) # torch.Size([32, 160])\r\n",
        "    #print(b_labels.size())  # torch.Size([32])\r\n",
        "    #print(b_input_mask.size()) #torch.Size([32, 160])\r\n",
        "    print(b_input_ids[batch_idx]) # torch.Size([32, 160])\r\n",
        "    print(b_labels[batch_idx])  # torch.Size([32])\r\n",
        "    print(b_input_mask[batch_idx]) #torch.Size([32, 160])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   101,   9004,  32537,   9651,  63243,  14801,  44359,   9523,  16985,\n",
            "         12424,   9685, 119118,  12965,  48549,    119,    119,    119,   9580,\n",
            "        118856,  19105,  10530,   9653, 119196,  11102,   9565,  21614,  84177,\n",
            "         53736, 119147,  12965,  48549,    102,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9519,    198,    198,   9659,  22458, 119136,  12965,    198,\n",
            "           198,    198,    102,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9665,  21789,  11513,   9356,  30936,   9670,  89523,   8867,\n",
            "         38709,  10622,   8932,  14423, 119424,  41850,   9246,  32537,  44130,\n",
            "           119,    119,    102,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,  83491,   8954,  23811,  11287,  25486,  29669,   9004,  32537,\n",
            "        119197,  14523,  48549,    102,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,    100,   9706,  35866,  35506, 118728,    117,    117,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9708, 119235,   9568,  12310,  28143,  27023,   9690,  10739,\n",
            "         77884,    119,    119,   9069,  12945,  12092,  30085,  35465,   9356,\n",
            "         14153,   9100,   8863,  53354,    119,    119,    102,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   8924,  82838,   9479, 119411,  42428,  12092,   9607,  12965,\n",
            "          9358,   9664,  40364,  28911,   9152, 118707, 119136,  10739,   9873,\n",
            "         48653,  26444,  12692,  10884,  10530,  18471,   9596,   9638,  82838,\n",
            "          9034,  29364,   8982, 118728,    102,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101,   100,  9926, 34907, 10739,  9596, 88332,  8992, 16985,   136,\n",
            "          132,   132,   132,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101,  8845, 19105, 10530, 25805,  9364, 41850,  9670, 89523,  9607,\n",
            "        67527, 10739,  8938, 10892, 42428, 11903,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([ 101, 8984,  100,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   8996,  11287,   9689,  89523, 101814,   9651, 119230,  49742,\n",
            "          9638,  42815,   9694,   9952,  16439,  11287,   9553,  23811,  11287,\n",
            "          9356, 118760,  59894,   9893,  12965, 118748,  11018,   9595,  11287,\n",
            "         77884,   9486,  17196,  20173,  11903,   8926,  28911,   9580, 118762,\n",
            "          8881,  89523,   9708, 119235,    119,    119,    119,   9952,    119,\n",
            "           119,    119,   9604,  12692,  82490,  53736,   9638,  82838,   9320,\n",
            "         10530,   9290,  19105,  15001,  16439,    136,    102,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101, 81571, 19105,  9685, 16985, 14843,   119,   119,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9779, 100372, 118624,  67527,   9056,   9278, 118760,  12692,\n",
            "         27023,  11102, 119351,   8984,  12692,  77884,  48549,   9133,  12092,\n",
            "         40523,   9405,  77763, 118825,  17342,  12424,   9485,  90295,  84295,\n",
            "          9637, 118743, 108436,  12508,   9067,  25387,  22879,   8843,  52560,\n",
            "         44359,  19105,   9282,  11403,  11403,  10622,   8987,  11467,   9408,\n",
            "         16985,  37093,  97802,   8881, 119439,  19105,  38378,   9303,  11287,\n",
            "         13441, 118627,   9246, 118729,   9739,  36240, 119274, 118658,  12453,\n",
            "           119,    119,   9747, 119136,  11018,   8987,  50450,  20173,  53736,\n",
            "         30005,  11664,   9663,  14153, 118913,  11018,  11287, 106154,   9495,\n",
            "         12965, 118774,  16985, 101202, 118671,   9294, 119041,  77884,  48549,\n",
            "         11287,  16605, 118992,  56999,  14646, 119274,  46216,    102,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9565,  76036,  27355,  11287,   9638, 119022,  17360,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9356,  11018,   9638,  12092,    100,   8932, 118651,  11102,\n",
            "          9568,  52363,    102,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9638,  14153,   9304,   9281,  38709,  10739,  21711,    100,\n",
            "           100,   9926, 108056,  35506,  17196,  19105,    102,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   8932,  14423,   9521,  12453,   8924, 118729,   9364,  41850,\n",
            "          9670,  89523,    100,    198,    102,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9580,  14040,  10739,   9246,  39420,  35866,   9670,  89523,\n",
            "          9067,  25387,  10739,  81571,  35506,  25387,   8863,   9256,  16439,\n",
            "         48549,    136,   9604,  46766,  14153,   8896,  66540,  12310,  18778,\n",
            "         14423,   9159,  39773,   9909,  30873,  11903,  96972,   8985,  26737,\n",
            "         16323,  25387,   8924,   9367,   9256, 119217,    136,   9812, 118956,\n",
            "         12605,  33323,    117,  19767,  33323,    117,   8925,  13890,  33323,\n",
            "         29414,  11513,   9004,  32537,  16439,  12092,    100,   9358,   9909,\n",
            "         15184,  56645,  10739,  41605,  11513,   9638,  82838,   9407,  21386,\n",
            "         20626,  66540,   8996, 118748, 119185,  14040,  14867,    100,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9612,  38709,  12030,  23130,  53736, 118802,   9604,  25486,\n",
            "         18108,   9364,  41850,    100,  48556,  33323,  12092,   9294,  12508,\n",
            "           100,    102,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,    100,  74293,  10892,   9270,  84177,  28911,   9955, 118920,\n",
            "         25503,  27023,   9004,  32537,   9485,  49543,   8924, 118729,    119,\n",
            "           119,    119,    102,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9659,  22458, 119136,  11018, 118627,    102,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101,  9004, 32537,   100,  9364, 82034,   119,   119,  9670, 89523,\n",
            "         9507,  9827, 11102, 42428,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9069,  40419,  89380,    119,    119,   9489,  17730, 106826,\n",
            "        119412,  11261,  12178, 118627,  12092,  16985,  25503,  11664,  67324,\n",
            "          9567,  29935,  12424,   9798, 119412,  19105,  27023,  11664,    119,\n",
            "           119,   9328,  21155,  10739,   9253,  10530,   9521,  90537,  11903,\n",
            "           119,    119,    102,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101,  9519, 83491,   119,   119,  9706, 40032,  9460, 74986,  8977,\n",
            "        16439, 11664, 98199,   100,  9978, 31503, 15387, 10244, 69725,  9405,\n",
            "        62200, 10739, 21711, 12310,   198,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9659,  22458, 119136,  32158,    102,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,  74293,  11882,  77039,  24683,  65349, 119109,  12066,  10954,\n",
            "         52602,   9420,  66540,  38378,  99958,   9659,  22458,  76820,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9638,  14153,  42428, 118728,    136,    136,    136,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9460,  10954,   9665,   8982,  37093,  29079,   9799,  97146,\n",
            "        118790, 105392,  11882,   9379,  25242,  14523,   9356,  14867,   9065,\n",
            "          8982, 119156,    119,   9267,  11287,   9968, 119205,  48533,    119,\n",
            "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   8924,  21711,  89523,  11261,   9519,  32537, 118627,  12092,\n",
            "         40364,   9511,  56645,  12310,    119,    102,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,  82564,  48549,  22333,  35506,  16439,   9926,  38709,  10739,\n",
            "         14871,  10892,   9008,  12310, 119471,  90537,  42428,    102,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9670,  89523,  54141,  10739,  14871, 119251,  10622,  73894,\n",
            "        119136,  77884,  48549,    117,    117,    117,   8954,  11664, 118686,\n",
            "         11664, 118839, 118686,  11664,    100,    136,    136,   9652,  11287,\n",
            "         22458,  55358,  41521,  16985, 118767,  11287,  48549,    136,    136,\n",
            "           136,    100,  25805,  11018,  30005,  11664, 119088,  32815, 119112,\n",
            "         17360,  48549,    106,    106,    106,    106,    106,    102,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9330,  31720,  43739,   9405,  29364,  30085,  69023,   9776,\n",
            "         14867,  11467,   9359, 118832,  21881,  10929,  10954,   8855,  12508,\n",
            "        119112,  11903,    119,    102,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9638,  14153,   9596,   9255,  11102,  30858,  18227,  12030,\n",
            "         12508,   9283,  31401, 118632,  32158,    100,   9952,  10739,  20309,\n",
            "          9489,  41605,    106,    106,    102,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   8865,  15001,  26737,  30005,  11664,  14040, 119088,  11903,\n",
            "          9926,  34907,    100,    102,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9532,  25503,  38378,  23130,  44359,    119,    119,   9526,\n",
            "          8924, 118871,  12508,    102,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   8848,  18778,  14801,  11925,   9281,  38709,  15303,   9638,\n",
            "         42815,  11287, 119192, 118825,    102,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   8924, 118729,   9640,  97146,  25503,  15184,   9489,  28143,\n",
            "         12092,  40523,   9568,  27792,   9654,  12453,   9960,  12424,   9708,\n",
            "         71013, 119214,  32158,    113,   9952,  15184,    114,   9640,  17138,\n",
            "         12092, 119214,  11664,    117,   9640,  12945,  17138,  12092,  42144,\n",
            "         81220,    117,   8924, 118729,    100,    106,    106,    106,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9638,  41521,  36210, 118959,  32158,    136,    100,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([101, 100, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9706,  35866,  35506,  18623,   9523, 119118,  28578,   9353,\n",
            "         11261,    119,    119,    102,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   8869,  16439,   9659,  22458, 119136,  77884,   9730,  17342,\n",
            "         12692,    122,  50450,  10739,  10008,    100,   9485,  18784,   9143,\n",
            "         27355,  12310,  24974,  11467,  12092,   9519, 118671, 119169,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([ 101, 9490,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9358,  42337,  12945,  15891,    106,    106,   9553,  16605,\n",
            "         18227,    117,   9319,  12424,  54867,    100,    106,    106,    100,\n",
            "          9353,  78136,  14423,  80001,   9074, 119008,  41584,  12424,  40419,\n",
            "         82034,    198,    100,    102,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9663,  19105,   8924,  56710,  11287,  48549,    119,    119,\n",
            "          9405,  14863,  70915,  12508,  65610,  24017,  11467,  73131,  42428,\n",
            "          8848,  14871,  11102, 118775,  14040,  11664,   9364,  41850,   9069,\n",
            "         29683,   9546,  48446,  12424,   8848,  18778, 108436,  14843,  12508,\n",
            "           100,   9069,  29683,   9428,  24017,   9405,  24017, 119446,  10892,\n",
            "          9596,  71568,  68828,  97403,  21406,   8907,  83811,  11882,  87437,\n",
            "          9303,  11287,   9891,  61844,  11102,  12508,    132,    132,    100,\n",
            "           119,    119,    119,    102,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101, 80956, 71439,  9665, 17342, 12092,  9367, 20173, 16439,  9356,\n",
            "        24982, 48549,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9521,  30005,  14867,  59330,  93834,   9596,  30005,  30936,\n",
            "          9600,  35506, 118728,  35506, 118632,  28578,    117,   9319,  30842,\n",
            "         12508, 118615,  10892,  84703,  11287,  77324,  14153,   9004,  32537,\n",
            "         25387,  12310,  14843, 119020,  10739,  77884,  48549,    117,   9952,\n",
            "         35866,  22200,  18392,    100,   9448,  12692,  19105,  12508,  81220,\n",
            "          9719,  26737,  30005,  11664,   9657,  30005,  12692,   9568,  36553,\n",
            "         16605,  11882,   9379,  25242,  38378,   9609,  32158, 118964,  10530,\n",
            "          9521,  16439,  28188,  77884,  48549,    119,   9448,  12692,  12508,\n",
            "         81220,   9719,  26737,  19105,   9358,  85634,   9520,  50814,  11287,\n",
            "         16985, 118767,  14423,    117,   9568,  46874,   9638,  14523,  61964,\n",
            "          9365,  52560,  11102, 118627, 118615, 119081,  48345,    119,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   8982,  81724,   9088, 108521,   8955,   8843, 118990,  21711,\n",
            "        118632,  12965,  48549,    119,    102,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9638,  42428,  11513,   9706,  40032,  18382,  12092,   9521,\n",
            "        118991,  11903,  14867,    117,   9706,  40032,   9067,  13890,   9061,\n",
            "         26737,  11287,  12424,   9356,  16985,  21711,   9955,  42428,  59894,\n",
            "          9251,  12453,   9495,  11903,    106,   9609,  32158,  11287, 118813,\n",
            "           117,   9034,  29364,  11287, 118813,    117,   8848,  18778,  11287,\n",
            "        118813,    117,  10023,  89292,  11287, 118813,  11102,   9327,  10622,\n",
            "          9330,  14646,  78131,   9266,  10892,   8932,  37712,  10739,  59894,\n",
            "          9955,  19105, 119328,    117,  42428,  11287,   8977,  49742,   9462,\n",
            "         18784,  18382,   9645,  11287,  10530,   9265,  40032,  10892,   9309,\n",
            "         86488,   9141, 119284,  37388,  21711,   9141, 119284,  73894,   9555,\n",
            "         10622,  19105, 119328,    117,   8932,  37712, 119214,  10892,  42428,\n",
            "           106,    102,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101,  9281, 14423, 12945,   119,  9855, 14040, 12310, 12945,   131,\n",
            "         9130, 92564, 58303, 48345,   198,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101,  9652, 52951, 17138, 13767, 42428,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,  80956,   9041, 118713,  10459,  42428,   9004,  32537,   9685,\n",
            "         16985,  48549,    100,    102,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   1792,  10739,  42428,   9356,  70221,   9965,  15891,    136,\n",
            "          9272,  11287, 119383,  12605,  79718,  11489,   9359,  15891,   9647,\n",
            "         32158,    106,   8848,  18778,    119,    119,    102,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   8977,  18382,   8977,  31605,  54867,  11903,    119,   9533,\n",
            "         59095,  33305,  32537,  78136,  13764,  29683,    119,   9246,  18622,\n",
            "          9083,  15001,  28188,  12965, 119134,  17342,  98789,   9233,  32487,\n",
            "         84177,  12310,  37712,    102,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9519,  16439,    117,   9689,  17730,  17655,    100,   8855,\n",
            "        119121,  12424,   9521,  30005,  26737,  80244,   9576,  51945,  17138,\n",
            "          9546, 119239,  17196,  35506, 118666,  37388,   9364,  17196,  19105,\n",
            "           117,   9511,  56645,  12310,  77884,    119,   9519,  25503,   9689,\n",
            "         87164,   8924, 118871,  11903, 119265,    119,   9297,  87164,   9640,\n",
            "         29364,   9491,  12692,  12638,   9966,  18778,  10739,   9638,  14523,\n",
            "         11287,   9521,  11287,    119,   9966,  18778,  10459,   8857,  25486,\n",
            "         17138,  12092,   9555,  11664,    117,    100,   8924,  56710,   9414,\n",
            "         65649, 118832,  25934,  10530,   9663,  56710,   9712,  10622,   9965,\n",
            "         17196,  16439,   9960,  21711,  23969,  28911,    117,   8924, 118729,\n",
            "          9056,   9496,  10739,  25517,  21711,    119,  81571,    100,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,  48556,  56789,  29414,   8982, 119022,  11664, 118992,  55358,\n",
            "         58931,  12453,  21789, 118739, 119136,  11018,    100,    119,  48556,\n",
            "         56789,   8982, 119023,  14153,  16985,  25503,  17342,   9059,  12508,\n",
            "         78136,  56710,  12945,  61250,  20173,  11102,  20479,  35979, 119192,\n",
            "         10622, 119020,    119,    119,    119,  48556,  12030,  29455,  35506,\n",
            "         11287,  71013,  35506,  77884,  48549,    119,  48556,  12030,  11467,\n",
            "         73131,   9503,  12310,  37712, 119214,  10892,  42428,  11018,   9519,\n",
            "        108578,    119,   9638,  42428,  73306,   9638,  42815,  11287,   9095,\n",
            "         37004,  16985,  12030,  25347,  24989,  29683,  11489,  48556,  83200,\n",
            "          9379,  33305,  22440,  19855,  48533,  12030,  11287,    136,   9926,\n",
            "         34907,  10929,  34907,  10739, 119136,  77884,  48549,    106,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101,  9034, 29364, 16439,   119,   119,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101,   121, 34907,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,  10407,  37712, 119235,  12692,   9309,  25503,  61156,  68984,\n",
            "          8903,  11664,    102,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,  10197,  10954,  16617,    117,  97748,  38709,    112,   9421,\n",
            "         10739, 106065,    112,   8898,   9379,  25242,  14523,   9356,  14040,\n",
            "         17342,    119,   8881,  89523,  19789,   9968,  89292,  48533,  10459,\n",
            "          8925,  18622,    119,    102,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9750,  13890,  14867,  30005,  11664,  42428,   9659,  22458,\n",
            "        119192, 118632,  17196,  16439,   9965,  41850,    119,    119,    119,\n",
            "          9004,  32537,   9706,  35866,  35506,  77884,  48549,   9706,  33305,\n",
            "         11882,  41521,  11513,   9998,  14871,  12178,  71439,  28911,   9004,\n",
            "         32537,   9706,  35866,  12453,   9659,  22458, 119136, 119081,  48345,\n",
            "           119,    119,   8848,  18778,  12092,   9555,  11664,   9485,  18784,\n",
            "         37004,  16985, 118794,  16985,  12092,  19709,  30858,  18227,   9356,\n",
            "         14040,  11018,  14153,   9685,  10622, 118627,   8855,  16985,  48549,\n",
            "           119,    119,    102,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,  48556,  11882,  37650,   9730,  57362,   9565,  31503,  10739,\n",
            "          9041, 118684,  32815,   8888,  25242,  75782,    102,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([   101,   9757,  12030,  15387,   9640,  31605,  11261,   9706,  35866,\n",
            "         48533,  10622,    100,  50342,  39773,  71439,   9555, 119137,  11903,\n",
            "           102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([101, 100, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([  101,  9616, 19105, 14523, 18471,   122, 34907, 10892,  9521, 54867,\n",
            "        11903,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-1ed8cfef506a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(b_labels.size())  # torch.Size([32])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(b_input_mask.size()) #torch.Size([32, 160])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([32, 160])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# torch.Size([32])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#torch.Size([32, 160])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 64 is out of bounds for dimension 0 with size 64"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVsAbGvQjIeO"
      },
      "source": [
        "# 시간 표시 함수\r\n",
        "def format_time(elapsed):\r\n",
        "\r\n",
        "    # 반올림\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # hh:mm:ss으로 형태 변경\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pSuCrbO2E6i"
      },
      "source": [
        "# 정확도 계산 함수\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    \r\n",
        "    pred_flat = preds.flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gvetI0DNysM5",
        "outputId": "48286dd8-1ba9-4c6d-8e7c-e68615fac957"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import torch\n",
        "from tqdm import tqdm_notebook # tqdm_notebook 진행 표시바\n",
        "import numpy as np\n",
        " \n",
        "batch_size=32\n",
        "model = Model() #정의된 모델 수행\n",
        "model.cuda()\n",
        "criterion = torch.nn.CrossEntropyLoss() # LogSoftmax & NLLLoss, tf.nn.weighted_cross_entropy_with_logits\n",
        "#class_weight = torch.FloatTensor(weights).cuda()\n",
        "#criterion = torch.nn.CrossEntropyLoss(weight = class_weight)\n",
        "optimizer = torch.optim.Adam(model.parameters(), learning_rate) #gradient descent 알고리즘 : Adam\n",
        "\n",
        "for i_epoch in range(n_epoch + 1):\n",
        "  print('i_epoch:', i_epoch)\n",
        "  train_loss = 0 \n",
        "  model.train()\n",
        "  for batch_idx, batch_data in enumerate(train_dataloader): \n",
        "\n",
        "    # 배치를 GPU에 넣음\n",
        "    batch_data = tuple(t.to(device) for t in batch_data)\n",
        "    \n",
        "    # 배치에서 데이터 추출\n",
        "    b_input_ids, b_labels, b_input_mask = batch_data\n",
        "\n",
        "    # 배치에서 데이터 추출\n",
        "    logit = model(b_input_ids, b_input_mask)\n",
        "    batch_loss = criterion(logit, b_labels)\n",
        "    train_loss += batch_loss.item()\n",
        "      \n",
        "    ########## jeong test ########\n",
        "    # dimension 확인\n",
        "    #print (logit.size()) #torch.Size([4, 1])\n",
        "    #print (logit)  \n",
        "    #print (b_labels.size()) #torch.Size([4, 1])\n",
        "    #print (b_labels)  \n",
        "    ########## jeong test ########\n",
        "    \n",
        "    model.zero_grad()\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print('Epoch: ', i_epoch + 1)\n",
        "    print(\"\\r\" + \"{0}/{1} loss: {2} \".format(batch_idx, len(train) / batch_size, train_loss / (batch_idx + 1)))  \n",
        "\n",
        "  #시작 시간 설정\n",
        "  t0 = time.time()\n",
        "\n",
        "  # 평가모드로 변경\n",
        "  bert_predicted = []\n",
        "  all_logits = []\n",
        "  model.eval()\n",
        "  pred_list, true_list = np.array([]), np.array([])\n",
        "  if torch.cuda.is_available():\n",
        "    for batch_idx, batch_data in enumerate(dev_dataloader):\n",
        "      # 배치를 GPU에 넣음\n",
        "      batch_data = tuple(t1.to(device) for t1 in batch_data)\n",
        "      # 배치에서 데이터 추출\n",
        "      b_input_ids, b_labels, b_input_mask = batch_data\n",
        "      logit = model(b_input_ids, b_input_mask)\n",
        "      _, max_idx = torch.max(logit, dim=-1) # tensor\n",
        "    \n",
        "      numpy_max_idx = max_idx.detach().cpu().numpy() \n",
        "      b_labels = b_labels.detach().cpu().numpy() \n",
        "\n",
        "      pred_list = np.append(pred_list, numpy_max_idx)\n",
        "      true_list = np.append(true_list, b_labels)\n",
        "    \n",
        "  evaluate(pred_list, true_list) # print results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i_epoch: 0\n",
            "Epoch:  1\n",
            "0/2343.75 loss: 0.7993124723434448 \n",
            "Epoch:  1\n",
            "1/2343.75 loss: 0.7538748979568481 \n",
            "Epoch:  1\n",
            "2/2343.75 loss: 0.7400270104408264 \n",
            "Epoch:  1\n",
            "3/2343.75 loss: 0.7274143695831299 \n",
            "Epoch:  1\n",
            "4/2343.75 loss: 0.7372400522232055 \n",
            "Epoch:  1\n",
            "5/2343.75 loss: 0.7237531542778015 \n",
            "Epoch:  1\n",
            "6/2343.75 loss: 0.7269537023135594 \n",
            "Epoch:  1\n",
            "7/2343.75 loss: 0.7328879907727242 \n",
            "Epoch:  1\n",
            "8/2343.75 loss: 0.7368628713819716 \n",
            "Epoch:  1\n",
            "9/2343.75 loss: 0.7333591222763062 \n",
            "Epoch:  1\n",
            "10/2343.75 loss: 0.7324114333499562 \n",
            "Epoch:  1\n",
            "11/2343.75 loss: 0.728434681892395 \n",
            "Epoch:  1\n",
            "12/2343.75 loss: 0.7265577499683087 \n",
            "Epoch:  1\n",
            "13/2343.75 loss: 0.7218483686447144 \n",
            "Epoch:  1\n",
            "14/2343.75 loss: 0.7195383151372273 \n",
            "Epoch:  1\n",
            "15/2343.75 loss: 0.7198252938687801 \n",
            "Epoch:  1\n",
            "16/2343.75 loss: 0.7161251411718481 \n",
            "Epoch:  1\n",
            "17/2343.75 loss: 0.7147836122247908 \n",
            "Epoch:  1\n",
            "18/2343.75 loss: 0.711703287927728 \n",
            "Epoch:  1\n",
            "19/2343.75 loss: 0.7092449605464936 \n",
            "Epoch:  1\n",
            "20/2343.75 loss: 0.7084062553587414 \n",
            "Epoch:  1\n",
            "21/2343.75 loss: 0.7085946473208341 \n",
            "Epoch:  1\n",
            "22/2343.75 loss: 0.7064311867174895 \n",
            "Epoch:  1\n",
            "23/2343.75 loss: 0.7034843812386194 \n",
            "Epoch:  1\n",
            "24/2343.75 loss: 0.7015158414840699 \n",
            "Epoch:  1\n",
            "25/2343.75 loss: 0.7005855211844811 \n",
            "Epoch:  1\n",
            "26/2343.75 loss: 0.6994160170908328 \n",
            "Epoch:  1\n",
            "27/2343.75 loss: 0.6975943957056318 \n",
            "Epoch:  1\n",
            "28/2343.75 loss: 0.6974111199378967 \n",
            "Epoch:  1\n",
            "29/2343.75 loss: 0.6974715610345205 \n",
            "Epoch:  1\n",
            "30/2343.75 loss: 0.6955778810285753 \n",
            "Epoch:  1\n",
            "31/2343.75 loss: 0.6943746861070395 \n",
            "Epoch:  1\n",
            "32/2343.75 loss: 0.6930951259352944 \n",
            "Epoch:  1\n",
            "33/2343.75 loss: 0.6930112453068004 \n",
            "Epoch:  1\n",
            "34/2343.75 loss: 0.6909204874719892 \n",
            "Epoch:  1\n",
            "35/2343.75 loss: 0.6896529793739319 \n",
            "Epoch:  1\n",
            "36/2343.75 loss: 0.6881487224553082 \n",
            "Epoch:  1\n",
            "37/2343.75 loss: 0.6856555609326613 \n",
            "Epoch:  1\n",
            "38/2343.75 loss: 0.6850675023519076 \n",
            "Epoch:  1\n",
            "39/2343.75 loss: 0.6841060802340507 \n",
            "Epoch:  1\n",
            "40/2343.75 loss: 0.6819497652170134 \n",
            "Epoch:  1\n",
            "41/2343.75 loss: 0.6797889981951032 \n",
            "Epoch:  1\n",
            "42/2343.75 loss: 0.6783496889957162 \n",
            "Epoch:  1\n",
            "43/2343.75 loss: 0.6770820279013027 \n",
            "Epoch:  1\n",
            "44/2343.75 loss: 0.6772615273793539 \n",
            "Epoch:  1\n",
            "45/2343.75 loss: 0.6768618083518484 \n",
            "Epoch:  1\n",
            "46/2343.75 loss: 0.6759718478994167 \n",
            "Epoch:  1\n",
            "47/2343.75 loss: 0.6744695814947287 \n",
            "Epoch:  1\n",
            "48/2343.75 loss: 0.6719943637750587 \n",
            "Epoch:  1\n",
            "49/2343.75 loss: 0.6698154211044312 \n",
            "Epoch:  1\n",
            "50/2343.75 loss: 0.6671010466182933 \n",
            "Epoch:  1\n",
            "51/2343.75 loss: 0.6657493458344386 \n",
            "Epoch:  1\n",
            "52/2343.75 loss: 0.6656832492576455 \n",
            "Epoch:  1\n",
            "53/2343.75 loss: 0.6648516666006159 \n",
            "Epoch:  1\n",
            "54/2343.75 loss: 0.6634516325863925 \n",
            "Epoch:  1\n",
            "55/2343.75 loss: 0.6618577518633434 \n",
            "Epoch:  1\n",
            "56/2343.75 loss: 0.6602550901864704 \n",
            "Epoch:  1\n",
            "57/2343.75 loss: 0.6587990347681374 \n",
            "Epoch:  1\n",
            "58/2343.75 loss: 0.6554209150500216 \n",
            "Epoch:  1\n",
            "59/2343.75 loss: 0.6547671402494113 \n",
            "Epoch:  1\n",
            "60/2343.75 loss: 0.6551866936879079 \n",
            "Epoch:  1\n",
            "61/2343.75 loss: 0.6545464603170272 \n",
            "Epoch:  1\n",
            "62/2343.75 loss: 0.6535740858978696 \n",
            "Epoch:  1\n",
            "63/2343.75 loss: 0.6533614271320403 \n",
            "Epoch:  1\n",
            "64/2343.75 loss: 0.6514098291213696 \n",
            "Epoch:  1\n",
            "65/2343.75 loss: 0.6518802710554816 \n",
            "Epoch:  1\n",
            "66/2343.75 loss: 0.6508025442486378 \n",
            "Epoch:  1\n",
            "67/2343.75 loss: 0.6498907379367772 \n",
            "Epoch:  1\n",
            "68/2343.75 loss: 0.6493330273939215 \n",
            "Epoch:  1\n",
            "69/2343.75 loss: 0.6485118052789143 \n",
            "Epoch:  1\n",
            "70/2343.75 loss: 0.648096289013473 \n",
            "Epoch:  1\n",
            "71/2343.75 loss: 0.6482258525987467 \n",
            "Epoch:  1\n",
            "72/2343.75 loss: 0.6467712378665192 \n",
            "Epoch:  1\n",
            "73/2343.75 loss: 0.6445309205635174 \n",
            "Epoch:  1\n",
            "74/2343.75 loss: 0.6431654000282288 \n",
            "Epoch:  1\n",
            "75/2343.75 loss: 0.6419018310935873 \n",
            "Epoch:  1\n",
            "76/2343.75 loss: 0.6409441673910463 \n",
            "Epoch:  1\n",
            "77/2343.75 loss: 0.6387171665063271 \n",
            "Epoch:  1\n",
            "78/2343.75 loss: 0.6382847741434846 \n",
            "Epoch:  1\n",
            "79/2343.75 loss: 0.6359224282205105 \n",
            "Epoch:  1\n",
            "80/2343.75 loss: 0.6339185583738634 \n",
            "Epoch:  1\n",
            "81/2343.75 loss: 0.630339569434887 \n",
            "Epoch:  1\n",
            "82/2343.75 loss: 0.6306822285594711 \n",
            "Epoch:  1\n",
            "83/2343.75 loss: 0.6307152992203122 \n",
            "Epoch:  1\n",
            "84/2343.75 loss: 0.6291977812262143 \n",
            "Epoch:  1\n",
            "85/2343.75 loss: 0.6283548072327015 \n",
            "Epoch:  1\n",
            "86/2343.75 loss: 0.6266958138038372 \n",
            "Epoch:  1\n",
            "87/2343.75 loss: 0.6265467588197101 \n",
            "Epoch:  1\n",
            "88/2343.75 loss: 0.6256754552380422 \n",
            "Epoch:  1\n",
            "89/2343.75 loss: 0.6242911305692461 \n",
            "Epoch:  1\n",
            "90/2343.75 loss: 0.6233641511791355 \n",
            "Epoch:  1\n",
            "91/2343.75 loss: 0.6210117738531984 \n",
            "Epoch:  1\n",
            "92/2343.75 loss: 0.6198995251168486 \n",
            "Epoch:  1\n",
            "93/2343.75 loss: 0.6179675548634631 \n",
            "Epoch:  1\n",
            "94/2343.75 loss: 0.6179772508771796 \n",
            "Epoch:  1\n",
            "95/2343.75 loss: 0.6163059597214063 \n",
            "Epoch:  1\n",
            "96/2343.75 loss: 0.6157879964592531 \n",
            "Epoch:  1\n",
            "97/2343.75 loss: 0.6150815517318492 \n",
            "Epoch:  1\n",
            "98/2343.75 loss: 0.6149088644018077 \n",
            "Epoch:  1\n",
            "99/2343.75 loss: 0.6141373723745346 \n",
            "Epoch:  1\n",
            "100/2343.75 loss: 0.612861447995252 \n",
            "Epoch:  1\n",
            "101/2343.75 loss: 0.6111817225521686 \n",
            "Epoch:  1\n",
            "102/2343.75 loss: 0.6097811842427671 \n",
            "Epoch:  1\n",
            "103/2343.75 loss: 0.6087858768609854 \n",
            "Epoch:  1\n",
            "104/2343.75 loss: 0.6067396649292537 \n",
            "Epoch:  1\n",
            "105/2343.75 loss: 0.6060040151735522 \n",
            "Epoch:  1\n",
            "106/2343.75 loss: 0.6055750036351034 \n",
            "Epoch:  1\n",
            "107/2343.75 loss: 0.6050747459133466 \n",
            "Epoch:  1\n",
            "108/2343.75 loss: 0.6036694891955874 \n",
            "Epoch:  1\n",
            "109/2343.75 loss: 0.6020063172687183 \n",
            "Epoch:  1\n",
            "110/2343.75 loss: 0.6016615784920014 \n",
            "Epoch:  1\n",
            "111/2343.75 loss: 0.6007609064025539 \n",
            "Epoch:  1\n",
            "112/2343.75 loss: 0.6012885053600885 \n",
            "Epoch:  1\n",
            "113/2343.75 loss: 0.5998148719469706 \n",
            "Epoch:  1\n",
            "114/2343.75 loss: 0.5990907104119011 \n",
            "Epoch:  1\n",
            "115/2343.75 loss: 0.5981315346113567 \n",
            "Epoch:  1\n",
            "116/2343.75 loss: 0.5969617820193625 \n",
            "Epoch:  1\n",
            "117/2343.75 loss: 0.5960435783964092 \n",
            "Epoch:  1\n",
            "118/2343.75 loss: 0.5947977464740016 \n",
            "Epoch:  1\n",
            "119/2343.75 loss: 0.5933513889710108 \n",
            "Epoch:  1\n",
            "120/2343.75 loss: 0.5927200696685098 \n",
            "Epoch:  1\n",
            "121/2343.75 loss: 0.5919002915014986 \n",
            "Epoch:  1\n",
            "122/2343.75 loss: 0.5904289169040152 \n",
            "Epoch:  1\n",
            "123/2343.75 loss: 0.5887083000233096 \n",
            "Epoch:  1\n",
            "124/2343.75 loss: 0.5871234533786773 \n",
            "Epoch:  1\n",
            "125/2343.75 loss: 0.5863127888195099 \n",
            "Epoch:  1\n",
            "126/2343.75 loss: 0.5854202107651028 \n",
            "Epoch:  1\n",
            "127/2343.75 loss: 0.5851751908194274 \n",
            "Epoch:  1\n",
            "128/2343.75 loss: 0.583923831004505 \n",
            "Epoch:  1\n",
            "129/2343.75 loss: 0.582453788473056 \n",
            "Epoch:  1\n",
            "130/2343.75 loss: 0.5818335857555157 \n",
            "Epoch:  1\n",
            "131/2343.75 loss: 0.5813967163364092 \n",
            "Epoch:  1\n",
            "132/2343.75 loss: 0.58100580833012 \n",
            "Epoch:  1\n",
            "133/2343.75 loss: 0.5805139650604618 \n",
            "Epoch:  1\n",
            "134/2343.75 loss: 0.5802437400376356 \n",
            "Epoch:  1\n",
            "135/2343.75 loss: 0.5793790265041239 \n",
            "Epoch:  1\n",
            "136/2343.75 loss: 0.5792656962888955 \n",
            "Epoch:  1\n",
            "137/2343.75 loss: 0.5785813411508781 \n",
            "Epoch:  1\n",
            "138/2343.75 loss: 0.5790016331689821 \n",
            "Epoch:  1\n",
            "139/2343.75 loss: 0.5787262556808336 \n",
            "Epoch:  1\n",
            "140/2343.75 loss: 0.5778350206554359 \n",
            "Epoch:  1\n",
            "141/2343.75 loss: 0.576815031993557 \n",
            "Epoch:  1\n",
            "142/2343.75 loss: 0.5760191806129642 \n",
            "Epoch:  1\n",
            "143/2343.75 loss: 0.5760233027653562 \n",
            "Epoch:  1\n",
            "144/2343.75 loss: 0.5746031251446954 \n",
            "Epoch:  1\n",
            "145/2343.75 loss: 0.5748228520563204 \n",
            "Epoch:  1\n",
            "146/2343.75 loss: 0.5740706815200598 \n",
            "Epoch:  1\n",
            "147/2343.75 loss: 0.5733166258480098 \n",
            "Epoch:  1\n",
            "148/2343.75 loss: 0.5727826894929745 \n",
            "Epoch:  1\n",
            "149/2343.75 loss: 0.5725153972705205 \n",
            "Epoch:  1\n",
            "150/2343.75 loss: 0.5710288517127763 \n",
            "Epoch:  1\n",
            "151/2343.75 loss: 0.5699694215467102 \n",
            "Epoch:  1\n",
            "152/2343.75 loss: 0.5689491504937215 \n",
            "Epoch:  1\n",
            "153/2343.75 loss: 0.5690167275342074 \n",
            "Epoch:  1\n",
            "154/2343.75 loss: 0.5677151230073745 \n",
            "Epoch:  1\n",
            "155/2343.75 loss: 0.5675319910813601 \n",
            "Epoch:  1\n",
            "156/2343.75 loss: 0.5670425675477192 \n",
            "Epoch:  1\n",
            "157/2343.75 loss: 0.5670633549931683 \n",
            "Epoch:  1\n",
            "158/2343.75 loss: 0.5666254084065275 \n",
            "Epoch:  1\n",
            "159/2343.75 loss: 0.5658193286508322 \n",
            "Epoch:  1\n",
            "160/2343.75 loss: 0.5664512189278691 \n",
            "Epoch:  1\n",
            "161/2343.75 loss: 0.5661460448194433 \n",
            "Epoch:  1\n",
            "162/2343.75 loss: 0.5661520142496729 \n",
            "Epoch:  1\n",
            "163/2343.75 loss: 0.5663561413927776 \n",
            "Epoch:  1\n",
            "164/2343.75 loss: 0.5655121698524013 \n",
            "Epoch:  1\n",
            "165/2343.75 loss: 0.5647058969879725 \n",
            "Epoch:  1\n",
            "166/2343.75 loss: 0.5640922283341071 \n",
            "Epoch:  1\n",
            "167/2343.75 loss: 0.5642413555511406 \n",
            "Epoch:  1\n",
            "168/2343.75 loss: 0.563697728884996 \n",
            "Epoch:  1\n",
            "169/2343.75 loss: 0.5631302644224728 \n",
            "Epoch:  1\n",
            "170/2343.75 loss: 0.5628534900514703 \n",
            "Epoch:  1\n",
            "171/2343.75 loss: 0.5616497264005417 \n",
            "Epoch:  1\n",
            "172/2343.75 loss: 0.5619818842135413 \n",
            "Epoch:  1\n",
            "173/2343.75 loss: 0.5607147951578272 \n",
            "Epoch:  1\n",
            "174/2343.75 loss: 0.559774455172675 \n",
            "Epoch:  1\n",
            "175/2343.75 loss: 0.5590853018855507 \n",
            "Epoch:  1\n",
            "176/2343.75 loss: 0.5580347227511433 \n",
            "Epoch:  1\n",
            "177/2343.75 loss: 0.5577609960952502 \n",
            "Epoch:  1\n",
            "178/2343.75 loss: 0.5577788256400125 \n",
            "Epoch:  1\n",
            "179/2343.75 loss: 0.5579431556993061 \n",
            "Epoch:  1\n",
            "180/2343.75 loss: 0.5571066126309706 \n",
            "Epoch:  1\n",
            "181/2343.75 loss: 0.5568374979954499 \n",
            "Epoch:  1\n",
            "182/2343.75 loss: 0.5560261553428212 \n",
            "Epoch:  1\n",
            "183/2343.75 loss: 0.5553436778161837 \n",
            "Epoch:  1\n",
            "184/2343.75 loss: 0.5549456559322976 \n",
            "Epoch:  1\n",
            "185/2343.75 loss: 0.5545438217219486 \n",
            "Epoch:  1\n",
            "186/2343.75 loss: 0.5539604262872175 \n",
            "Epoch:  1\n",
            "187/2343.75 loss: 0.5527882220897269 \n",
            "Epoch:  1\n",
            "188/2343.75 loss: 0.5519598181600924 \n",
            "Epoch:  1\n",
            "189/2343.75 loss: 0.5518122536571403 \n",
            "Epoch:  1\n",
            "190/2343.75 loss: 0.5508879323280295 \n",
            "Epoch:  1\n",
            "191/2343.75 loss: 0.5500219385139644 \n",
            "Epoch:  1\n",
            "192/2343.75 loss: 0.5492099880245683 \n",
            "Epoch:  1\n",
            "193/2343.75 loss: 0.5485926041590798 \n",
            "Epoch:  1\n",
            "194/2343.75 loss: 0.5476177392861782 \n",
            "Epoch:  1\n",
            "195/2343.75 loss: 0.5474252192949762 \n",
            "Epoch:  1\n",
            "196/2343.75 loss: 0.5468508675316264 \n",
            "Epoch:  1\n",
            "197/2343.75 loss: 0.5470907448819189 \n",
            "Epoch:  1\n",
            "198/2343.75 loss: 0.5467947034081023 \n",
            "Epoch:  1\n",
            "199/2343.75 loss: 0.5459966999292374 \n",
            "Epoch:  1\n",
            "200/2343.75 loss: 0.5454741898757308 \n",
            "Epoch:  1\n",
            "201/2343.75 loss: 0.5454270245120076 \n",
            "Epoch:  1\n",
            "202/2343.75 loss: 0.5448437118765168 \n",
            "Epoch:  1\n",
            "203/2343.75 loss: 0.5440027629043541 \n",
            "Epoch:  1\n",
            "204/2343.75 loss: 0.543471289989425 \n",
            "Epoch:  1\n",
            "205/2343.75 loss: 0.5429074307089871 \n",
            "Epoch:  1\n",
            "206/2343.75 loss: 0.5425561836376283 \n",
            "Epoch:  1\n",
            "207/2343.75 loss: 0.5417555990413978 \n",
            "Epoch:  1\n",
            "208/2343.75 loss: 0.5411412513427187 \n",
            "Epoch:  1\n",
            "209/2343.75 loss: 0.5406580756107966 \n",
            "Epoch:  1\n",
            "210/2343.75 loss: 0.5403662387511177 \n",
            "Epoch:  1\n",
            "211/2343.75 loss: 0.5399064851819344 \n",
            "Epoch:  1\n",
            "212/2343.75 loss: 0.539209246635437 \n",
            "Epoch:  1\n",
            "213/2343.75 loss: 0.5387413089520463 \n",
            "Epoch:  1\n",
            "214/2343.75 loss: 0.538468409277672 \n",
            "Epoch:  1\n",
            "215/2343.75 loss: 0.5381078863585437 \n",
            "Epoch:  1\n",
            "216/2343.75 loss: 0.5373022677162276 \n",
            "Epoch:  1\n",
            "217/2343.75 loss: 0.5368117032521361 \n",
            "Epoch:  1\n",
            "218/2343.75 loss: 0.5364479496054453 \n",
            "Epoch:  1\n",
            "219/2343.75 loss: 0.5357072858647867 \n",
            "Epoch:  1\n",
            "220/2343.75 loss: 0.5351828227485467 \n",
            "Epoch:  1\n",
            "221/2343.75 loss: 0.5347917567233782 \n",
            "Epoch:  1\n",
            "222/2343.75 loss: 0.5338090426985993 \n",
            "Epoch:  1\n",
            "223/2343.75 loss: 0.533118638875229 \n",
            "Epoch:  1\n",
            "224/2343.75 loss: 0.5331609275605943 \n",
            "Epoch:  1\n",
            "225/2343.75 loss: 0.5329599418735083 \n",
            "Epoch:  1\n",
            "226/2343.75 loss: 0.5320834048781626 \n",
            "Epoch:  1\n",
            "227/2343.75 loss: 0.5317276109728897 \n",
            "Epoch:  1\n",
            "228/2343.75 loss: 0.5320932651711343 \n",
            "Epoch:  1\n",
            "229/2343.75 loss: 0.5318881283635678 \n",
            "Epoch:  1\n",
            "230/2343.75 loss: 0.5316145394271586 \n",
            "Epoch:  1\n",
            "231/2343.75 loss: 0.5311402830070463 \n",
            "Epoch:  1\n",
            "232/2343.75 loss: 0.5308611825300389 \n",
            "Epoch:  1\n",
            "233/2343.75 loss: 0.5303937545698932 \n",
            "Epoch:  1\n",
            "234/2343.75 loss: 0.5300040411188247 \n",
            "Epoch:  1\n",
            "235/2343.75 loss: 0.5298790263675027 \n",
            "Epoch:  1\n",
            "236/2343.75 loss: 0.5297491017273207 \n",
            "Epoch:  1\n",
            "237/2343.75 loss: 0.5294077535386846 \n",
            "Epoch:  1\n",
            "238/2343.75 loss: 0.5289328226733906 \n",
            "Epoch:  1\n",
            "239/2343.75 loss: 0.5285786882042884 \n",
            "Epoch:  1\n",
            "240/2343.75 loss: 0.5282797012091672 \n",
            "Epoch:  1\n",
            "241/2343.75 loss: 0.5280036242786518 \n",
            "Epoch:  1\n",
            "242/2343.75 loss: 0.527604505971626 \n",
            "Epoch:  1\n",
            "243/2343.75 loss: 0.5276561129044314 \n",
            "Epoch:  1\n",
            "244/2343.75 loss: 0.526760504926954 \n",
            "Epoch:  1\n",
            "245/2343.75 loss: 0.5261667151518954 \n",
            "Epoch:  1\n",
            "246/2343.75 loss: 0.5257822331870615 \n",
            "Epoch:  1\n",
            "247/2343.75 loss: 0.5257803697980219 \n",
            "Epoch:  1\n",
            "248/2343.75 loss: 0.5254927004676267 \n",
            "Epoch:  1\n",
            "249/2343.75 loss: 0.5249525517225265 \n",
            "Epoch:  1\n",
            "250/2343.75 loss: 0.5244213612193606 \n",
            "Epoch:  1\n",
            "251/2343.75 loss: 0.523852809199265 \n",
            "Epoch:  1\n",
            "252/2343.75 loss: 0.5232256033675001 \n",
            "Epoch:  1\n",
            "253/2343.75 loss: 0.5227207154270233 \n",
            "Epoch:  1\n",
            "254/2343.75 loss: 0.5220528916985381 \n",
            "Epoch:  1\n",
            "255/2343.75 loss: 0.5214779117377475 \n",
            "Epoch:  1\n",
            "256/2343.75 loss: 0.5207122725503454 \n",
            "Epoch:  1\n",
            "257/2343.75 loss: 0.5204300377951112 \n",
            "Epoch:  1\n",
            "258/2343.75 loss: 0.5205340186371307 \n",
            "Epoch:  1\n",
            "259/2343.75 loss: 0.5199108762236742 \n",
            "Epoch:  1\n",
            "260/2343.75 loss: 0.5191711350647426 \n",
            "Epoch:  1\n",
            "261/2343.75 loss: 0.5188584101336603 \n",
            "Epoch:  1\n",
            "262/2343.75 loss: 0.5185991999314312 \n",
            "Epoch:  1\n",
            "263/2343.75 loss: 0.5183397915327188 \n",
            "Epoch:  1\n",
            "264/2343.75 loss: 0.5174950580551939 \n",
            "Epoch:  1\n",
            "265/2343.75 loss: 0.5170989352509492 \n",
            "Epoch:  1\n",
            "266/2343.75 loss: 0.5171936636560419 \n",
            "Epoch:  1\n",
            "267/2343.75 loss: 0.5168193712163327 \n",
            "Epoch:  1\n",
            "268/2343.75 loss: 0.5166588539749273 \n",
            "Epoch:  1\n",
            "269/2343.75 loss: 0.516535132571503 \n",
            "Epoch:  1\n",
            "270/2343.75 loss: 0.5162967137744946 \n",
            "Epoch:  1\n",
            "271/2343.75 loss: 0.5160297648433376 \n",
            "Epoch:  1\n",
            "272/2343.75 loss: 0.5155950793416508 \n",
            "Epoch:  1\n",
            "273/2343.75 loss: 0.5150702372737175 \n",
            "Epoch:  1\n",
            "274/2343.75 loss: 0.5147830898111517 \n",
            "Epoch:  1\n",
            "275/2343.75 loss: 0.5145664550903915 \n",
            "Epoch:  1\n",
            "276/2343.75 loss: 0.5142595499000825 \n",
            "Epoch:  1\n",
            "277/2343.75 loss: 0.5140019013084096 \n",
            "Epoch:  1\n",
            "278/2343.75 loss: 0.5134653140353472 \n",
            "Epoch:  1\n",
            "279/2343.75 loss: 0.5134083443454334 \n",
            "Epoch:  1\n",
            "280/2343.75 loss: 0.513017586753886 \n",
            "Epoch:  1\n",
            "281/2343.75 loss: 0.5132249211165922 \n",
            "Epoch:  1\n",
            "282/2343.75 loss: 0.5127494836443305 \n",
            "Epoch:  1\n",
            "283/2343.75 loss: 0.5122967335120053 \n",
            "Epoch:  1\n",
            "284/2343.75 loss: 0.5116680735035947 \n",
            "Epoch:  1\n",
            "285/2343.75 loss: 0.5110905387184836 \n",
            "Epoch:  1\n",
            "286/2343.75 loss: 0.5107787132055502 \n",
            "Epoch:  1\n",
            "287/2343.75 loss: 0.5102590480819345 \n",
            "Epoch:  1\n",
            "288/2343.75 loss: 0.5108847299455359 \n",
            "Epoch:  1\n",
            "289/2343.75 loss: 0.5106818090225088 \n",
            "Epoch:  1\n",
            "290/2343.75 loss: 0.5105155870267206 \n",
            "Epoch:  1\n",
            "291/2343.75 loss: 0.5097287206412995 \n",
            "Epoch:  1\n",
            "292/2343.75 loss: 0.5093969947852373 \n",
            "Epoch:  1\n",
            "293/2343.75 loss: 0.5092557141570007 \n",
            "Epoch:  1\n",
            "294/2343.75 loss: 0.5091530201798778 \n",
            "Epoch:  1\n",
            "295/2343.75 loss: 0.5086211922603685 \n",
            "Epoch:  1\n",
            "296/2343.75 loss: 0.5084395578212609 \n",
            "Epoch:  1\n",
            "297/2343.75 loss: 0.5081047281722895 \n",
            "Epoch:  1\n",
            "298/2343.75 loss: 0.5080052364032005 \n",
            "Epoch:  1\n",
            "299/2343.75 loss: 0.5078682255744934 \n",
            "Epoch:  1\n",
            "300/2343.75 loss: 0.5080876017725745 \n",
            "Epoch:  1\n",
            "301/2343.75 loss: 0.507907208524003 \n",
            "Epoch:  1\n",
            "302/2343.75 loss: 0.5079669392738405 \n",
            "Epoch:  1\n",
            "303/2343.75 loss: 0.5076363585693272 \n",
            "Epoch:  1\n",
            "304/2343.75 loss: 0.5071400054165575 \n",
            "Epoch:  1\n",
            "305/2343.75 loss: 0.5068370134027955 \n",
            "Epoch:  1\n",
            "306/2343.75 loss: 0.5068208015120379 \n",
            "Epoch:  1\n",
            "307/2343.75 loss: 0.5070010006621286 \n",
            "Epoch:  1\n",
            "308/2343.75 loss: 0.5073981035295814 \n",
            "Epoch:  1\n",
            "309/2343.75 loss: 0.5070766764302408 \n",
            "Epoch:  1\n",
            "310/2343.75 loss: 0.5068457885570464 \n",
            "Epoch:  1\n",
            "311/2343.75 loss: 0.5064956400638971 \n",
            "Epoch:  1\n",
            "312/2343.75 loss: 0.5064832663383728 \n",
            "Epoch:  1\n",
            "313/2343.75 loss: 0.5061946517912446 \n",
            "Epoch:  1\n",
            "314/2343.75 loss: 0.5062993847188495 \n",
            "Epoch:  1\n",
            "315/2343.75 loss: 0.5062395640755002 \n",
            "Epoch:  1\n",
            "316/2343.75 loss: 0.5058935200942427 \n",
            "Epoch:  1\n",
            "317/2343.75 loss: 0.5057460914992686 \n",
            "Epoch:  1\n",
            "318/2343.75 loss: 0.5054377323034042 \n",
            "Epoch:  1\n",
            "319/2343.75 loss: 0.5051696772687138 \n",
            "Epoch:  1\n",
            "320/2343.75 loss: 0.5051946466399874 \n",
            "Epoch:  1\n",
            "321/2343.75 loss: 0.5049756436799624 \n",
            "Epoch:  1\n",
            "322/2343.75 loss: 0.5046743410106045 \n",
            "Epoch:  1\n",
            "323/2343.75 loss: 0.5041533956925074 \n",
            "Epoch:  1\n",
            "324/2343.75 loss: 0.5040447744039389 \n",
            "Epoch:  1\n",
            "325/2343.75 loss: 0.5036548052463063 \n",
            "Epoch:  1\n",
            "326/2343.75 loss: 0.5032786498193712 \n",
            "Epoch:  1\n",
            "327/2343.75 loss: 0.50301812398361 \n",
            "Epoch:  1\n",
            "328/2343.75 loss: 0.502572922630513 \n",
            "Epoch:  1\n",
            "329/2343.75 loss: 0.5021912518775824 \n",
            "Epoch:  1\n",
            "330/2343.75 loss: 0.501812307406048 \n",
            "Epoch:  1\n",
            "331/2343.75 loss: 0.5011914092374136 \n",
            "Epoch:  1\n",
            "332/2343.75 loss: 0.5010070705914998 \n",
            "Epoch:  1\n",
            "333/2343.75 loss: 0.5007555171579658 \n",
            "Epoch:  1\n",
            "334/2343.75 loss: 0.5006003522161228 \n",
            "Epoch:  1\n",
            "335/2343.75 loss: 0.5005945407208943 \n",
            "Epoch:  1\n",
            "336/2343.75 loss: 0.5005542467892701 \n",
            "Epoch:  1\n",
            "337/2343.75 loss: 0.5003012327047495 \n",
            "Epoch:  1\n",
            "338/2343.75 loss: 0.5000266550564836 \n",
            "Epoch:  1\n",
            "339/2343.75 loss: 0.49985041697235666 \n",
            "Epoch:  1\n",
            "340/2343.75 loss: 0.49987676774651424 \n",
            "Epoch:  1\n",
            "341/2343.75 loss: 0.499677831840794 \n",
            "Epoch:  1\n",
            "342/2343.75 loss: 0.49960405833519583 \n",
            "Epoch:  1\n",
            "343/2343.75 loss: 0.49961267532997355 \n",
            "Epoch:  1\n",
            "344/2343.75 loss: 0.4995832417322242 \n",
            "Epoch:  1\n",
            "345/2343.75 loss: 0.4997416886291063 \n",
            "Epoch:  1\n",
            "346/2343.75 loss: 0.4995545250194561 \n",
            "Epoch:  1\n",
            "347/2343.75 loss: 0.49947456690086717 \n",
            "Epoch:  1\n",
            "348/2343.75 loss: 0.4995479341222768 \n",
            "Epoch:  1\n",
            "349/2343.75 loss: 0.49931190763201033 \n",
            "Epoch:  1\n",
            "350/2343.75 loss: 0.49886351356818803 \n",
            "Epoch:  1\n",
            "351/2343.75 loss: 0.4986341626976024 \n",
            "Epoch:  1\n",
            "352/2343.75 loss: 0.4983377519984421 \n",
            "Epoch:  1\n",
            "353/2343.75 loss: 0.49765653857740305 \n",
            "Epoch:  1\n",
            "354/2343.75 loss: 0.49773360599934213 \n",
            "Epoch:  1\n",
            "355/2343.75 loss: 0.49714366321483355 \n",
            "Epoch:  1\n",
            "356/2343.75 loss: 0.49701419823309956 \n",
            "Epoch:  1\n",
            "357/2343.75 loss: 0.49709993575871325 \n",
            "Epoch:  1\n",
            "358/2343.75 loss: 0.49685786137341786 \n",
            "Epoch:  1\n",
            "359/2343.75 loss: 0.4964982498851087 \n",
            "Epoch:  1\n",
            "360/2343.75 loss: 0.49595684333191026 \n",
            "Epoch:  1\n",
            "361/2343.75 loss: 0.4955552918476294 \n",
            "Epoch:  1\n",
            "362/2343.75 loss: 0.49523414971086277 \n",
            "Epoch:  1\n",
            "363/2343.75 loss: 0.4952475536655594 \n",
            "Epoch:  1\n",
            "364/2343.75 loss: 0.49520765298033415 \n",
            "Epoch:  1\n",
            "365/2343.75 loss: 0.49482312651931265 \n",
            "Epoch:  1\n",
            "366/2343.75 loss: 0.49465571900154653 \n",
            "Epoch:  1\n",
            "367/2343.75 loss: 0.49444111266537855 \n",
            "Epoch:  1\n",
            "368/2343.75 loss: 0.49436514253215735 \n",
            "Epoch:  1\n",
            "369/2343.75 loss: 0.4942514935860763 \n",
            "Epoch:  1\n",
            "370/2343.75 loss: 0.49419110306511027 \n",
            "Epoch:  1\n",
            "371/2343.75 loss: 0.49391521489427936 \n",
            "Epoch:  1\n",
            "372/2343.75 loss: 0.49377366964363223 \n",
            "Epoch:  1\n",
            "373/2343.75 loss: 0.4935724732869449 \n",
            "Epoch:  1\n",
            "374/2343.75 loss: 0.49332682450612386 \n",
            "Epoch:  1\n",
            "375/2343.75 loss: 0.49330754578113556 \n",
            "Epoch:  1\n",
            "376/2343.75 loss: 0.4929219942826491 \n",
            "Epoch:  1\n",
            "377/2343.75 loss: 0.49265915872874083 \n",
            "Epoch:  1\n",
            "378/2343.75 loss: 0.4923708555251753 \n",
            "Epoch:  1\n",
            "379/2343.75 loss: 0.4921698017339957 \n",
            "Epoch:  1\n",
            "380/2343.75 loss: 0.49175728016638065 \n",
            "Epoch:  1\n",
            "381/2343.75 loss: 0.4918149331321267 \n",
            "Epoch:  1\n",
            "382/2343.75 loss: 0.49143714304069935 \n",
            "Epoch:  1\n",
            "383/2343.75 loss: 0.49124153633601964 \n",
            "Epoch:  1\n",
            "384/2343.75 loss: 0.49140035510063174 \n",
            "Epoch:  1\n",
            "385/2343.75 loss: 0.49112731017596983 \n",
            "Epoch:  1\n",
            "386/2343.75 loss: 0.4913678215455639 \n",
            "Epoch:  1\n",
            "387/2343.75 loss: 0.49132292940444555 \n",
            "Epoch:  1\n",
            "388/2343.75 loss: 0.49112343083310556 \n",
            "Epoch:  1\n",
            "389/2343.75 loss: 0.4908580307012949 \n",
            "Epoch:  1\n",
            "390/2343.75 loss: 0.49084677827327755 \n",
            "Epoch:  1\n",
            "391/2343.75 loss: 0.4905909854842692 \n",
            "Epoch:  1\n",
            "392/2343.75 loss: 0.490468343326457 \n",
            "Epoch:  1\n",
            "393/2343.75 loss: 0.49018865362339215 \n",
            "Epoch:  1\n",
            "394/2343.75 loss: 0.49007000908066956 \n",
            "Epoch:  1\n",
            "395/2343.75 loss: 0.4899302401927986 \n",
            "Epoch:  1\n",
            "396/2343.75 loss: 0.48952358112827654 \n",
            "Epoch:  1\n",
            "397/2343.75 loss: 0.4891746539117104 \n",
            "Epoch:  1\n",
            "398/2343.75 loss: 0.4891338967589806 \n",
            "Epoch:  1\n",
            "399/2343.75 loss: 0.4889494997262955 \n",
            "Epoch:  1\n",
            "400/2343.75 loss: 0.48875347686527376 \n",
            "Epoch:  1\n",
            "401/2343.75 loss: 0.4887045711575456 \n",
            "Epoch:  1\n",
            "402/2343.75 loss: 0.4885261208188741 \n",
            "Epoch:  1\n",
            "403/2343.75 loss: 0.4886736061313365 \n",
            "Epoch:  1\n",
            "404/2343.75 loss: 0.4884953792448397 \n",
            "Epoch:  1\n",
            "405/2343.75 loss: 0.4883166137587261 \n",
            "Epoch:  1\n",
            "406/2343.75 loss: 0.48806780386322546 \n",
            "Epoch:  1\n",
            "407/2343.75 loss: 0.48808324373528067 \n",
            "Epoch:  1\n",
            "408/2343.75 loss: 0.48785221212359103 \n",
            "Epoch:  1\n",
            "409/2343.75 loss: 0.4880522797747356 \n",
            "Epoch:  1\n",
            "410/2343.75 loss: 0.48768955628657285 \n",
            "Epoch:  1\n",
            "411/2343.75 loss: 0.4875255225642214 \n",
            "Epoch:  1\n",
            "412/2343.75 loss: 0.4874412237010337 \n",
            "Epoch:  1\n",
            "413/2343.75 loss: 0.4870371365028879 \n",
            "Epoch:  1\n",
            "414/2343.75 loss: 0.486896833023393 \n",
            "Epoch:  1\n",
            "415/2343.75 loss: 0.486464282641044 \n",
            "Epoch:  1\n",
            "416/2343.75 loss: 0.48624566847757755 \n",
            "Epoch:  1\n",
            "417/2343.75 loss: 0.48612189328556427 \n",
            "Epoch:  1\n",
            "418/2343.75 loss: 0.48601781852490006 \n",
            "Epoch:  1\n",
            "419/2343.75 loss: 0.485928585983458 \n",
            "Epoch:  1\n",
            "420/2343.75 loss: 0.4855679955306925 \n",
            "Epoch:  1\n",
            "421/2343.75 loss: 0.4853510893351659 \n",
            "Epoch:  1\n",
            "422/2343.75 loss: 0.48518573159867145 \n",
            "Epoch:  1\n",
            "423/2343.75 loss: 0.4849508728761718 \n",
            "Epoch:  1\n",
            "424/2343.75 loss: 0.48486430757185994 \n",
            "Epoch:  1\n",
            "425/2343.75 loss: 0.48456761331905224 \n",
            "Epoch:  1\n",
            "426/2343.75 loss: 0.48452123029449784 \n",
            "Epoch:  1\n",
            "427/2343.75 loss: 0.48422514271234796 \n",
            "Epoch:  1\n",
            "428/2343.75 loss: 0.484213262796402 \n",
            "Epoch:  1\n",
            "429/2343.75 loss: 0.48392680890338363 \n",
            "Epoch:  1\n",
            "430/2343.75 loss: 0.48359187344940524 \n",
            "Epoch:  1\n",
            "431/2343.75 loss: 0.4832287160334764 \n",
            "Epoch:  1\n",
            "432/2343.75 loss: 0.4831624053779981 \n",
            "Epoch:  1\n",
            "433/2343.75 loss: 0.48280970944512275 \n",
            "Epoch:  1\n",
            "434/2343.75 loss: 0.48234089016914367 \n",
            "Epoch:  1\n",
            "435/2343.75 loss: 0.48230679080300376 \n",
            "Epoch:  1\n",
            "436/2343.75 loss: 0.4822369680933876 \n",
            "Epoch:  1\n",
            "437/2343.75 loss: 0.4818904912907239 \n",
            "Epoch:  1\n",
            "438/2343.75 loss: 0.4817941857497621 \n",
            "Epoch:  1\n",
            "439/2343.75 loss: 0.48150739893317224 \n",
            "Epoch:  1\n",
            "440/2343.75 loss: 0.4812939806995478 \n",
            "Epoch:  1\n",
            "441/2343.75 loss: 0.48103655294864966 \n",
            "Epoch:  1\n",
            "442/2343.75 loss: 0.48086766451260843 \n",
            "Epoch:  1\n",
            "443/2343.75 loss: 0.4806546942070798 \n",
            "Epoch:  1\n",
            "444/2343.75 loss: 0.48050014390034623 \n",
            "Epoch:  1\n",
            "445/2343.75 loss: 0.4806164224452502 \n",
            "Epoch:  1\n",
            "446/2343.75 loss: 0.4804007304994852 \n",
            "Epoch:  1\n",
            "447/2343.75 loss: 0.4807371068080621 \n",
            "Epoch:  1\n",
            "448/2343.75 loss: 0.4808565174418197 \n",
            "Epoch:  1\n",
            "449/2343.75 loss: 0.48073496374819014 \n",
            "Epoch:  1\n",
            "450/2343.75 loss: 0.48056904380178767 \n",
            "Epoch:  1\n",
            "451/2343.75 loss: 0.4804999139324754 \n",
            "Epoch:  1\n",
            "452/2343.75 loss: 0.48053760902244785 \n",
            "Epoch:  1\n",
            "453/2343.75 loss: 0.48060413446720474 \n",
            "Epoch:  1\n",
            "454/2343.75 loss: 0.48053184125449633 \n",
            "Epoch:  1\n",
            "455/2343.75 loss: 0.4805404837324954 \n",
            "Epoch:  1\n",
            "456/2343.75 loss: 0.480606931761005 \n",
            "Epoch:  1\n",
            "457/2343.75 loss: 0.48047925486314763 \n",
            "Epoch:  1\n",
            "458/2343.75 loss: 0.48043099467790723 \n",
            "Epoch:  1\n",
            "459/2343.75 loss: 0.48016502118628956 \n",
            "Epoch:  1\n",
            "460/2343.75 loss: 0.48004597069901656 \n",
            "Epoch:  1\n",
            "461/2343.75 loss: 0.47962745649990063 \n",
            "Epoch:  1\n",
            "462/2343.75 loss: 0.47949853133949294 \n",
            "Epoch:  1\n",
            "463/2343.75 loss: 0.4792575217654993 \n",
            "Epoch:  1\n",
            "464/2343.75 loss: 0.4789855261643728 \n",
            "Epoch:  1\n",
            "465/2343.75 loss: 0.4786347888186254 \n",
            "Epoch:  1\n",
            "466/2343.75 loss: 0.47841332364643924 \n",
            "Epoch:  1\n",
            "467/2343.75 loss: 0.47846285362019497 \n",
            "Epoch:  1\n",
            "468/2343.75 loss: 0.47850322926730743 \n",
            "Epoch:  1\n",
            "469/2343.75 loss: 0.4782835009884327 \n",
            "Epoch:  1\n",
            "470/2343.75 loss: 0.4781937833930783 \n",
            "Epoch:  1\n",
            "471/2343.75 loss: 0.47764044993762245 \n",
            "Epoch:  1\n",
            "472/2343.75 loss: 0.47718775562201693 \n",
            "Epoch:  1\n",
            "473/2343.75 loss: 0.47723654905955 \n",
            "Epoch:  1\n",
            "474/2343.75 loss: 0.4771370614202399 \n",
            "Epoch:  1\n",
            "475/2343.75 loss: 0.47734485078258676 \n",
            "Epoch:  1\n",
            "476/2343.75 loss: 0.4773012227232351 \n",
            "Epoch:  1\n",
            "477/2343.75 loss: 0.4771018440379258 \n",
            "Epoch:  1\n",
            "478/2343.75 loss: 0.4768728986538029 \n",
            "Epoch:  1\n",
            "479/2343.75 loss: 0.4767431579530239 \n",
            "Epoch:  1\n",
            "480/2343.75 loss: 0.4765510441979351 \n",
            "Epoch:  1\n",
            "481/2343.75 loss: 0.47640714148268165 \n",
            "Epoch:  1\n",
            "482/2343.75 loss: 0.4764319573746952 \n",
            "Epoch:  1\n",
            "483/2343.75 loss: 0.476297861657852 \n",
            "Epoch:  1\n",
            "484/2343.75 loss: 0.4760368895284908 \n",
            "Epoch:  1\n",
            "485/2343.75 loss: 0.4761018992206197 \n",
            "Epoch:  1\n",
            "486/2343.75 loss: 0.47588796243530523 \n",
            "Epoch:  1\n",
            "487/2343.75 loss: 0.4758250816557251 \n",
            "Epoch:  1\n",
            "488/2343.75 loss: 0.4756293488182661 \n",
            "Epoch:  1\n",
            "489/2343.75 loss: 0.47551834972537294 \n",
            "Epoch:  1\n",
            "490/2343.75 loss: 0.47508436459630665 \n",
            "Epoch:  1\n",
            "491/2343.75 loss: 0.4749210457491681 \n",
            "Epoch:  1\n",
            "492/2343.75 loss: 0.47480150700582696 \n",
            "Epoch:  1\n",
            "493/2343.75 loss: 0.4743822845611495 \n",
            "Epoch:  1\n",
            "494/2343.75 loss: 0.4741557810643707 \n",
            "Epoch:  1\n",
            "495/2343.75 loss: 0.4738350192144994 \n",
            "Epoch:  1\n",
            "496/2343.75 loss: 0.4736454670937728 \n",
            "Epoch:  1\n",
            "497/2343.75 loss: 0.47331306434060677 \n",
            "Epoch:  1\n",
            "498/2343.75 loss: 0.4731023530802411 \n",
            "Epoch:  1\n",
            "499/2343.75 loss: 0.4728361926674843 \n",
            "Epoch:  1\n",
            "500/2343.75 loss: 0.4728231961260775 \n",
            "Epoch:  1\n",
            "501/2343.75 loss: 0.47303850604480957 \n",
            "Epoch:  1\n",
            "502/2343.75 loss: 0.47276309671743255 \n",
            "Epoch:  1\n",
            "503/2343.75 loss: 0.47287965045561864 \n",
            "Epoch:  1\n",
            "504/2343.75 loss: 0.4727280115137006 \n",
            "Epoch:  1\n",
            "505/2343.75 loss: 0.4724612674223104 \n",
            "Epoch:  1\n",
            "506/2343.75 loss: 0.4723813811820404 \n",
            "Epoch:  1\n",
            "507/2343.75 loss: 0.4721401343195457 \n",
            "Epoch:  1\n",
            "508/2343.75 loss: 0.4720822009450092 \n",
            "Epoch:  1\n",
            "509/2343.75 loss: 0.4718605495551053 \n",
            "Epoch:  1\n",
            "510/2343.75 loss: 0.47177704568011886 \n",
            "Epoch:  1\n",
            "511/2343.75 loss: 0.47167701687430963 \n",
            "Epoch:  1\n",
            "512/2343.75 loss: 0.4716006013733602 \n",
            "Epoch:  1\n",
            "513/2343.75 loss: 0.47151941756793964 \n",
            "Epoch:  1\n",
            "514/2343.75 loss: 0.4714433439726968 \n",
            "Epoch:  1\n",
            "515/2343.75 loss: 0.4714467538766159 \n",
            "Epoch:  1\n",
            "516/2343.75 loss: 0.47135797679078417 \n",
            "Epoch:  1\n",
            "517/2343.75 loss: 0.4710378706685365 \n",
            "Epoch:  1\n",
            "518/2343.75 loss: 0.47090586032251863 \n",
            "Epoch:  1\n",
            "519/2343.75 loss: 0.47094947151266614 \n",
            "Epoch:  1\n",
            "520/2343.75 loss: 0.4707288188348576 \n",
            "Epoch:  1\n",
            "521/2343.75 loss: 0.47072162950175933 \n",
            "Epoch:  1\n",
            "522/2343.75 loss: 0.47036529276840544 \n",
            "Epoch:  1\n",
            "523/2343.75 loss: 0.47013110639029787 \n",
            "Epoch:  1\n",
            "524/2343.75 loss: 0.469930210908254 \n",
            "Epoch:  1\n",
            "525/2343.75 loss: 0.4697705945814517 \n",
            "Epoch:  1\n",
            "526/2343.75 loss: 0.469652766520882 \n",
            "Epoch:  1\n",
            "527/2343.75 loss: 0.46941615866892267 \n",
            "Epoch:  1\n",
            "528/2343.75 loss: 0.46908336996130773 \n",
            "Epoch:  1\n",
            "529/2343.75 loss: 0.46892488177092573 \n",
            "Epoch:  1\n",
            "530/2343.75 loss: 0.4686804325679377 \n",
            "Epoch:  1\n",
            "531/2343.75 loss: 0.4685136439432775 \n",
            "Epoch:  1\n",
            "532/2343.75 loss: 0.46844047660228233 \n",
            "Epoch:  1\n",
            "533/2343.75 loss: 0.4680122562059749 \n",
            "Epoch:  1\n",
            "534/2343.75 loss: 0.46782856146308865 \n",
            "Epoch:  1\n",
            "535/2343.75 loss: 0.4677529566601586 \n",
            "Epoch:  1\n",
            "536/2343.75 loss: 0.46765291438311396 \n",
            "Epoch:  1\n",
            "537/2343.75 loss: 0.4674229828232283 \n",
            "Epoch:  1\n",
            "538/2343.75 loss: 0.4671385243175645 \n",
            "Epoch:  1\n",
            "539/2343.75 loss: 0.46706194720334476 \n",
            "Epoch:  1\n",
            "540/2343.75 loss: 0.4668692437701216 \n",
            "Epoch:  1\n",
            "541/2343.75 loss: 0.4664688498551555 \n",
            "Epoch:  1\n",
            "542/2343.75 loss: 0.46649193906432795 \n",
            "Epoch:  1\n",
            "543/2343.75 loss: 0.466377492829719 \n",
            "Epoch:  1\n",
            "544/2343.75 loss: 0.4662742702785982 \n",
            "Epoch:  1\n",
            "545/2343.75 loss: 0.4660699056945878 \n",
            "Epoch:  1\n",
            "546/2343.75 loss: 0.46590402155733196 \n",
            "Epoch:  1\n",
            "547/2343.75 loss: 0.46576862254717055 \n",
            "Epoch:  1\n",
            "548/2343.75 loss: 0.4657617593506429 \n",
            "Epoch:  1\n",
            "549/2343.75 loss: 0.4655383382602171 \n",
            "Epoch:  1\n",
            "550/2343.75 loss: 0.4655765127897695 \n",
            "Epoch:  1\n",
            "551/2343.75 loss: 0.4651936193847138 \n",
            "Epoch:  1\n",
            "552/2343.75 loss: 0.46520129331197274 \n",
            "Epoch:  1\n",
            "553/2343.75 loss: 0.46505966809467286 \n",
            "Epoch:  1\n",
            "554/2343.75 loss: 0.4649852012191807 \n",
            "Epoch:  1\n",
            "555/2343.75 loss: 0.4650465795140472 \n",
            "Epoch:  1\n",
            "556/2343.75 loss: 0.46500515402754505 \n",
            "Epoch:  1\n",
            "557/2343.75 loss: 0.46480471552700126 \n",
            "Epoch:  1\n",
            "558/2343.75 loss: 0.4644864142900716 \n",
            "Epoch:  1\n",
            "559/2343.75 loss: 0.4644026330006974 \n",
            "Epoch:  1\n",
            "560/2343.75 loss: 0.4642221828832983 \n",
            "Epoch:  1\n",
            "561/2343.75 loss: 0.4643258054697641 \n",
            "Epoch:  1\n",
            "562/2343.75 loss: 0.4639808221672187 \n",
            "Epoch:  1\n",
            "563/2343.75 loss: 0.4636670463579766 \n",
            "Epoch:  1\n",
            "564/2343.75 loss: 0.46345870167808195 \n",
            "Epoch:  1\n",
            "565/2343.75 loss: 0.46330881545274083 \n",
            "Epoch:  1\n",
            "566/2343.75 loss: 0.4630796374257069 \n",
            "Epoch:  1\n",
            "567/2343.75 loss: 0.4628629203833325 \n",
            "Epoch:  1\n",
            "568/2343.75 loss: 0.4624832002527894 \n",
            "Epoch:  1\n",
            "569/2343.75 loss: 0.46238847748752226 \n",
            "Epoch:  1\n",
            "570/2343.75 loss: 0.462284654509374 \n",
            "Epoch:  1\n",
            "571/2343.75 loss: 0.46231804076920857 \n",
            "Epoch:  1\n",
            "572/2343.75 loss: 0.46213937047576403 \n",
            "Epoch:  1\n",
            "573/2343.75 loss: 0.4619019026073014 \n",
            "Epoch:  1\n",
            "574/2343.75 loss: 0.4617939330443092 \n",
            "Epoch:  1\n",
            "575/2343.75 loss: 0.4617306509624339 \n",
            "Epoch:  1\n",
            "576/2343.75 loss: 0.46150629501305585 \n",
            "Epoch:  1\n",
            "577/2343.75 loss: 0.4615122692653052 \n",
            "Epoch:  1\n",
            "578/2343.75 loss: 0.4613325708322163 \n",
            "Epoch:  1\n",
            "579/2343.75 loss: 0.46138635824980406 \n",
            "Epoch:  1\n",
            "580/2343.75 loss: 0.46120734666690566 \n",
            "Epoch:  1\n",
            "581/2343.75 loss: 0.46112126718784113 \n",
            "Epoch:  1\n",
            "582/2343.75 loss: 0.46095500457041466 \n",
            "Epoch:  1\n",
            "583/2343.75 loss: 0.4609794671557946 \n",
            "Epoch:  1\n",
            "584/2343.75 loss: 0.460874535003279 \n",
            "Epoch:  1\n",
            "585/2343.75 loss: 0.46069517383294706 \n",
            "Epoch:  1\n",
            "586/2343.75 loss: 0.46040632399322634 \n",
            "Epoch:  1\n",
            "587/2343.75 loss: 0.4602214607099692 \n",
            "Epoch:  1\n",
            "588/2343.75 loss: 0.46008837711325323 \n",
            "Epoch:  1\n",
            "589/2343.75 loss: 0.46000369234105287 \n",
            "Epoch:  1\n",
            "590/2343.75 loss: 0.4598187525108986 \n",
            "Epoch:  1\n",
            "591/2343.75 loss: 0.4597944775596261 \n",
            "Epoch:  1\n",
            "592/2343.75 loss: 0.45966534120537983 \n",
            "Epoch:  1\n",
            "593/2343.75 loss: 0.4597532533686169 \n",
            "Epoch:  1\n",
            "594/2343.75 loss: 0.45981350258117965 \n",
            "Epoch:  1\n",
            "595/2343.75 loss: 0.459507632650585 \n",
            "Epoch:  1\n",
            "596/2343.75 loss: 0.459407152617397 \n",
            "Epoch:  1\n",
            "597/2343.75 loss: 0.45947597674203156 \n",
            "Epoch:  1\n",
            "598/2343.75 loss: 0.45947911482123976 \n",
            "Epoch:  1\n",
            "599/2343.75 loss: 0.45920844299097857 \n",
            "Epoch:  1\n",
            "600/2343.75 loss: 0.4590612943229977 \n",
            "Epoch:  1\n",
            "601/2343.75 loss: 0.45881350558758577 \n",
            "Epoch:  1\n",
            "602/2343.75 loss: 0.458772173988483 \n",
            "Epoch:  1\n",
            "603/2343.75 loss: 0.4584166133739301 \n",
            "Epoch:  1\n",
            "604/2343.75 loss: 0.4583825859156522 \n",
            "Epoch:  1\n",
            "605/2343.75 loss: 0.4582158780038947 \n",
            "Epoch:  1\n",
            "606/2343.75 loss: 0.4579957431977035 \n",
            "Epoch:  1\n",
            "607/2343.75 loss: 0.45793565872468445 \n",
            "Epoch:  1\n",
            "608/2343.75 loss: 0.45784553562479063 \n",
            "Epoch:  1\n",
            "609/2343.75 loss: 0.4576935377277312 \n",
            "Epoch:  1\n",
            "610/2343.75 loss: 0.45757426070308527 \n",
            "Epoch:  1\n",
            "611/2343.75 loss: 0.4574122144018902 \n",
            "Epoch:  1\n",
            "612/2343.75 loss: 0.45724150402316455 \n",
            "Epoch:  1\n",
            "613/2343.75 loss: 0.45715207317753026 \n",
            "Epoch:  1\n",
            "614/2343.75 loss: 0.45699989151179304 \n",
            "Epoch:  1\n",
            "615/2343.75 loss: 0.4569516513351496 \n",
            "Epoch:  1\n",
            "616/2343.75 loss: 0.4569763068345223 \n",
            "Epoch:  1\n",
            "617/2343.75 loss: 0.45716071509813416 \n",
            "Epoch:  1\n",
            "618/2343.75 loss: 0.4569425476964726 \n",
            "Epoch:  1\n",
            "619/2343.75 loss: 0.45685581997517616 \n",
            "Epoch:  1\n",
            "620/2343.75 loss: 0.45677844500964004 \n",
            "Epoch:  1\n",
            "621/2343.75 loss: 0.456547635975758 \n",
            "Epoch:  1\n",
            "622/2343.75 loss: 0.45638216613383775 \n",
            "Epoch:  1\n",
            "623/2343.75 loss: 0.4562944024323653 \n",
            "Epoch:  1\n",
            "624/2343.75 loss: 0.45609774465560915 \n",
            "Epoch:  1\n",
            "625/2343.75 loss: 0.4560400610343336 \n",
            "Epoch:  1\n",
            "626/2343.75 loss: 0.4561286241624154 \n",
            "Epoch:  1\n",
            "627/2343.75 loss: 0.4559853895075002 \n",
            "Epoch:  1\n",
            "628/2343.75 loss: 0.455831431622346 \n",
            "Epoch:  1\n",
            "629/2343.75 loss: 0.4557996002454606 \n",
            "Epoch:  1\n",
            "630/2343.75 loss: 0.4556571600837677 \n",
            "Epoch:  1\n",
            "631/2343.75 loss: 0.45550790042439593 \n",
            "Epoch:  1\n",
            "632/2343.75 loss: 0.4553595491306853 \n",
            "Epoch:  1\n",
            "633/2343.75 loss: 0.45523791583930656 \n",
            "Epoch:  1\n",
            "634/2343.75 loss: 0.4551697823475665 \n",
            "Epoch:  1\n",
            "635/2343.75 loss: 0.45511744593111975 \n",
            "Epoch:  1\n",
            "636/2343.75 loss: 0.45491053765198125 \n",
            "Epoch:  1\n",
            "637/2343.75 loss: 0.45465495897497876 \n",
            "Epoch:  1\n",
            "638/2343.75 loss: 0.4544061383637651 \n",
            "Epoch:  1\n",
            "639/2343.75 loss: 0.4543209433089942 \n",
            "Epoch:  1\n",
            "640/2343.75 loss: 0.45430900363393956 \n",
            "Epoch:  1\n",
            "641/2343.75 loss: 0.45422087897578506 \n",
            "Epoch:  1\n",
            "642/2343.75 loss: 0.45405779030423143 \n",
            "Epoch:  1\n",
            "643/2343.75 loss: 0.45396423659154345 \n",
            "Epoch:  1\n",
            "644/2343.75 loss: 0.45386962987655816 \n",
            "Epoch:  1\n",
            "645/2343.75 loss: 0.4537751230846618 \n",
            "Epoch:  1\n",
            "646/2343.75 loss: 0.45366960529013434 \n",
            "Epoch:  1\n",
            "647/2343.75 loss: 0.45356824230632664 \n",
            "Epoch:  1\n",
            "648/2343.75 loss: 0.4535905344093158 \n",
            "Epoch:  1\n",
            "649/2343.75 loss: 0.45351293495068185 \n",
            "Epoch:  1\n",
            "650/2343.75 loss: 0.45332803919384923 \n",
            "Epoch:  1\n",
            "651/2343.75 loss: 0.4531280050979801 \n",
            "Epoch:  1\n",
            "652/2343.75 loss: 0.4529643856548058 \n",
            "Epoch:  1\n",
            "653/2343.75 loss: 0.4529959261873082 \n",
            "Epoch:  1\n",
            "654/2343.75 loss: 0.4528045889530473 \n",
            "Epoch:  1\n",
            "655/2343.75 loss: 0.45286595298931365 \n",
            "Epoch:  1\n",
            "656/2343.75 loss: 0.45259457667850106 \n",
            "Epoch:  1\n",
            "657/2343.75 loss: 0.4524565622618133 \n",
            "Epoch:  1\n",
            "658/2343.75 loss: 0.45243164621101345 \n",
            "Epoch:  1\n",
            "659/2343.75 loss: 0.45233974063938315 \n",
            "Epoch:  1\n",
            "660/2343.75 loss: 0.45238014969991663 \n",
            "Epoch:  1\n",
            "661/2343.75 loss: 0.4520735295877716 \n",
            "Epoch:  1\n",
            "662/2343.75 loss: 0.4518038142320797 \n",
            "Epoch:  1\n",
            "663/2343.75 loss: 0.45172537598444756 \n",
            "Epoch:  1\n",
            "664/2343.75 loss: 0.4516310812835407 \n",
            "Epoch:  1\n",
            "665/2343.75 loss: 0.4513761463823977 \n",
            "Epoch:  1\n",
            "666/2343.75 loss: 0.45118968776319696 \n",
            "Epoch:  1\n",
            "667/2343.75 loss: 0.4513789182473086 \n",
            "Epoch:  1\n",
            "668/2343.75 loss: 0.45131329659745656 \n",
            "Epoch:  1\n",
            "669/2343.75 loss: 0.45111289006560595 \n",
            "Epoch:  1\n",
            "670/2343.75 loss: 0.45090966128734467 \n",
            "Epoch:  1\n",
            "671/2343.75 loss: 0.4509010925622923 \n",
            "Epoch:  1\n",
            "672/2343.75 loss: 0.45067728725974665 \n",
            "Epoch:  1\n",
            "673/2343.75 loss: 0.4507856388474077 \n",
            "Epoch:  1\n",
            "674/2343.75 loss: 0.45055752091937595 \n",
            "Epoch:  1\n",
            "675/2343.75 loss: 0.450655486897604 \n",
            "Epoch:  1\n",
            "676/2343.75 loss: 0.45067554478983407 \n",
            "Epoch:  1\n",
            "677/2343.75 loss: 0.45038395148638777 \n",
            "Epoch:  1\n",
            "678/2343.75 loss: 0.45030936309093933 \n",
            "Epoch:  1\n",
            "679/2343.75 loss: 0.4500875160536345 \n",
            "Epoch:  1\n",
            "680/2343.75 loss: 0.4499247029489358 \n",
            "Epoch:  1\n",
            "681/2343.75 loss: 0.4499519445067627 \n",
            "Epoch:  1\n",
            "682/2343.75 loss: 0.449847552778082 \n",
            "Epoch:  1\n",
            "683/2343.75 loss: 0.4498446275703391 \n",
            "Epoch:  1\n",
            "684/2343.75 loss: 0.44965007353003006 \n",
            "Epoch:  1\n",
            "685/2343.75 loss: 0.4495311799098034 \n",
            "Epoch:  1\n",
            "686/2343.75 loss: 0.449511270335668 \n",
            "Epoch:  1\n",
            "687/2343.75 loss: 0.44950773865850857 \n",
            "Epoch:  1\n",
            "688/2343.75 loss: 0.4493964597777462 \n",
            "Epoch:  1\n",
            "689/2343.75 loss: 0.4495393946118977 \n",
            "Epoch:  1\n",
            "690/2343.75 loss: 0.4493454457968258 \n",
            "Epoch:  1\n",
            "691/2343.75 loss: 0.4491851927572592 \n",
            "Epoch:  1\n",
            "692/2343.75 loss: 0.44919661573345354 \n",
            "Epoch:  1\n",
            "693/2343.75 loss: 0.4492216148878037 \n",
            "Epoch:  1\n",
            "694/2343.75 loss: 0.4489940949481168 \n",
            "Epoch:  1\n",
            "695/2343.75 loss: 0.4488312527537346 \n",
            "Epoch:  1\n",
            "696/2343.75 loss: 0.44867797719696845 \n",
            "Epoch:  1\n",
            "697/2343.75 loss: 0.44865105418557766 \n",
            "Epoch:  1\n",
            "698/2343.75 loss: 0.44858072730093046 \n",
            "Epoch:  1\n",
            "699/2343.75 loss: 0.44841905764171053 \n",
            "Epoch:  1\n",
            "700/2343.75 loss: 0.4482718725262287 \n",
            "Epoch:  1\n",
            "701/2343.75 loss: 0.44829836933531314 \n",
            "Epoch:  1\n",
            "702/2343.75 loss: 0.4481007013771985 \n",
            "Epoch:  1\n",
            "703/2343.75 loss: 0.4478093816433102 \n",
            "Epoch:  1\n",
            "704/2343.75 loss: 0.44748630692772834 \n",
            "Epoch:  1\n",
            "705/2343.75 loss: 0.4476120927853895 \n",
            "Epoch:  1\n",
            "706/2343.75 loss: 0.4474168889003225 \n",
            "Epoch:  1\n",
            "707/2343.75 loss: 0.4473793222069067 \n",
            "Epoch:  1\n",
            "708/2343.75 loss: 0.44714215919060163 \n",
            "Epoch:  1\n",
            "709/2343.75 loss: 0.4470961786072019 \n",
            "Epoch:  1\n",
            "710/2343.75 loss: 0.44691248684492796 \n",
            "Epoch:  1\n",
            "711/2343.75 loss: 0.44686448695452025 \n",
            "Epoch:  1\n",
            "712/2343.75 loss: 0.44694624396059324 \n",
            "Epoch:  1\n",
            "713/2343.75 loss: 0.4467057234265891 \n",
            "Epoch:  1\n",
            "714/2343.75 loss: 0.4465544900277278 \n",
            "Epoch:  1\n",
            "715/2343.75 loss: 0.4464762453926342 \n",
            "Epoch:  1\n",
            "716/2343.75 loss: 0.4463128392250801 \n",
            "Epoch:  1\n",
            "717/2343.75 loss: 0.4461580631958741 \n",
            "Epoch:  1\n",
            "718/2343.75 loss: 0.44603280376990084 \n",
            "Epoch:  1\n",
            "719/2343.75 loss: 0.445869568693969 \n",
            "Epoch:  1\n",
            "720/2343.75 loss: 0.4458209190636501 \n",
            "Epoch:  1\n",
            "721/2343.75 loss: 0.4455931195344291 \n",
            "Epoch:  1\n",
            "722/2343.75 loss: 0.4454008084857118 \n",
            "Epoch:  1\n",
            "723/2343.75 loss: 0.445404631327529 \n",
            "Epoch:  1\n",
            "724/2343.75 loss: 0.4454087895360486 \n",
            "Epoch:  1\n",
            "725/2343.75 loss: 0.4452200428102956 \n",
            "Epoch:  1\n",
            "726/2343.75 loss: 0.4449925211141657 \n",
            "Epoch:  1\n",
            "727/2343.75 loss: 0.44478748841108856 \n",
            "Epoch:  1\n",
            "728/2343.75 loss: 0.44470100319434586 \n",
            "Epoch:  1\n",
            "729/2343.75 loss: 0.44484478872116295 \n",
            "Epoch:  1\n",
            "730/2343.75 loss: 0.4446969182726133 \n",
            "Epoch:  1\n",
            "731/2343.75 loss: 0.44453121966025866 \n",
            "Epoch:  1\n",
            "732/2343.75 loss: 0.4443728247248297 \n",
            "Epoch:  1\n",
            "733/2343.75 loss: 0.4442788078690745 \n",
            "Epoch:  1\n",
            "734/2343.75 loss: 0.4442055214019049 \n",
            "Epoch:  1\n",
            "735/2343.75 loss: 0.4440297190423893 \n",
            "Epoch:  1\n",
            "736/2343.75 loss: 0.44397153415220564 \n",
            "Epoch:  1\n",
            "737/2343.75 loss: 0.44385395964309776 \n",
            "Epoch:  1\n",
            "738/2343.75 loss: 0.4438018904890517 \n",
            "Epoch:  1\n",
            "739/2343.75 loss: 0.4438363453826389 \n",
            "Epoch:  1\n",
            "740/2343.75 loss: 0.44376529598364783 \n",
            "Epoch:  1\n",
            "741/2343.75 loss: 0.4436283963067191 \n",
            "Epoch:  1\n",
            "742/2343.75 loss: 0.44360208916471594 \n",
            "Epoch:  1\n",
            "743/2343.75 loss: 0.4435940073622811 \n",
            "Epoch:  1\n",
            "744/2343.75 loss: 0.4433514183799692 \n",
            "Epoch:  1\n",
            "745/2343.75 loss: 0.4432533478049746 \n",
            "Epoch:  1\n",
            "746/2343.75 loss: 0.44313254633262616 \n",
            "Epoch:  1\n",
            "747/2343.75 loss: 0.4430108455652222 \n",
            "Epoch:  1\n",
            "748/2343.75 loss: 0.4430868077739695 \n",
            "Epoch:  1\n",
            "749/2343.75 loss: 0.44305047345161436 \n",
            "Epoch:  1\n",
            "750/2343.75 loss: 0.44299087468063464 \n",
            "Epoch:  1\n",
            "751/2343.75 loss: 0.44300661719542866 \n",
            "Epoch:  1\n",
            "752/2343.75 loss: 0.44309929737531806 \n",
            "Epoch:  1\n",
            "753/2343.75 loss: 0.44296996208495737 \n",
            "Epoch:  1\n",
            "754/2343.75 loss: 0.44285917700521205 \n",
            "Epoch:  1\n",
            "755/2343.75 loss: 0.4426899251168367 \n",
            "Epoch:  1\n",
            "756/2343.75 loss: 0.44258371956124803 \n",
            "Epoch:  1\n",
            "757/2343.75 loss: 0.4425123820282853 \n",
            "Epoch:  1\n",
            "758/2343.75 loss: 0.4423788201274922 \n",
            "Epoch:  1\n",
            "759/2343.75 loss: 0.4423185957105536 \n",
            "Epoch:  1\n",
            "760/2343.75 loss: 0.44219368308508444 \n",
            "Epoch:  1\n",
            "761/2343.75 loss: 0.44220543365309556 \n",
            "Epoch:  1\n",
            "762/2343.75 loss: 0.4422211914199989 \n",
            "Epoch:  1\n",
            "763/2343.75 loss: 0.44200458631153505 \n",
            "Epoch:  1\n",
            "764/2343.75 loss: 0.44196018487020255 \n",
            "Epoch:  1\n",
            "765/2343.75 loss: 0.4417883021168547 \n",
            "Epoch:  1\n",
            "766/2343.75 loss: 0.4416069823184878 \n",
            "Epoch:  1\n",
            "767/2343.75 loss: 0.44163236413927126 \n",
            "Epoch:  1\n",
            "768/2343.75 loss: 0.44162490619019507 \n",
            "Epoch:  1\n",
            "769/2343.75 loss: 0.44141257997457084 \n",
            "Epoch:  1\n",
            "770/2343.75 loss: 0.4415311998507701 \n",
            "Epoch:  1\n",
            "771/2343.75 loss: 0.4413844636916497 \n",
            "Epoch:  1\n",
            "772/2343.75 loss: 0.44129930187937205 \n",
            "Epoch:  1\n",
            "773/2343.75 loss: 0.4414056769406149 \n",
            "Epoch:  1\n",
            "774/2343.75 loss: 0.44123572699485286 \n",
            "Epoch:  1\n",
            "775/2343.75 loss: 0.4412099837719165 \n",
            "Epoch:  1\n",
            "776/2343.75 loss: 0.4410913723929662 \n",
            "Epoch:  1\n",
            "777/2343.75 loss: 0.4408818702081788 \n",
            "Epoch:  1\n",
            "778/2343.75 loss: 0.441011735923177 \n",
            "Epoch:  1\n",
            "779/2343.75 loss: 0.44091867468295953 \n",
            "Epoch:  1\n",
            "780/2343.75 loss: 0.44078780128769623 \n",
            "Epoch:  1\n",
            "781/2343.75 loss: 0.4406470760817418 \n",
            "Epoch:  1\n",
            "782/2343.75 loss: 0.4405186153082403 \n",
            "Epoch:  1\n",
            "783/2343.75 loss: 0.44065969425956814 \n",
            "Epoch:  1\n",
            "784/2343.75 loss: 0.44047784402871587 \n",
            "Epoch:  1\n",
            "785/2343.75 loss: 0.44040997519747904 \n",
            "Epoch:  1\n",
            "786/2343.75 loss: 0.44033021147551044 \n",
            "Epoch:  1\n",
            "787/2343.75 loss: 0.4401612665674408 \n",
            "Epoch:  1\n",
            "788/2343.75 loss: 0.4400651002035999 \n",
            "Epoch:  1\n",
            "789/2343.75 loss: 0.43988631016845947 \n",
            "Epoch:  1\n",
            "790/2343.75 loss: 0.43992491712612386 \n",
            "Epoch:  1\n",
            "791/2343.75 loss: 0.4399051515742986 \n",
            "Epoch:  1\n",
            "792/2343.75 loss: 0.43972363477997944 \n",
            "Epoch:  1\n",
            "793/2343.75 loss: 0.4396625760970848 \n",
            "Epoch:  1\n",
            "794/2343.75 loss: 0.43962945908120593 \n",
            "Epoch:  1\n",
            "795/2343.75 loss: 0.4396463598138723 \n",
            "Epoch:  1\n",
            "796/2343.75 loss: 0.4395737451696336 \n",
            "Epoch:  1\n",
            "797/2343.75 loss: 0.43954942620039583 \n",
            "Epoch:  1\n",
            "798/2343.75 loss: 0.4394204204386853 \n",
            "Epoch:  1\n",
            "799/2343.75 loss: 0.4392593604326248 \n",
            "Epoch:  1\n",
            "800/2343.75 loss: 0.43920749544948523 \n",
            "Epoch:  1\n",
            "801/2343.75 loss: 0.43932369708123054 \n",
            "Epoch:  1\n",
            "802/2343.75 loss: 0.43922350259676374 \n",
            "Epoch:  1\n",
            "803/2343.75 loss: 0.43906970285064545 \n",
            "Epoch:  1\n",
            "804/2343.75 loss: 0.4390059442994017 \n",
            "Epoch:  1\n",
            "805/2343.75 loss: 0.43888234374836715 \n",
            "Epoch:  1\n",
            "806/2343.75 loss: 0.4389334883831691 \n",
            "Epoch:  1\n",
            "807/2343.75 loss: 0.4387791059026034 \n",
            "Epoch:  1\n",
            "808/2343.75 loss: 0.43870878687453063 \n",
            "Epoch:  1\n",
            "809/2343.75 loss: 0.4385997567279839 \n",
            "Epoch:  1\n",
            "810/2343.75 loss: 0.43855862879870705 \n",
            "Epoch:  1\n",
            "811/2343.75 loss: 0.438494622010022 \n",
            "Epoch:  1\n",
            "812/2343.75 loss: 0.43841922356487056 \n",
            "Epoch:  1\n",
            "813/2343.75 loss: 0.43843344974283505 \n",
            "Epoch:  1\n",
            "814/2343.75 loss: 0.4381906005136806 \n",
            "Epoch:  1\n",
            "815/2343.75 loss: 0.4381090521666349 \n",
            "Epoch:  1\n",
            "816/2343.75 loss: 0.43793628666654794 \n",
            "Epoch:  1\n",
            "817/2343.75 loss: 0.4378648663354095 \n",
            "Epoch:  1\n",
            "818/2343.75 loss: 0.4376722597886646 \n",
            "Epoch:  1\n",
            "819/2343.75 loss: 0.4376190829931236 \n",
            "Epoch:  1\n",
            "820/2343.75 loss: 0.4374465360130375 \n",
            "Epoch:  1\n",
            "821/2343.75 loss: 0.43740076392236416 \n",
            "Epoch:  1\n",
            "822/2343.75 loss: 0.43720030929454184 \n",
            "Epoch:  1\n",
            "823/2343.75 loss: 0.43713877053515426 \n",
            "Epoch:  1\n",
            "824/2343.75 loss: 0.4370555863235936 \n",
            "Epoch:  1\n",
            "825/2343.75 loss: 0.43698208059294746 \n",
            "Epoch:  1\n",
            "826/2343.75 loss: 0.4368925226498688 \n",
            "Epoch:  1\n",
            "827/2343.75 loss: 0.43687172230459065 \n",
            "Epoch:  1\n",
            "828/2343.75 loss: 0.4369429567395131 \n",
            "Epoch:  1\n",
            "829/2343.75 loss: 0.4368252696402102 \n",
            "Epoch:  1\n",
            "830/2343.75 loss: 0.4366419053895379 \n",
            "Epoch:  1\n",
            "831/2343.75 loss: 0.4364987047962271 \n",
            "Epoch:  1\n",
            "832/2343.75 loss: 0.43641752584212395 \n",
            "Epoch:  1\n",
            "833/2343.75 loss: 0.4361631695708211 \n",
            "Epoch:  1\n",
            "834/2343.75 loss: 0.43604609082915824 \n",
            "Epoch:  1\n",
            "835/2343.75 loss: 0.4360652399298392 \n",
            "Epoch:  1\n",
            "836/2343.75 loss: 0.4360346351722378 \n",
            "Epoch:  1\n",
            "837/2343.75 loss: 0.43618215103066904 \n",
            "Epoch:  1\n",
            "838/2343.75 loss: 0.43617917153278324 \n",
            "Epoch:  1\n",
            "839/2343.75 loss: 0.4361777598127013 \n",
            "Epoch:  1\n",
            "840/2343.75 loss: 0.43612206395046604 \n",
            "Epoch:  1\n",
            "841/2343.75 loss: 0.43604054390463864 \n",
            "Epoch:  1\n",
            "842/2343.75 loss: 0.4358550511822016 \n",
            "Epoch:  1\n",
            "843/2343.75 loss: 0.43570110715607896 \n",
            "Epoch:  1\n",
            "844/2343.75 loss: 0.43560807650258554 \n",
            "Epoch:  1\n",
            "845/2343.75 loss: 0.4355318391823318 \n",
            "Epoch:  1\n",
            "846/2343.75 loss: 0.4354318032896645 \n",
            "Epoch:  1\n",
            "847/2343.75 loss: 0.43542429377799324 \n",
            "Epoch:  1\n",
            "848/2343.75 loss: 0.43525451398920817 \n",
            "Epoch:  1\n",
            "849/2343.75 loss: 0.4352568816963364 \n",
            "Epoch:  1\n",
            "850/2343.75 loss: 0.43514161508806165 \n",
            "Epoch:  1\n",
            "851/2343.75 loss: 0.4349815831322905 \n",
            "Epoch:  1\n",
            "852/2343.75 loss: 0.43492945852271275 \n",
            "Epoch:  1\n",
            "853/2343.75 loss: 0.4348695161087172 \n",
            "Epoch:  1\n",
            "854/2343.75 loss: 0.4349166883188382 \n",
            "Epoch:  1\n",
            "855/2343.75 loss: 0.4349650148232685 \n",
            "Epoch:  1\n",
            "856/2343.75 loss: 0.4348023464096886 \n",
            "Epoch:  1\n",
            "857/2343.75 loss: 0.43471103842372383 \n",
            "Epoch:  1\n",
            "858/2343.75 loss: 0.43460577679526957 \n",
            "Epoch:  1\n",
            "859/2343.75 loss: 0.4345424322021562 \n",
            "Epoch:  1\n",
            "860/2343.75 loss: 0.4345151265427626 \n",
            "Epoch:  1\n",
            "861/2343.75 loss: 0.4343348544635518 \n",
            "Epoch:  1\n",
            "862/2343.75 loss: 0.4341260412511516 \n",
            "Epoch:  1\n",
            "863/2343.75 loss: 0.43396126718639777 \n",
            "Epoch:  1\n",
            "864/2343.75 loss: 0.4338260404808673 \n",
            "Epoch:  1\n",
            "865/2343.75 loss: 0.4339652336944609 \n",
            "Epoch:  1\n",
            "866/2343.75 loss: 0.43399347457712495 \n",
            "Epoch:  1\n",
            "867/2343.75 loss: 0.43396947867653335 \n",
            "Epoch:  1\n",
            "868/2343.75 loss: 0.433908304030245 \n",
            "Epoch:  1\n",
            "869/2343.75 loss: 0.43384168129200223 \n",
            "Epoch:  1\n",
            "870/2343.75 loss: 0.433777311964369 \n",
            "Epoch:  1\n",
            "871/2343.75 loss: 0.4338739367219013 \n",
            "Epoch:  1\n",
            "872/2343.75 loss: 0.4337858855895024 \n",
            "Epoch:  1\n",
            "873/2343.75 loss: 0.4336203144569146 \n",
            "Epoch:  1\n",
            "874/2343.75 loss: 0.433515249473708 \n",
            "Epoch:  1\n",
            "875/2343.75 loss: 0.4334942001900444 \n",
            "Epoch:  1\n",
            "876/2343.75 loss: 0.4335812616545363 \n",
            "Epoch:  1\n",
            "877/2343.75 loss: 0.4334919282101553 \n",
            "Epoch:  1\n",
            "878/2343.75 loss: 0.43350638818754406 \n",
            "Epoch:  1\n",
            "879/2343.75 loss: 0.43353286681866104 \n",
            "Epoch:  1\n",
            "880/2343.75 loss: 0.4333572334754751 \n",
            "Epoch:  1\n",
            "881/2343.75 loss: 0.43332803328764413 \n",
            "Epoch:  1\n",
            "882/2343.75 loss: 0.4332773179355861 \n",
            "Epoch:  1\n",
            "883/2343.75 loss: 0.4331164805227006 \n",
            "Epoch:  1\n",
            "884/2343.75 loss: 0.43303632306850565 \n",
            "Epoch:  1\n",
            "885/2343.75 loss: 0.43296596026111134 \n",
            "Epoch:  1\n",
            "886/2343.75 loss: 0.43299561421457244 \n",
            "Epoch:  1\n",
            "887/2343.75 loss: 0.43279122445430307 \n",
            "Epoch:  1\n",
            "888/2343.75 loss: 0.43267305346186974 \n",
            "Epoch:  1\n",
            "889/2343.75 loss: 0.4326270271720511 \n",
            "Epoch:  1\n",
            "890/2343.75 loss: 0.43253407486025597 \n",
            "Epoch:  1\n",
            "891/2343.75 loss: 0.4323886268575897 \n",
            "Epoch:  1\n",
            "892/2343.75 loss: 0.43240567663014934 \n",
            "Epoch:  1\n",
            "893/2343.75 loss: 0.43233278618762955 \n",
            "Epoch:  1\n",
            "894/2343.75 loss: 0.43224272443262557 \n",
            "Epoch:  1\n",
            "895/2343.75 loss: 0.43212444686131285 \n",
            "Epoch:  1\n",
            "896/2343.75 loss: 0.43215047899496595 \n",
            "Epoch:  1\n",
            "897/2343.75 loss: 0.43209213023395476 \n",
            "Epoch:  1\n",
            "898/2343.75 loss: 0.432162926827575 \n",
            "Epoch:  1\n",
            "899/2343.75 loss: 0.43215691670775414 \n",
            "Epoch:  1\n",
            "900/2343.75 loss: 0.4321313737069859 \n",
            "Epoch:  1\n",
            "901/2343.75 loss: 0.4319519130162019 \n",
            "Epoch:  1\n",
            "902/2343.75 loss: 0.4318974676685344 \n",
            "Epoch:  1\n",
            "903/2343.75 loss: 0.43180010014469117 \n",
            "Epoch:  1\n",
            "904/2343.75 loss: 0.4318439335618888 \n",
            "Epoch:  1\n",
            "905/2343.75 loss: 0.43193184166620374 \n",
            "Epoch:  1\n",
            "906/2343.75 loss: 0.4319360383848897 \n",
            "Epoch:  1\n",
            "907/2343.75 loss: 0.431746716918793 \n",
            "Epoch:  1\n",
            "908/2343.75 loss: 0.4317853834861183 \n",
            "Epoch:  1\n",
            "909/2343.75 loss: 0.4316745327887954 \n",
            "Epoch:  1\n",
            "910/2343.75 loss: 0.4315639139693864 \n",
            "Epoch:  1\n",
            "911/2343.75 loss: 0.43147144687214967 \n",
            "Epoch:  1\n",
            "912/2343.75 loss: 0.43143948760834344 \n",
            "Epoch:  1\n",
            "913/2343.75 loss: 0.43132232030124057 \n",
            "Epoch:  1\n",
            "914/2343.75 loss: 0.4312863796623678 \n",
            "Epoch:  1\n",
            "915/2343.75 loss: 0.4311432178136303 \n",
            "Epoch:  1\n",
            "916/2343.75 loss: 0.4310540884018204 \n",
            "Epoch:  1\n",
            "917/2343.75 loss: 0.43111310939838166 \n",
            "Epoch:  1\n",
            "918/2343.75 loss: 0.4309845561074484 \n",
            "Epoch:  1\n",
            "919/2343.75 loss: 0.43092372276536794 \n",
            "Epoch:  1\n",
            "920/2343.75 loss: 0.43076125248441477 \n",
            "Epoch:  1\n",
            "921/2343.75 loss: 0.4306743191696558 \n",
            "Epoch:  1\n",
            "922/2343.75 loss: 0.43062755831982436 \n",
            "Epoch:  1\n",
            "923/2343.75 loss: 0.4306041861025544 \n",
            "Epoch:  1\n",
            "924/2343.75 loss: 0.4304869263719868 \n",
            "Epoch:  1\n",
            "925/2343.75 loss: 0.43031787378291075 \n",
            "Epoch:  1\n",
            "926/2343.75 loss: 0.4301631620561678 \n",
            "Epoch:  1\n",
            "927/2343.75 loss: 0.4300798583807873 \n",
            "Epoch:  1\n",
            "928/2343.75 loss: 0.42995713612684267 \n",
            "Epoch:  1\n",
            "929/2343.75 loss: 0.4298336985771374 \n",
            "Epoch:  1\n",
            "930/2343.75 loss: 0.4298483905903635 \n",
            "Epoch:  1\n",
            "931/2343.75 loss: 0.4297960228386611 \n",
            "Epoch:  1\n",
            "932/2343.75 loss: 0.4296985272585643 \n",
            "Epoch:  1\n",
            "933/2343.75 loss: 0.4297650405853923 \n",
            "Epoch:  1\n",
            "934/2343.75 loss: 0.42976152544671836 \n",
            "Epoch:  1\n",
            "935/2343.75 loss: 0.42956668130544007 \n",
            "Epoch:  1\n",
            "936/2343.75 loss: 0.42950317538726035 \n",
            "Epoch:  1\n",
            "937/2343.75 loss: 0.42943475293770017 \n",
            "Epoch:  1\n",
            "938/2343.75 loss: 0.42933014915964474 \n",
            "Epoch:  1\n",
            "939/2343.75 loss: 0.42914254340402624 \n",
            "Epoch:  1\n",
            "940/2343.75 loss: 0.4290903508188114 \n",
            "Epoch:  1\n",
            "941/2343.75 loss: 0.42908039007280535 \n",
            "Epoch:  1\n",
            "942/2343.75 loss: 0.4291821909892597 \n",
            "Epoch:  1\n",
            "943/2343.75 loss: 0.4293074354813513 \n",
            "Epoch:  1\n",
            "944/2343.75 loss: 0.4293810789704954 \n",
            "Epoch:  1\n",
            "945/2343.75 loss: 0.42927037140090674 \n",
            "Epoch:  1\n",
            "946/2343.75 loss: 0.42937425356417297 \n",
            "Epoch:  1\n",
            "947/2343.75 loss: 0.4294059027127827 \n",
            "Epoch:  1\n",
            "948/2343.75 loss: 0.429290630998928 \n",
            "Epoch:  1\n",
            "949/2343.75 loss: 0.4292609994191872 \n",
            "Epoch:  1\n",
            "950/2343.75 loss: 0.4292390026410671 \n",
            "Epoch:  1\n",
            "951/2343.75 loss: 0.4292125335792784 \n",
            "Epoch:  1\n",
            "952/2343.75 loss: 0.4291934923729141 \n",
            "Epoch:  1\n",
            "953/2343.75 loss: 0.429107659189831 \n",
            "Epoch:  1\n",
            "954/2343.75 loss: 0.4290759557048688 \n",
            "Epoch:  1\n",
            "955/2343.75 loss: 0.42899950782312507 \n",
            "Epoch:  1\n",
            "956/2343.75 loss: 0.42885939611847995 \n",
            "Epoch:  1\n",
            "957/2343.75 loss: 0.4288648616265902 \n",
            "Epoch:  1\n",
            "958/2343.75 loss: 0.4288300754846199 \n",
            "Epoch:  1\n",
            "959/2343.75 loss: 0.42872273302637043 \n",
            "Epoch:  1\n",
            "960/2343.75 loss: 0.42871595376426047 \n",
            "Epoch:  1\n",
            "961/2343.75 loss: 0.4287817798663326 \n",
            "Epoch:  1\n",
            "962/2343.75 loss: 0.42872288632801386 \n",
            "Epoch:  1\n",
            "963/2343.75 loss: 0.42859472243184865 \n",
            "Epoch:  1\n",
            "964/2343.75 loss: 0.4285550166751437 \n",
            "Epoch:  1\n",
            "965/2343.75 loss: 0.4284468991508395 \n",
            "Epoch:  1\n",
            "966/2343.75 loss: 0.4284038763597315 \n",
            "Epoch:  1\n",
            "967/2343.75 loss: 0.4283802766229742 \n",
            "Epoch:  1\n",
            "968/2343.75 loss: 0.4284435650420263 \n",
            "Epoch:  1\n",
            "969/2343.75 loss: 0.42851078087828826 \n",
            "Epoch:  1\n",
            "970/2343.75 loss: 0.4284059528556837 \n",
            "Epoch:  1\n",
            "971/2343.75 loss: 0.4282606905901138 \n",
            "Epoch:  1\n",
            "972/2343.75 loss: 0.42820512116690457 \n",
            "Epoch:  1\n",
            "973/2343.75 loss: 0.4280977919391783 \n",
            "Epoch:  1\n",
            "974/2343.75 loss: 0.4279831954913262 \n",
            "Epoch:  1\n",
            "975/2343.75 loss: 0.4278385291853156 \n",
            "Epoch:  1\n",
            "976/2343.75 loss: 0.4277079192770906 \n",
            "Epoch:  1\n",
            "977/2343.75 loss: 0.4275370413448913 \n",
            "Epoch:  1\n",
            "978/2343.75 loss: 0.42743698086692317 \n",
            "Epoch:  1\n",
            "979/2343.75 loss: 0.4273326472208208 \n",
            "Epoch:  1\n",
            "980/2343.75 loss: 0.42727271726432803 \n",
            "Epoch:  1\n",
            "981/2343.75 loss: 0.4272370232978566 \n",
            "Epoch:  1\n",
            "982/2343.75 loss: 0.42729660924732504 \n",
            "Epoch:  1\n",
            "983/2343.75 loss: 0.4270211329335362 \n",
            "Epoch:  1\n",
            "984/2343.75 loss: 0.4269881655873381 \n",
            "Epoch:  1\n",
            "985/2343.75 loss: 0.42700514773144926 \n",
            "Epoch:  1\n",
            "986/2343.75 loss: 0.42693294947089394 \n",
            "Epoch:  1\n",
            "987/2343.75 loss: 0.42702847343647043 \n",
            "Epoch:  1\n",
            "988/2343.75 loss: 0.4269672666117926 \n",
            "Epoch:  1\n",
            "989/2343.75 loss: 0.4269565403310939 \n",
            "Epoch:  1\n",
            "990/2343.75 loss: 0.4269115175566928 \n",
            "Epoch:  1\n",
            "991/2343.75 loss: 0.4267562455497682 \n",
            "Epoch:  1\n",
            "992/2343.75 loss: 0.4267232569773032 \n",
            "Epoch:  1\n",
            "993/2343.75 loss: 0.42663566598769886 \n",
            "Epoch:  1\n",
            "994/2343.75 loss: 0.4266350128393077 \n",
            "Epoch:  1\n",
            "995/2343.75 loss: 0.4265204243601326 \n",
            "Epoch:  1\n",
            "996/2343.75 loss: 0.4264178828615602 \n",
            "Epoch:  1\n",
            "997/2343.75 loss: 0.42635702031229683 \n",
            "Epoch:  1\n",
            "998/2343.75 loss: 0.4263293474793315 \n",
            "Epoch:  1\n",
            "999/2343.75 loss: 0.4262124959975481 \n",
            "Epoch:  1\n",
            "1000/2343.75 loss: 0.42635839309666185 \n",
            "Epoch:  1\n",
            "1001/2343.75 loss: 0.4262454556282647 \n",
            "Epoch:  1\n",
            "1002/2343.75 loss: 0.426104973121393 \n",
            "Epoch:  1\n",
            "1003/2343.75 loss: 0.42609855020604287 \n",
            "Epoch:  1\n",
            "1004/2343.75 loss: 0.4260016782515085 \n",
            "Epoch:  1\n",
            "1005/2343.75 loss: 0.4259685547078106 \n",
            "Epoch:  1\n",
            "1006/2343.75 loss: 0.4258992108622753 \n",
            "Epoch:  1\n",
            "1007/2343.75 loss: 0.4258215727343682 \n",
            "Epoch:  1\n",
            "1008/2343.75 loss: 0.42569453639187826 \n",
            "Epoch:  1\n",
            "1009/2343.75 loss: 0.42558934985115976 \n",
            "Epoch:  1\n",
            "1010/2343.75 loss: 0.42564584863115135 \n",
            "Epoch:  1\n",
            "1011/2343.75 loss: 0.42543616736300377 \n",
            "Epoch:  1\n",
            "1012/2343.75 loss: 0.42548672669555215 \n",
            "Epoch:  1\n",
            "1013/2343.75 loss: 0.42547850191769515 \n",
            "Epoch:  1\n",
            "1014/2343.75 loss: 0.4254538198850425 \n",
            "Epoch:  1\n",
            "1015/2343.75 loss: 0.42536719408854257 \n",
            "Epoch:  1\n",
            "1016/2343.75 loss: 0.4252809227948812 \n",
            "Epoch:  1\n",
            "1017/2343.75 loss: 0.42518218246623435 \n",
            "Epoch:  1\n",
            "1018/2343.75 loss: 0.4250995115074489 \n",
            "Epoch:  1\n",
            "1019/2343.75 loss: 0.42501815503426627 \n",
            "Epoch:  1\n",
            "1020/2343.75 loss: 0.4250125707684488 \n",
            "Epoch:  1\n",
            "1021/2343.75 loss: 0.42503096759610914 \n",
            "Epoch:  1\n",
            "1022/2343.75 loss: 0.42501511052615015 \n",
            "Epoch:  1\n",
            "1023/2343.75 loss: 0.4249874240049394 \n",
            "Epoch:  1\n",
            "1024/2343.75 loss: 0.42484780574717174 \n",
            "Epoch:  1\n",
            "1025/2343.75 loss: 0.4247843118467991 \n",
            "Epoch:  1\n",
            "1026/2343.75 loss: 0.42488855537038 \n",
            "Epoch:  1\n",
            "1027/2343.75 loss: 0.42481754228820595 \n",
            "Epoch:  1\n",
            "1028/2343.75 loss: 0.4248260036762532 \n",
            "Epoch:  1\n",
            "1029/2343.75 loss: 0.42482096302567174 \n",
            "Epoch:  1\n",
            "1030/2343.75 loss: 0.4248577024928582 \n",
            "Epoch:  1\n",
            "1031/2343.75 loss: 0.42480421651068123 \n",
            "Epoch:  1\n",
            "1032/2343.75 loss: 0.4247306446974875 \n",
            "Epoch:  1\n",
            "1033/2343.75 loss: 0.42464264986651085 \n",
            "Epoch:  1\n",
            "1034/2343.75 loss: 0.42455159159386213 \n",
            "Epoch:  1\n",
            "1035/2343.75 loss: 0.42453018416251453 \n",
            "Epoch:  1\n",
            "1036/2343.75 loss: 0.4244646617824799 \n",
            "Epoch:  1\n",
            "1037/2343.75 loss: 0.42440324411628794 \n",
            "Epoch:  1\n",
            "1038/2343.75 loss: 0.42425041190412205 \n",
            "Epoch:  1\n",
            "1039/2343.75 loss: 0.4242588352555266 \n",
            "Epoch:  1\n",
            "1040/2343.75 loss: 0.42418672676904057 \n",
            "Epoch:  1\n",
            "1041/2343.75 loss: 0.4241181576406429 \n",
            "Epoch:  1\n",
            "1042/2343.75 loss: 0.4241141025852037 \n",
            "Epoch:  1\n",
            "1043/2343.75 loss: 0.4240685613875873 \n",
            "Epoch:  1\n",
            "1044/2343.75 loss: 0.4239357913937865 \n",
            "Epoch:  1\n",
            "1045/2343.75 loss: 0.42390603312855696 \n",
            "Epoch:  1\n",
            "1046/2343.75 loss: 0.42379922239748546 \n",
            "Epoch:  1\n",
            "1047/2343.75 loss: 0.4236722770680226 \n",
            "Epoch:  1\n",
            "1048/2343.75 loss: 0.4235282518303428 \n",
            "Epoch:  1\n",
            "1049/2343.75 loss: 0.42357217934869584 \n",
            "Epoch:  1\n",
            "1050/2343.75 loss: 0.4234721871442051 \n",
            "Epoch:  1\n",
            "1051/2343.75 loss: 0.4234926352001188 \n",
            "Epoch:  1\n",
            "1052/2343.75 loss: 0.42343764686346735 \n",
            "Epoch:  1\n",
            "1053/2343.75 loss: 0.423389218380035 \n",
            "Epoch:  1\n",
            "1054/2343.75 loss: 0.42331015296739427 \n",
            "Epoch:  1\n",
            "1055/2343.75 loss: 0.42337387937092874 \n",
            "Epoch:  1\n",
            "1056/2343.75 loss: 0.423276967828073 \n",
            "Epoch:  1\n",
            "1057/2343.75 loss: 0.42309504734714903 \n",
            "Epoch:  1\n",
            "1058/2343.75 loss: 0.4231135143746285 \n",
            "Epoch:  1\n",
            "1059/2343.75 loss: 0.42300515103171454 \n",
            "Epoch:  1\n",
            "1060/2343.75 loss: 0.4229230916269763 \n",
            "Epoch:  1\n",
            "1061/2343.75 loss: 0.422875233219056 \n",
            "Epoch:  1\n",
            "1062/2343.75 loss: 0.42283640131844963 \n",
            "Epoch:  1\n",
            "1063/2343.75 loss: 0.42264445130258127 \n",
            "Epoch:  1\n",
            "1064/2343.75 loss: 0.4225545031783726 \n",
            "Epoch:  1\n",
            "1065/2343.75 loss: 0.4226199477827124 \n",
            "Epoch:  1\n",
            "1066/2343.75 loss: 0.4224723062592348 \n",
            "Epoch:  1\n",
            "1067/2343.75 loss: 0.42230617726563513 \n",
            "Epoch:  1\n",
            "1068/2343.75 loss: 0.42225669554397255 \n",
            "Epoch:  1\n",
            "1069/2343.75 loss: 0.4221724247821024 \n",
            "Epoch:  1\n",
            "1070/2343.75 loss: 0.4221118375812035 \n",
            "Epoch:  1\n",
            "1071/2343.75 loss: 0.4221639088841517 \n",
            "Epoch:  1\n",
            "1072/2343.75 loss: 0.4222163860751221 \n",
            "Epoch:  1\n",
            "1073/2343.75 loss: 0.42225705775357714 \n",
            "Epoch:  1\n",
            "1074/2343.75 loss: 0.4221031356689542 \n",
            "Epoch:  1\n",
            "1075/2343.75 loss: 0.4220721705861694 \n",
            "Epoch:  1\n",
            "1076/2343.75 loss: 0.42204211114066115 \n",
            "Epoch:  1\n",
            "1077/2343.75 loss: 0.4220678986832914 \n",
            "Epoch:  1\n",
            "1078/2343.75 loss: 0.4220234266964786 \n",
            "Epoch:  1\n",
            "1079/2343.75 loss: 0.42198225833751535 \n",
            "Epoch:  1\n",
            "1080/2343.75 loss: 0.42201256164789863 \n",
            "Epoch:  1\n",
            "1081/2343.75 loss: 0.42195278097653344 \n",
            "Epoch:  1\n",
            "1082/2343.75 loss: 0.42189863481019674 \n",
            "Epoch:  1\n",
            "1083/2343.75 loss: 0.4217656841467227 \n",
            "Epoch:  1\n",
            "1084/2343.75 loss: 0.4216482397048704 \n",
            "Epoch:  1\n",
            "1085/2343.75 loss: 0.42164374196397664 \n",
            "Epoch:  1\n",
            "1086/2343.75 loss: 0.4215060649602481 \n",
            "Epoch:  1\n",
            "1087/2343.75 loss: 0.42141868163119345 \n",
            "Epoch:  1\n",
            "1088/2343.75 loss: 0.42134421981727455 \n",
            "Epoch:  1\n",
            "1089/2343.75 loss: 0.42128530795421076 \n",
            "Epoch:  1\n",
            "1090/2343.75 loss: 0.42113463724020095 \n",
            "Epoch:  1\n",
            "1091/2343.75 loss: 0.421124520892407 \n",
            "Epoch:  1\n",
            "1092/2343.75 loss: 0.42102507438445896 \n",
            "Epoch:  1\n",
            "1093/2343.75 loss: 0.42097751973316044 \n",
            "Epoch:  1\n",
            "1094/2343.75 loss: 0.42089924529262873 \n",
            "Epoch:  1\n",
            "1095/2343.75 loss: 0.4207509958395993 \n",
            "Epoch:  1\n",
            "1096/2343.75 loss: 0.42073155677416374 \n",
            "Epoch:  1\n",
            "1097/2343.75 loss: 0.42074400859669475 \n",
            "Epoch:  1\n",
            "1098/2343.75 loss: 0.42076744680192063 \n",
            "Epoch:  1\n",
            "1099/2343.75 loss: 0.4207194239984859 \n",
            "Epoch:  1\n",
            "1100/2343.75 loss: 0.42065033950013103 \n",
            "Epoch:  1\n",
            "1101/2343.75 loss: 0.42051446721796115 \n",
            "Epoch:  1\n",
            "1102/2343.75 loss: 0.4205378646052975 \n",
            "Epoch:  1\n",
            "1103/2343.75 loss: 0.4204499424043773 \n",
            "Epoch:  1\n",
            "1104/2343.75 loss: 0.42049205799448003 \n",
            "Epoch:  1\n",
            "1105/2343.75 loss: 0.42050993488343674 \n",
            "Epoch:  1\n",
            "1106/2343.75 loss: 0.4203907073674809 \n",
            "Epoch:  1\n",
            "1107/2343.75 loss: 0.4204134421981199 \n",
            "Epoch:  1\n",
            "1108/2343.75 loss: 0.420408768735564 \n",
            "Epoch:  1\n",
            "1109/2343.75 loss: 0.4203041620619662 \n",
            "Epoch:  1\n",
            "1110/2343.75 loss: 0.4202677796919211 \n",
            "Epoch:  1\n",
            "1111/2343.75 loss: 0.4201101886121918 \n",
            "Epoch:  1\n",
            "1112/2343.75 loss: 0.4202023177187612 \n",
            "Epoch:  1\n",
            "1113/2343.75 loss: 0.42012269393652946 \n",
            "Epoch:  1\n",
            "1114/2343.75 loss: 0.42003442503411675 \n",
            "Epoch:  1\n",
            "1115/2343.75 loss: 0.4200220430089581 \n",
            "Epoch:  1\n",
            "1116/2343.75 loss: 0.419953878692142 \n",
            "Epoch:  1\n",
            "1117/2343.75 loss: 0.4198094292915358 \n",
            "Epoch:  1\n",
            "1118/2343.75 loss: 0.41970432640933053 \n",
            "Epoch:  1\n",
            "1119/2343.75 loss: 0.41974296000386985 \n",
            "Epoch:  1\n",
            "1120/2343.75 loss: 0.41962799110867743 \n",
            "Epoch:  1\n",
            "1121/2343.75 loss: 0.4196573898275906 \n",
            "Epoch:  1\n",
            "1122/2343.75 loss: 0.4196404898846033 \n",
            "Epoch:  1\n",
            "1123/2343.75 loss: 0.41943214103261345 \n",
            "Epoch:  1\n",
            "1124/2343.75 loss: 0.419343188908365 \n",
            "Epoch:  1\n",
            "1125/2343.75 loss: 0.4192478495185159 \n",
            "Epoch:  1\n",
            "1126/2343.75 loss: 0.41918526468228323 \n",
            "Epoch:  1\n",
            "1127/2343.75 loss: 0.4190966388778695 \n",
            "Epoch:  1\n",
            "1128/2343.75 loss: 0.4190428899097907 \n",
            "Epoch:  1\n",
            "1129/2343.75 loss: 0.4189872037388582 \n",
            "Epoch:  1\n",
            "1130/2343.75 loss: 0.418884421124804 \n",
            "Epoch:  1\n",
            "1131/2343.75 loss: 0.41877022207889036 \n",
            "Epoch:  1\n",
            "1132/2343.75 loss: 0.41874736056902295 \n",
            "Epoch:  1\n",
            "1133/2343.75 loss: 0.4186852175538712 \n",
            "Epoch:  1\n",
            "1134/2343.75 loss: 0.4186484522100062 \n",
            "Epoch:  1\n",
            "1135/2343.75 loss: 0.41860737004907617 \n",
            "Epoch:  1\n",
            "1136/2343.75 loss: 0.41851953454413954 \n",
            "Epoch:  1\n",
            "1137/2343.75 loss: 0.4184694680498229 \n",
            "Epoch:  1\n",
            "1138/2343.75 loss: 0.4183864920965091 \n",
            "Epoch:  1\n",
            "1139/2343.75 loss: 0.4182925143916356 \n",
            "Epoch:  1\n",
            "1140/2343.75 loss: 0.41822360239198186 \n",
            "Epoch:  1\n",
            "1141/2343.75 loss: 0.41815670556184914 \n",
            "Epoch:  1\n",
            "1142/2343.75 loss: 0.4180535544378134 \n",
            "Epoch:  1\n",
            "1143/2343.75 loss: 0.417982838018895 \n",
            "Epoch:  1\n",
            "1144/2343.75 loss: 0.417960370771229 \n",
            "Epoch:  1\n",
            "1145/2343.75 loss: 0.41788965319307686 \n",
            "Epoch:  1\n",
            "1146/2343.75 loss: 0.41765347075077797 \n",
            "Epoch:  1\n",
            "1147/2343.75 loss: 0.4175715986833963 \n",
            "Epoch:  1\n",
            "1148/2343.75 loss: 0.41751383979025875 \n",
            "Epoch:  1\n",
            "1149/2343.75 loss: 0.41749710133542184 \n",
            "Epoch:  1\n",
            "1150/2343.75 loss: 0.41741166695576976 \n",
            "Epoch:  1\n",
            "1151/2343.75 loss: 0.41731103642895406 \n",
            "Epoch:  1\n",
            "1152/2343.75 loss: 0.41733820180684095 \n",
            "Epoch:  1\n",
            "1153/2343.75 loss: 0.41727359181002066 \n",
            "Epoch:  1\n",
            "1154/2343.75 loss: 0.41728375970801235 \n",
            "Epoch:  1\n",
            "1155/2343.75 loss: 0.417185551631925 \n",
            "Epoch:  1\n",
            "1156/2343.75 loss: 0.4171162460549906 \n",
            "Epoch:  1\n",
            "1157/2343.75 loss: 0.417112420478736 \n",
            "Epoch:  1\n",
            "1158/2343.75 loss: 0.4171140037470588 \n",
            "Epoch:  1\n",
            "1159/2343.75 loss: 0.41701098309251766 \n",
            "Epoch:  1\n",
            "1160/2343.75 loss: 0.4169303078804419 \n",
            "Epoch:  1\n",
            "1161/2343.75 loss: 0.41682973530996276 \n",
            "Epoch:  1\n",
            "1162/2343.75 loss: 0.41678063889207867 \n",
            "Epoch:  1\n",
            "1163/2343.75 loss: 0.4166673642688805 \n",
            "Epoch:  1\n",
            "1164/2343.75 loss: 0.4166396840831241 \n",
            "Epoch:  1\n",
            "1165/2343.75 loss: 0.4165467343214855 \n",
            "Epoch:  1\n",
            "1166/2343.75 loss: 0.41655790707497076 \n",
            "Epoch:  1\n",
            "1167/2343.75 loss: 0.41640750669606336 \n",
            "Epoch:  1\n",
            "1168/2343.75 loss: 0.4163652485486684 \n",
            "Epoch:  1\n",
            "1169/2343.75 loss: 0.4162918558614886 \n",
            "Epoch:  1\n",
            "1170/2343.75 loss: 0.4162216241368873 \n",
            "Epoch:  1\n",
            "1171/2343.75 loss: 0.4162557249564563 \n",
            "Epoch:  1\n",
            "1172/2343.75 loss: 0.4162476227284494 \n",
            "Epoch:  1\n",
            "1173/2343.75 loss: 0.41618254167714763 \n",
            "Epoch:  1\n",
            "1174/2343.75 loss: 0.4160908938849226 \n",
            "Epoch:  1\n",
            "1175/2343.75 loss: 0.41602822523690813 \n",
            "Epoch:  1\n",
            "1176/2343.75 loss: 0.4159799161079888 \n",
            "Epoch:  1\n",
            "1177/2343.75 loss: 0.41602627590222757 \n",
            "Epoch:  1\n",
            "1178/2343.75 loss: 0.415979800775862 \n",
            "Epoch:  1\n",
            "1179/2343.75 loss: 0.4158759113716877 \n",
            "Epoch:  1\n",
            "1180/2343.75 loss: 0.4159040663653163 \n",
            "Epoch:  1\n",
            "1181/2343.75 loss: 0.41581770698279896 \n",
            "Epoch:  1\n",
            "1182/2343.75 loss: 0.41583235508944155 \n",
            "Epoch:  1\n",
            "1183/2343.75 loss: 0.41568889063657133 \n",
            "Epoch:  1\n",
            "1184/2343.75 loss: 0.4156084543039024 \n",
            "Epoch:  1\n",
            "1185/2343.75 loss: 0.4155689368109277 \n",
            "Epoch:  1\n",
            "1186/2343.75 loss: 0.4155398688852636 \n",
            "Epoch:  1\n",
            "1187/2343.75 loss: 0.4155132298166503 \n",
            "Epoch:  1\n",
            "1188/2343.75 loss: 0.4153467407739894 \n",
            "Epoch:  1\n",
            "1189/2343.75 loss: 0.41531476964469716 \n",
            "Epoch:  1\n",
            "1190/2343.75 loss: 0.4152980195835395 \n",
            "Epoch:  1\n",
            "1191/2343.75 loss: 0.4152965893931437 \n",
            "Epoch:  1\n",
            "1192/2343.75 loss: 0.41544614046537265 \n",
            "Epoch:  1\n",
            "1193/2343.75 loss: 0.41533654593742475 \n",
            "Epoch:  1\n",
            "1194/2343.75 loss: 0.41516992175429435 \n",
            "Epoch:  1\n",
            "1195/2343.75 loss: 0.41505428149448986 \n",
            "Epoch:  1\n",
            "1196/2343.75 loss: 0.41505317333945657 \n",
            "Epoch:  1\n",
            "1197/2343.75 loss: 0.4151052651998396 \n",
            "Epoch:  1\n",
            "1198/2343.75 loss: 0.41504527734059704 \n",
            "Epoch:  1\n",
            "1199/2343.75 loss: 0.41499493914345903 \n",
            "Epoch:  1\n",
            "1200/2343.75 loss: 0.4148666281733882 \n",
            "Epoch:  1\n",
            "1201/2343.75 loss: 0.4147889448977548 \n",
            "Epoch:  1\n",
            "1202/2343.75 loss: 0.41474644712180964 \n",
            "Epoch:  1\n",
            "1203/2343.75 loss: 0.41467884700262664 \n",
            "Epoch:  1\n",
            "1204/2343.75 loss: 0.41469613659925975 \n",
            "Epoch:  1\n",
            "1205/2343.75 loss: 0.4146043717317518 \n",
            "Epoch:  1\n",
            "1206/2343.75 loss: 0.4144464866894775 \n",
            "Epoch:  1\n",
            "1207/2343.75 loss: 0.4143853158079433 \n",
            "Epoch:  1\n",
            "1208/2343.75 loss: 0.4143442330501413 \n",
            "Epoch:  1\n",
            "1209/2343.75 loss: 0.4142749919255903 \n",
            "Epoch:  1\n",
            "1210/2343.75 loss: 0.4141303707532505 \n",
            "Epoch:  1\n",
            "1211/2343.75 loss: 0.4140694422813335 \n",
            "Epoch:  1\n",
            "1212/2343.75 loss: 0.41409984522139054 \n",
            "Epoch:  1\n",
            "1213/2343.75 loss: 0.41413310151761995 \n",
            "Epoch:  1\n",
            "1214/2343.75 loss: 0.4140449690843315 \n",
            "Epoch:  1\n",
            "1215/2343.75 loss: 0.41396812586064796 \n",
            "Epoch:  1\n",
            "1216/2343.75 loss: 0.41375362454326603 \n",
            "Epoch:  1\n",
            "1217/2343.75 loss: 0.41369196547467524 \n",
            "Epoch:  1\n",
            "1218/2343.75 loss: 0.4137203888241828 \n",
            "Epoch:  1\n",
            "1219/2343.75 loss: 0.41378447349931374 \n",
            "Epoch:  1\n",
            "1220/2343.75 loss: 0.4136477506839282 \n",
            "Epoch:  1\n",
            "1221/2343.75 loss: 0.41369153806747666 \n",
            "Epoch:  1\n",
            "1222/2343.75 loss: 0.41368874314127646 \n",
            "Epoch:  1\n",
            "1223/2343.75 loss: 0.41368831958305213 \n",
            "Epoch:  1\n",
            "1224/2343.75 loss: 0.41360252408348785 \n",
            "Epoch:  1\n",
            "1225/2343.75 loss: 0.4135100676102312 \n",
            "Epoch:  1\n",
            "1226/2343.75 loss: 0.4134790290028658 \n",
            "Epoch:  1\n",
            "1227/2343.75 loss: 0.41352018855924716 \n",
            "Epoch:  1\n",
            "1228/2343.75 loss: 0.4135059182895113 \n",
            "Epoch:  1\n",
            "1229/2343.75 loss: 0.4134936542651518 \n",
            "Epoch:  1\n",
            "1230/2343.75 loss: 0.4134204192896571 \n",
            "Epoch:  1\n",
            "1231/2343.75 loss: 0.4132821937635928 \n",
            "Epoch:  1\n",
            "1232/2343.75 loss: 0.4131993910861228 \n",
            "Epoch:  1\n",
            "1233/2343.75 loss: 0.4131140783429146 \n",
            "Epoch:  1\n",
            "1234/2343.75 loss: 0.4130692899347799 \n",
            "Epoch:  1\n",
            "1235/2343.75 loss: 0.41312851854464383 \n",
            "Epoch:  1\n",
            "1236/2343.75 loss: 0.41311849656130095 \n",
            "Epoch:  1\n",
            "1237/2343.75 loss: 0.4130900667225024 \n",
            "Epoch:  1\n",
            "1238/2343.75 loss: 0.4129913646887155 \n",
            "Epoch:  1\n",
            "1239/2343.75 loss: 0.4129009829052994 \n",
            "Epoch:  1\n",
            "1240/2343.75 loss: 0.41285890058631575 \n",
            "Epoch:  1\n",
            "1241/2343.75 loss: 0.41282135441347406 \n",
            "Epoch:  1\n",
            "1242/2343.75 loss: 0.41278866028123967 \n",
            "Epoch:  1\n",
            "1243/2343.75 loss: 0.41283159917430096 \n",
            "Epoch:  1\n",
            "1244/2343.75 loss: 0.4127572907621123 \n",
            "Epoch:  1\n",
            "1245/2343.75 loss: 0.4126785465099073 \n",
            "Epoch:  1\n",
            "1246/2343.75 loss: 0.41258788884355246 \n",
            "Epoch:  1\n",
            "1247/2343.75 loss: 0.41253052638748133 \n",
            "Epoch:  1\n",
            "1248/2343.75 loss: 0.4124663945457475 \n",
            "Epoch:  1\n",
            "1249/2343.75 loss: 0.41238810192346576 \n",
            "Epoch:  1\n",
            "1250/2343.75 loss: 0.41240332427499393 \n",
            "Epoch:  1\n",
            "1251/2343.75 loss: 0.4123296931076545 \n",
            "Epoch:  1\n",
            "1252/2343.75 loss: 0.4122425592597351 \n",
            "Epoch:  1\n",
            "1253/2343.75 loss: 0.4122063571875365 \n",
            "Epoch:  1\n",
            "1254/2343.75 loss: 0.4121041336263793 \n",
            "Epoch:  1\n",
            "1255/2343.75 loss: 0.4119358797717816 \n",
            "Epoch:  1\n",
            "1256/2343.75 loss: 0.4119510480284975 \n",
            "Epoch:  1\n",
            "1257/2343.75 loss: 0.41194319475104585 \n",
            "Epoch:  1\n",
            "1258/2343.75 loss: 0.4120143064082474 \n",
            "Epoch:  1\n",
            "1259/2343.75 loss: 0.4119027628666825 \n",
            "Epoch:  1\n",
            "1260/2343.75 loss: 0.41182856661142764 \n",
            "Epoch:  1\n",
            "1261/2343.75 loss: 0.4118386703866597 \n",
            "Epoch:  1\n",
            "1262/2343.75 loss: 0.41180954510176454 \n",
            "Epoch:  1\n",
            "1263/2343.75 loss: 0.4117183880880475 \n",
            "Epoch:  1\n",
            "1264/2343.75 loss: 0.4116606389698775 \n",
            "Epoch:  1\n",
            "1265/2343.75 loss: 0.41161795538419044 \n",
            "Epoch:  1\n",
            "1266/2343.75 loss: 0.41164624000640015 \n",
            "Epoch:  1\n",
            "1267/2343.75 loss: 0.41160132335234517 \n",
            "Epoch:  1\n",
            "1268/2343.75 loss: 0.4115107899650614 \n",
            "Epoch:  1\n",
            "1269/2343.75 loss: 0.41151796568801086 \n",
            "Epoch:  1\n",
            "1270/2343.75 loss: 0.41141742884081056 \n",
            "Epoch:  1\n",
            "1271/2343.75 loss: 0.4113743204576602 \n",
            "Epoch:  1\n",
            "1272/2343.75 loss: 0.41126921298882196 \n",
            "Epoch:  1\n",
            "1273/2343.75 loss: 0.4111618264527111 \n",
            "Epoch:  1\n",
            "1274/2343.75 loss: 0.4112100846977795 \n",
            "Epoch:  1\n",
            "1275/2343.75 loss: 0.411155027433716 \n",
            "Epoch:  1\n",
            "1276/2343.75 loss: 0.4111859620714337 \n",
            "Epoch:  1\n",
            "1277/2343.75 loss: 0.41115104446715217 \n",
            "Epoch:  1\n",
            "1278/2343.75 loss: 0.4110868833347444 \n",
            "Epoch:  1\n",
            "1279/2343.75 loss: 0.41102521141292525 \n",
            "Epoch:  1\n",
            "1280/2343.75 loss: 0.41103198304248917 \n",
            "Epoch:  1\n",
            "1281/2343.75 loss: 0.4109883088380238 \n",
            "Epoch:  1\n",
            "1282/2343.75 loss: 0.4109198331530914 \n",
            "Epoch:  1\n",
            "1283/2343.75 loss: 0.41090257564484145 \n",
            "Epoch:  1\n",
            "1284/2343.75 loss: 0.4108183441574935 \n",
            "Epoch:  1\n",
            "1285/2343.75 loss: 0.4107792923577466 \n",
            "Epoch:  1\n",
            "1286/2343.75 loss: 0.4106849289463765 \n",
            "Epoch:  1\n",
            "1287/2343.75 loss: 0.41068467880766957 \n",
            "Epoch:  1\n",
            "1288/2343.75 loss: 0.41058688163988527 \n",
            "Epoch:  1\n",
            "1289/2343.75 loss: 0.41062817875035973 \n",
            "Epoch:  1\n",
            "1290/2343.75 loss: 0.4106956297325405 \n",
            "Epoch:  1\n",
            "1291/2343.75 loss: 0.4106804511103283 \n",
            "Epoch:  1\n",
            "1292/2343.75 loss: 0.41064121732625125 \n",
            "Epoch:  1\n",
            "1293/2343.75 loss: 0.41052537234111397 \n",
            "Epoch:  1\n",
            "1294/2343.75 loss: 0.4105060016901797 \n",
            "Epoch:  1\n",
            "1295/2343.75 loss: 0.4104076482838503 \n",
            "Epoch:  1\n",
            "1296/2343.75 loss: 0.4104027226624713 \n",
            "Epoch:  1\n",
            "1297/2343.75 loss: 0.4103477741082203 \n",
            "Epoch:  1\n",
            "1298/2343.75 loss: 0.4101826950643868 \n",
            "Epoch:  1\n",
            "1299/2343.75 loss: 0.4102225046318311 \n",
            "Epoch:  1\n",
            "1300/2343.75 loss: 0.4102204711298866 \n",
            "Epoch:  1\n",
            "1301/2343.75 loss: 0.410183816862088 \n",
            "Epoch:  1\n",
            "1302/2343.75 loss: 0.41013109550188803 \n",
            "Epoch:  1\n",
            "1303/2343.75 loss: 0.4101844555326949 \n",
            "Epoch:  1\n",
            "1304/2343.75 loss: 0.4101700360176664 \n",
            "Epoch:  1\n",
            "1305/2343.75 loss: 0.4101802266415374 \n",
            "Epoch:  1\n",
            "1306/2343.75 loss: 0.41020682774478456 \n",
            "Epoch:  1\n",
            "1307/2343.75 loss: 0.41012984565501914 \n",
            "Epoch:  1\n",
            "1308/2343.75 loss: 0.41014195707711304 \n",
            "Epoch:  1\n",
            "1309/2343.75 loss: 0.4100632079116261 \n",
            "Epoch:  1\n",
            "1310/2343.75 loss: 0.40998865359389625 \n",
            "Epoch:  1\n",
            "1311/2343.75 loss: 0.4099812263781886 \n",
            "Epoch:  1\n",
            "1312/2343.75 loss: 0.4099129088058421 \n",
            "Epoch:  1\n",
            "1313/2343.75 loss: 0.4098726006698209 \n",
            "Epoch:  1\n",
            "1314/2343.75 loss: 0.40981384813785554 \n",
            "Epoch:  1\n",
            "1315/2343.75 loss: 0.4096980385799357 \n",
            "Epoch:  1\n",
            "1316/2343.75 loss: 0.4095564407997269 \n",
            "Epoch:  1\n",
            "1317/2343.75 loss: 0.40952786881256176 \n",
            "Epoch:  1\n",
            "1318/2343.75 loss: 0.40946731516996776 \n",
            "Epoch:  1\n",
            "1319/2343.75 loss: 0.40945714241401715 \n",
            "Epoch:  1\n",
            "1320/2343.75 loss: 0.40936568691864056 \n",
            "Epoch:  1\n",
            "1321/2343.75 loss: 0.40927630030576834 \n",
            "Epoch:  1\n",
            "1322/2343.75 loss: 0.4094033630414164 \n",
            "Epoch:  1\n",
            "1323/2343.75 loss: 0.40935982429504036 \n",
            "Epoch:  1\n",
            "1324/2343.75 loss: 0.4093057154596976 \n",
            "Epoch:  1\n",
            "1325/2343.75 loss: 0.4092187795051982 \n",
            "Epoch:  1\n",
            "1326/2343.75 loss: 0.4090695681209032 \n",
            "Epoch:  1\n",
            "1327/2343.75 loss: 0.4089206859065467 \n",
            "Epoch:  1\n",
            "1328/2343.75 loss: 0.4088192876789963 \n",
            "Epoch:  1\n",
            "1329/2343.75 loss: 0.4087480291612166 \n",
            "Epoch:  1\n",
            "1330/2343.75 loss: 0.40880299127254155 \n",
            "Epoch:  1\n",
            "1331/2343.75 loss: 0.4087690824905673 \n",
            "Epoch:  1\n",
            "1332/2343.75 loss: 0.40871022904357424 \n",
            "Epoch:  1\n",
            "1333/2343.75 loss: 0.40862948575924185 \n",
            "Epoch:  1\n",
            "1334/2343.75 loss: 0.4086412526918261 \n",
            "Epoch:  1\n",
            "1335/2343.75 loss: 0.40864372712944796 \n",
            "Epoch:  1\n",
            "1336/2343.75 loss: 0.4086359854768084 \n",
            "Epoch:  1\n",
            "1337/2343.75 loss: 0.4086033478490618 \n",
            "Epoch:  1\n",
            "1338/2343.75 loss: 0.4085958593144534 \n",
            "Epoch:  1\n",
            "1339/2343.75 loss: 0.4085727728124875 \n",
            "Epoch:  1\n",
            "1340/2343.75 loss: 0.4085343743019545 \n",
            "Epoch:  1\n",
            "1341/2343.75 loss: 0.4084751220689268 \n",
            "Epoch:  1\n",
            "1342/2343.75 loss: 0.4084790934845306 \n",
            "Epoch:  1\n",
            "1343/2343.75 loss: 0.40841957696136977 \n",
            "Epoch:  1\n",
            "1344/2343.75 loss: 0.4084023602833092 \n",
            "Epoch:  1\n",
            "1345/2343.75 loss: 0.40836914815838943 \n",
            "Epoch:  1\n",
            "1346/2343.75 loss: 0.4083209970569115 \n",
            "Epoch:  1\n",
            "1347/2343.75 loss: 0.4082871344968544 \n",
            "Epoch:  1\n",
            "1348/2343.75 loss: 0.4082370170670319 \n",
            "Epoch:  1\n",
            "1349/2343.75 loss: 0.40816794123914507 \n",
            "Epoch:  1\n",
            "1350/2343.75 loss: 0.4081622734941614 \n",
            "Epoch:  1\n",
            "1351/2343.75 loss: 0.40822874750258653 \n",
            "Epoch:  1\n",
            "1352/2343.75 loss: 0.40815779771614497 \n",
            "Epoch:  1\n",
            "1353/2343.75 loss: 0.4082092242191602 \n",
            "Epoch:  1\n",
            "1354/2343.75 loss: 0.4081732374935572 \n",
            "Epoch:  1\n",
            "1355/2343.75 loss: 0.40823866079690535 \n",
            "Epoch:  1\n",
            "1356/2343.75 loss: 0.40820040769584054 \n",
            "Epoch:  1\n",
            "1357/2343.75 loss: 0.40814938906454024 \n",
            "Epoch:  1\n",
            "1358/2343.75 loss: 0.4081211996218841 \n",
            "Epoch:  1\n",
            "1359/2343.75 loss: 0.40804002852562593 \n",
            "Epoch:  1\n",
            "1360/2343.75 loss: 0.40803503793096996 \n",
            "Epoch:  1\n",
            "1361/2343.75 loss: 0.4078688006000197 \n",
            "Epoch:  1\n",
            "1362/2343.75 loss: 0.407819233719951 \n",
            "Epoch:  1\n",
            "1363/2343.75 loss: 0.40777217588029646 \n",
            "Epoch:  1\n",
            "1364/2343.75 loss: 0.40774913866877993 \n",
            "Epoch:  1\n",
            "1365/2343.75 loss: 0.40766829529321735 \n",
            "Epoch:  1\n",
            "1366/2343.75 loss: 0.4076828009775283 \n",
            "Epoch:  1\n",
            "1367/2343.75 loss: 0.4076232828243434 \n",
            "Epoch:  1\n",
            "1368/2343.75 loss: 0.4074954639241328 \n",
            "Epoch:  1\n",
            "1369/2343.75 loss: 0.40744312284854206 \n",
            "Epoch:  1\n",
            "1370/2343.75 loss: 0.40747689995384845 \n",
            "Epoch:  1\n",
            "1371/2343.75 loss: 0.40742984925649256 \n",
            "Epoch:  1\n",
            "1372/2343.75 loss: 0.40737378448573497 \n",
            "Epoch:  1\n",
            "1373/2343.75 loss: 0.4072990157740432 \n",
            "Epoch:  1\n",
            "1374/2343.75 loss: 0.40730792146379297 \n",
            "Epoch:  1\n",
            "1375/2343.75 loss: 0.4072433654110619 \n",
            "Epoch:  1\n",
            "1376/2343.75 loss: 0.40716916395211966 \n",
            "Epoch:  1\n",
            "1377/2343.75 loss: 0.40707872698762076 \n",
            "Epoch:  1\n",
            "1378/2343.75 loss: 0.4069920227072047 \n",
            "Epoch:  1\n",
            "1379/2343.75 loss: 0.4069405248739581 \n",
            "Epoch:  1\n",
            "1380/2343.75 loss: 0.4068202974462233 \n",
            "Epoch:  1\n",
            "1381/2343.75 loss: 0.4067364734976233 \n",
            "Epoch:  1\n",
            "1382/2343.75 loss: 0.4067743904915566 \n",
            "Epoch:  1\n",
            "1383/2343.75 loss: 0.40672933080938856 \n",
            "Epoch:  1\n",
            "1384/2343.75 loss: 0.40659713931247216 \n",
            "Epoch:  1\n",
            "1385/2343.75 loss: 0.40664527726508837 \n",
            "Epoch:  1\n",
            "1386/2343.75 loss: 0.40659752523589665 \n",
            "Epoch:  1\n",
            "1387/2343.75 loss: 0.40673112803774886 \n",
            "Epoch:  1\n",
            "1388/2343.75 loss: 0.40673845842255374 \n",
            "Epoch:  1\n",
            "1389/2343.75 loss: 0.4066459974475044 \n",
            "Epoch:  1\n",
            "1390/2343.75 loss: 0.4066387208715606 \n",
            "Epoch:  1\n",
            "1391/2343.75 loss: 0.4065309465038537 \n",
            "Epoch:  1\n",
            "1392/2343.75 loss: 0.40649664584929707 \n",
            "Epoch:  1\n",
            "1393/2343.75 loss: 0.4065156167365044 \n",
            "Epoch:  1\n",
            "1394/2343.75 loss: 0.4064475287780112 \n",
            "Epoch:  1\n",
            "1395/2343.75 loss: 0.4064972899683396 \n",
            "Epoch:  1\n",
            "1396/2343.75 loss: 0.4064361656467319 \n",
            "Epoch:  1\n",
            "1397/2343.75 loss: 0.4063771891598197 \n",
            "Epoch:  1\n",
            "1398/2343.75 loss: 0.40637692518324575 \n",
            "Epoch:  1\n",
            "1399/2343.75 loss: 0.4063481827186687 \n",
            "Epoch:  1\n",
            "1400/2343.75 loss: 0.40627847952259005 \n",
            "Epoch:  1\n",
            "1401/2343.75 loss: 0.4062613694053063 \n",
            "Epoch:  1\n",
            "1402/2343.75 loss: 0.4063222507600349 \n",
            "Epoch:  1\n",
            "1403/2343.75 loss: 0.40627694888799276 \n",
            "Epoch:  1\n",
            "1404/2343.75 loss: 0.4062054068698578 \n",
            "Epoch:  1\n",
            "1405/2343.75 loss: 0.40619058617699366 \n",
            "Epoch:  1\n",
            "1406/2343.75 loss: 0.4060968225974094 \n",
            "Epoch:  1\n",
            "1407/2343.75 loss: 0.4061262333575128 \n",
            "Epoch:  1\n",
            "1408/2343.75 loss: 0.40612605528217244 \n",
            "Epoch:  1\n",
            "1409/2343.75 loss: 0.4060305820394915 \n",
            "Epoch:  1\n",
            "1410/2343.75 loss: 0.40597820287262776 \n",
            "Epoch:  1\n",
            "1411/2343.75 loss: 0.4059278110079151 \n",
            "Epoch:  1\n",
            "1412/2343.75 loss: 0.40583126559083715 \n",
            "Epoch:  1\n",
            "1413/2343.75 loss: 0.4058356439059829 \n",
            "Epoch:  1\n",
            "1414/2343.75 loss: 0.40574014013100007 \n",
            "Epoch:  1\n",
            "1415/2343.75 loss: 0.4056445792590999 \n",
            "Epoch:  1\n",
            "1416/2343.75 loss: 0.4055175018419913 \n",
            "Epoch:  1\n",
            "1417/2343.75 loss: 0.40556131395785866 \n",
            "Epoch:  1\n",
            "1418/2343.75 loss: 0.4055816019144253 \n",
            "Epoch:  1\n",
            "1419/2343.75 loss: 0.4055470056307148 \n",
            "Epoch:  1\n",
            "1420/2343.75 loss: 0.40554041224076326 \n",
            "Epoch:  1\n",
            "1421/2343.75 loss: 0.4055324442653213 \n",
            "Epoch:  1\n",
            "1422/2343.75 loss: 0.40552369612501443 \n",
            "Epoch:  1\n",
            "1423/2343.75 loss: 0.40542904283390957 \n",
            "Epoch:  1\n",
            "1424/2343.75 loss: 0.40543302826714095 \n",
            "Epoch:  1\n",
            "1425/2343.75 loss: 0.40539121774102327 \n",
            "Epoch:  1\n",
            "1426/2343.75 loss: 0.4054284989833832 \n",
            "Epoch:  1\n",
            "1427/2343.75 loss: 0.40535002861286745 \n",
            "Epoch:  1\n",
            "1428/2343.75 loss: 0.40532914089772815 \n",
            "Epoch:  1\n",
            "1429/2343.75 loss: 0.40535863319060184 \n",
            "Epoch:  1\n",
            "1430/2343.75 loss: 0.4052974023367457 \n",
            "Epoch:  1\n",
            "1431/2343.75 loss: 0.4053004838775989 \n",
            "Epoch:  1\n",
            "1432/2343.75 loss: 0.4053034375201383 \n",
            "Epoch:  1\n",
            "1433/2343.75 loss: 0.40523521383935934 \n",
            "Epoch:  1\n",
            "1434/2343.75 loss: 0.40521834631830145 \n",
            "Epoch:  1\n",
            "1435/2343.75 loss: 0.4052061928347957 \n",
            "Epoch:  1\n",
            "1436/2343.75 loss: 0.4051118206048725 \n",
            "Epoch:  1\n",
            "1437/2343.75 loss: 0.40513056858607227 \n",
            "Epoch:  1\n",
            "1438/2343.75 loss: 0.40507005044285666 \n",
            "Epoch:  1\n",
            "1439/2343.75 loss: 0.40507419926838745 \n",
            "Epoch:  1\n",
            "1440/2343.75 loss: 0.4049822820672387 \n",
            "Epoch:  1\n",
            "1441/2343.75 loss: 0.4048721964849513 \n",
            "Epoch:  1\n",
            "1442/2343.75 loss: 0.4048055940032088 \n",
            "Epoch:  1\n",
            "1443/2343.75 loss: 0.40466582662634903 \n",
            "Epoch:  1\n",
            "1444/2343.75 loss: 0.40461443593551544 \n",
            "Epoch:  1\n",
            "1445/2343.75 loss: 0.40450229191763604 \n",
            "Epoch:  1\n",
            "1446/2343.75 loss: 0.4045083950780224 \n",
            "Epoch:  1\n",
            "1447/2343.75 loss: 0.4044227530531462 \n",
            "Epoch:  1\n",
            "1448/2343.75 loss: 0.40435512068766905 \n",
            "Epoch:  1\n",
            "1449/2343.75 loss: 0.40431013968484153 \n",
            "Epoch:  1\n",
            "1450/2343.75 loss: 0.4042415224133813 \n",
            "Epoch:  1\n",
            "1451/2343.75 loss: 0.4042045652620063 \n",
            "Epoch:  1\n",
            "1452/2343.75 loss: 0.4041812586234671 \n",
            "Epoch:  1\n",
            "1453/2343.75 loss: 0.4040739456552915 \n",
            "Epoch:  1\n",
            "1454/2343.75 loss: 0.40409455170336456 \n",
            "Epoch:  1\n",
            "1455/2343.75 loss: 0.40403295994050553 \n",
            "Epoch:  1\n",
            "1456/2343.75 loss: 0.404050776196737 \n",
            "Epoch:  1\n",
            "1457/2343.75 loss: 0.4040574683860169 \n",
            "Epoch:  1\n",
            "1458/2343.75 loss: 0.40402100109089883 \n",
            "Epoch:  1\n",
            "1459/2343.75 loss: 0.40402450663586187 \n",
            "Epoch:  1\n",
            "1460/2343.75 loss: 0.4039354766785976 \n",
            "Epoch:  1\n",
            "1461/2343.75 loss: 0.40385218449803284 \n",
            "Epoch:  1\n",
            "1462/2343.75 loss: 0.4038363801479666 \n",
            "Epoch:  1\n",
            "1463/2343.75 loss: 0.40378107834872 \n",
            "Epoch:  1\n",
            "1464/2343.75 loss: 0.40378644830537735 \n",
            "Epoch:  1\n",
            "1465/2343.75 loss: 0.40369708525044856 \n",
            "Epoch:  1\n",
            "1466/2343.75 loss: 0.4036475484397189 \n",
            "Epoch:  1\n",
            "1467/2343.75 loss: 0.40367622462377883 \n",
            "Epoch:  1\n",
            "1468/2343.75 loss: 0.40364830194889406 \n",
            "Epoch:  1\n",
            "1469/2343.75 loss: 0.4035865469043758 \n",
            "Epoch:  1\n",
            "1470/2343.75 loss: 0.40348571553107104 \n",
            "Epoch:  1\n",
            "1471/2343.75 loss: 0.4034626982984659 \n",
            "Epoch:  1\n",
            "1472/2343.75 loss: 0.403407499184029 \n",
            "Epoch:  1\n",
            "1473/2343.75 loss: 0.40339598854248043 \n",
            "Epoch:  1\n",
            "1474/2343.75 loss: 0.4033404989565833 \n",
            "Epoch:  1\n",
            "1475/2343.75 loss: 0.40331088184179653 \n",
            "Epoch:  1\n",
            "1476/2343.75 loss: 0.4033256508687419 \n",
            "Epoch:  1\n",
            "1477/2343.75 loss: 0.40331734017203075 \n",
            "Epoch:  1\n",
            "1478/2343.75 loss: 0.4033151875282156 \n",
            "Epoch:  1\n",
            "1479/2343.75 loss: 0.4033565511574616 \n",
            "Epoch:  1\n",
            "1480/2343.75 loss: 0.4034274819330297 \n",
            "Epoch:  1\n",
            "1481/2343.75 loss: 0.4033774856613715 \n",
            "Epoch:  1\n",
            "1482/2343.75 loss: 0.4032996487995297 \n",
            "Epoch:  1\n",
            "1483/2343.75 loss: 0.4032573177406409 \n",
            "Epoch:  1\n",
            "1484/2343.75 loss: 0.4031832664301901 \n",
            "Epoch:  1\n",
            "1485/2343.75 loss: 0.4031663245018847 \n",
            "Epoch:  1\n",
            "1486/2343.75 loss: 0.40311379994252583 \n",
            "Epoch:  1\n",
            "1487/2343.75 loss: 0.4031554605531436 \n",
            "Epoch:  1\n",
            "1488/2343.75 loss: 0.40308886936561145 \n",
            "Epoch:  1\n",
            "1489/2343.75 loss: 0.4031646651109593 \n",
            "Epoch:  1\n",
            "1490/2343.75 loss: 0.4031198507463428 \n",
            "Epoch:  1\n",
            "1491/2343.75 loss: 0.4030488352353707 \n",
            "Epoch:  1\n",
            "1492/2343.75 loss: 0.40300595840190934 \n",
            "Epoch:  1\n",
            "1493/2343.75 loss: 0.40293015521692943 \n",
            "Epoch:  1\n",
            "1494/2343.75 loss: 0.4029093343477983 \n",
            "Epoch:  1\n",
            "1495/2343.75 loss: 0.402851667832882 \n",
            "Epoch:  1\n",
            "1496/2343.75 loss: 0.4028679021374735 \n",
            "Epoch:  1\n",
            "1497/2343.75 loss: 0.4027828738113271 \n",
            "Epoch:  1\n",
            "1498/2343.75 loss: 0.40287028971793576 \n",
            "Epoch:  1\n",
            "1499/2343.75 loss: 0.40279796536763507 \n",
            "Epoch:  1\n",
            "1500/2343.75 loss: 0.4026995092769371 \n",
            "Epoch:  1\n",
            "1501/2343.75 loss: 0.40267198258011383 \n",
            "Epoch:  1\n",
            "1502/2343.75 loss: 0.4025590921729228 \n",
            "Epoch:  1\n",
            "1503/2343.75 loss: 0.40249266403112954 \n",
            "Epoch:  1\n",
            "1504/2343.75 loss: 0.40245120280802843 \n",
            "Epoch:  1\n",
            "1505/2343.75 loss: 0.40236859036433903 \n",
            "Epoch:  1\n",
            "1506/2343.75 loss: 0.40227910355178426 \n",
            "Epoch:  1\n",
            "1507/2343.75 loss: 0.40234217675160666 \n",
            "Epoch:  1\n",
            "1508/2343.75 loss: 0.40220376008824055 \n",
            "Epoch:  1\n",
            "1509/2343.75 loss: 0.4022172451907436 \n",
            "Epoch:  1\n",
            "1510/2343.75 loss: 0.4021248576125903 \n",
            "Epoch:  1\n",
            "1511/2343.75 loss: 0.40204930050229587 \n",
            "Epoch:  1\n",
            "1512/2343.75 loss: 0.40205947593868924 \n",
            "Epoch:  1\n",
            "1513/2343.75 loss: 0.40204201161349623 \n",
            "Epoch:  1\n",
            "1514/2343.75 loss: 0.4020195082961136 \n",
            "Epoch:  1\n",
            "1515/2343.75 loss: 0.4019819822722658 \n",
            "Epoch:  1\n",
            "1516/2343.75 loss: 0.4019015139176332 \n",
            "Epoch:  1\n",
            "1517/2343.75 loss: 0.40183786552139267 \n",
            "Epoch:  1\n",
            "1518/2343.75 loss: 0.40184949662161157 \n",
            "Epoch:  1\n",
            "1519/2343.75 loss: 0.4018073039521512 \n",
            "Epoch:  1\n",
            "1520/2343.75 loss: 0.40172919543811475 \n",
            "Epoch:  1\n",
            "1521/2343.75 loss: 0.40164805256544994 \n",
            "Epoch:  1\n",
            "1522/2343.75 loss: 0.4016001122078454 \n",
            "Epoch:  1\n",
            "1523/2343.75 loss: 0.4016539663408841 \n",
            "Epoch:  1\n",
            "1524/2343.75 loss: 0.4015773877061781 \n",
            "Epoch:  1\n",
            "1525/2343.75 loss: 0.4015494369722288 \n",
            "Epoch:  1\n",
            "1526/2343.75 loss: 0.40147660161321586 \n",
            "Epoch:  1\n",
            "1527/2343.75 loss: 0.40144291296796336 \n",
            "Epoch:  1\n",
            "1528/2343.75 loss: 0.4013610111782175 \n",
            "Epoch:  1\n",
            "1529/2343.75 loss: 0.40130834882360655 \n",
            "Epoch:  1\n",
            "1530/2343.75 loss: 0.4012226218553558 \n",
            "Epoch:  1\n",
            "1531/2343.75 loss: 0.40110114776084377 \n",
            "Epoch:  1\n",
            "1532/2343.75 loss: 0.401051646449639 \n",
            "Epoch:  1\n",
            "1533/2343.75 loss: 0.4010430374601674 \n",
            "Epoch:  1\n",
            "1534/2343.75 loss: 0.4009614912520014 \n",
            "Epoch:  1\n",
            "1535/2343.75 loss: 0.40081528429679264 \n",
            "Epoch:  1\n",
            "1536/2343.75 loss: 0.40075282545041635 \n",
            "Epoch:  1\n",
            "1537/2343.75 loss: 0.4007378776590443 \n",
            "Epoch:  1\n",
            "1538/2343.75 loss: 0.40064344616622566 \n",
            "Epoch:  1\n",
            "1539/2343.75 loss: 0.4005915899845687 \n",
            "Epoch:  1\n",
            "1540/2343.75 loss: 0.40055650185447944 \n",
            "Epoch:  1\n",
            "1541/2343.75 loss: 0.4004977332807391 \n",
            "Epoch:  1\n",
            "1542/2343.75 loss: 0.4004262339755922 \n",
            "Epoch:  1\n",
            "1543/2343.75 loss: 0.4004121443324293 \n",
            "Epoch:  1\n",
            "1544/2343.75 loss: 0.4003375977470651 \n",
            "Epoch:  1\n",
            "1545/2343.75 loss: 0.4002888808641366 \n",
            "Epoch:  1\n",
            "1546/2343.75 loss: 0.40027173000262023 \n",
            "Epoch:  1\n",
            "1547/2343.75 loss: 0.40031198572935367 \n",
            "Epoch:  1\n",
            "1548/2343.75 loss: 0.4002049370513107 \n",
            "Epoch:  1\n",
            "1549/2343.75 loss: 0.40015861444896267 \n",
            "Epoch:  1\n",
            "1550/2343.75 loss: 0.40012596895393443 \n",
            "Epoch:  1\n",
            "1551/2343.75 loss: 0.40014890722507973 \n",
            "Epoch:  1\n",
            "1552/2343.75 loss: 0.4001688110064631 \n",
            "Epoch:  1\n",
            "1553/2343.75 loss: 0.4000762060975016 \n",
            "Epoch:  1\n",
            "1554/2343.75 loss: 0.4000868397797802 \n",
            "Epoch:  1\n",
            "1555/2343.75 loss: 0.4000234921153989 \n",
            "Epoch:  1\n",
            "1556/2343.75 loss: 0.3999506047829344 \n",
            "Epoch:  1\n",
            "1557/2343.75 loss: 0.39991332557104975 \n",
            "Epoch:  1\n",
            "1558/2343.75 loss: 0.39986800930490857 \n",
            "Epoch:  1\n",
            "1559/2343.75 loss: 0.399815218618665 \n",
            "Epoch:  1\n",
            "1560/2343.75 loss: 0.39976497618797724 \n",
            "Epoch:  1\n",
            "1561/2343.75 loss: 0.39970672694386655 \n",
            "Epoch:  1\n",
            "1562/2343.75 loss: 0.39966590719694367 \n",
            "Epoch:  1\n",
            "1563/2343.75 loss: 0.39968821851303205 \n",
            "Epoch:  1\n",
            "1564/2343.75 loss: 0.39958791773730573 \n",
            "Epoch:  1\n",
            "1565/2343.75 loss: 0.399554710059711 \n",
            "Epoch:  1\n",
            "1566/2343.75 loss: 0.3995261089906011 \n",
            "Epoch:  1\n",
            "1567/2343.75 loss: 0.39952520074855 \n",
            "Epoch:  1\n",
            "1568/2343.75 loss: 0.399413637109362 \n",
            "Epoch:  1\n",
            "1569/2343.75 loss: 0.3993164237232725 \n",
            "Epoch:  1\n",
            "1570/2343.75 loss: 0.399414215599709 \n",
            "Epoch:  1\n",
            "1571/2343.75 loss: 0.399281984048187 \n",
            "Epoch:  1\n",
            "1572/2343.75 loss: 0.39921382355674845 \n",
            "Epoch:  1\n",
            "1573/2343.75 loss: 0.3991597333054379 \n",
            "Epoch:  1\n",
            "1574/2343.75 loss: 0.3991541832401639 \n",
            "Epoch:  1\n",
            "1575/2343.75 loss: 0.39911904384566443 \n",
            "Epoch:  1\n",
            "1576/2343.75 loss: 0.399019524870829 \n",
            "Epoch:  1\n",
            "1577/2343.75 loss: 0.3990609578892154 \n",
            "Epoch:  1\n",
            "1578/2343.75 loss: 0.39905563416814716 \n",
            "Epoch:  1\n",
            "1579/2343.75 loss: 0.3991016772068754 \n",
            "Epoch:  1\n",
            "1580/2343.75 loss: 0.3990457892361357 \n",
            "Epoch:  1\n",
            "1581/2343.75 loss: 0.39903062735548817 \n",
            "Epoch:  1\n",
            "1582/2343.75 loss: 0.399009023080577 \n",
            "Epoch:  1\n",
            "1583/2343.75 loss: 0.398972890095202 \n",
            "Epoch:  1\n",
            "1584/2343.75 loss: 0.3989417032582527 \n",
            "Epoch:  1\n",
            "1585/2343.75 loss: 0.3989116156104985 \n",
            "Epoch:  1\n",
            "1586/2343.75 loss: 0.3988342634915705 \n",
            "Epoch:  1\n",
            "1587/2343.75 loss: 0.3988223898902498 \n",
            "Epoch:  1\n",
            "1588/2343.75 loss: 0.3987523027847517 \n",
            "Epoch:  1\n",
            "1589/2343.75 loss: 0.39875047835726407 \n",
            "Epoch:  1\n",
            "1590/2343.75 loss: 0.3987168931665097 \n",
            "Epoch:  1\n",
            "1591/2343.75 loss: 0.39873484691520733 \n",
            "Epoch:  1\n",
            "1592/2343.75 loss: 0.39872889052339866 \n",
            "Epoch:  1\n",
            "1593/2343.75 loss: 0.39878059703964214 \n",
            "Epoch:  1\n",
            "1594/2343.75 loss: 0.39882881948753585 \n",
            "Epoch:  1\n",
            "1595/2343.75 loss: 0.39883468805212424 \n",
            "Epoch:  1\n",
            "1596/2343.75 loss: 0.39883989977440987 \n",
            "Epoch:  1\n",
            "1597/2343.75 loss: 0.3987681514796835 \n",
            "Epoch:  1\n",
            "1598/2343.75 loss: 0.3987244432553267 \n",
            "Epoch:  1\n",
            "1599/2343.75 loss: 0.39867116305045786 \n",
            "Epoch:  1\n",
            "1600/2343.75 loss: 0.3986071980070279 \n",
            "Epoch:  1\n",
            "1601/2343.75 loss: 0.39853164295987986 \n",
            "Epoch:  1\n",
            "1602/2343.75 loss: 0.39860325753316983 \n",
            "Epoch:  1\n",
            "1603/2343.75 loss: 0.39856360117879297 \n",
            "Epoch:  1\n",
            "1604/2343.75 loss: 0.3985286113721931 \n",
            "Epoch:  1\n",
            "1605/2343.75 loss: 0.3984981522260983 \n",
            "Epoch:  1\n",
            "1606/2343.75 loss: 0.3984655390615855 \n",
            "Epoch:  1\n",
            "1607/2343.75 loss: 0.3983976882581242 \n",
            "Epoch:  1\n",
            "1608/2343.75 loss: 0.3983581464452874 \n",
            "Epoch:  1\n",
            "1609/2343.75 loss: 0.398279891352846 \n",
            "Epoch:  1\n",
            "1610/2343.75 loss: 0.39817709103457904 \n",
            "Epoch:  1\n",
            "1611/2343.75 loss: 0.39813787659292776 \n",
            "Epoch:  1\n",
            "1612/2343.75 loss: 0.39815593646205294 \n",
            "Epoch:  1\n",
            "1613/2343.75 loss: 0.3980572861071117 \n",
            "Epoch:  1\n",
            "1614/2343.75 loss: 0.3980273140288728 \n",
            "Epoch:  1\n",
            "1615/2343.75 loss: 0.39814992450030134 \n",
            "Epoch:  1\n",
            "1616/2343.75 loss: 0.39807565181286303 \n",
            "Epoch:  1\n",
            "1617/2343.75 loss: 0.39805149584456045 \n",
            "Epoch:  1\n",
            "1618/2343.75 loss: 0.3980182714304709 \n",
            "Epoch:  1\n",
            "1619/2343.75 loss: 0.3980099877641525 \n",
            "Epoch:  1\n",
            "1620/2343.75 loss: 0.39796505679987154 \n",
            "Epoch:  1\n",
            "1621/2343.75 loss: 0.39792156274603857 \n",
            "Epoch:  1\n",
            "1622/2343.75 loss: 0.39792181550538325 \n",
            "Epoch:  1\n",
            "1623/2343.75 loss: 0.39786271224248 \n",
            "Epoch:  1\n",
            "1624/2343.75 loss: 0.39781036415466897 \n",
            "Epoch:  1\n",
            "1625/2343.75 loss: 0.39773305394626046 \n",
            "Epoch:  1\n",
            "1626/2343.75 loss: 0.3976573742967335 \n",
            "Epoch:  1\n",
            "1627/2343.75 loss: 0.39767340393119127 \n",
            "Epoch:  1\n",
            "1628/2343.75 loss: 0.39763038627862785 \n",
            "Epoch:  1\n",
            "1629/2343.75 loss: 0.3976128249263471 \n",
            "Epoch:  1\n",
            "1630/2343.75 loss: 0.39754336056425704 \n",
            "Epoch:  1\n",
            "1631/2343.75 loss: 0.3975615278069003 \n",
            "Epoch:  1\n",
            "1632/2343.75 loss: 0.3974442916528184 \n",
            "Epoch:  1\n",
            "1633/2343.75 loss: 0.3973677625453312 \n",
            "Epoch:  1\n",
            "1634/2343.75 loss: 0.3973386367526623 \n",
            "Epoch:  1\n",
            "1635/2343.75 loss: 0.39733758235400346 \n",
            "Epoch:  1\n",
            "1636/2343.75 loss: 0.3973264611561122 \n",
            "Epoch:  1\n",
            "1637/2343.75 loss: 0.39735386456034266 \n",
            "Epoch:  1\n",
            "1638/2343.75 loss: 0.39735698576305173 \n",
            "Epoch:  1\n",
            "1639/2343.75 loss: 0.3973722522396867 \n",
            "Epoch:  1\n",
            "1640/2343.75 loss: 0.39723701360093466 \n",
            "Epoch:  1\n",
            "1641/2343.75 loss: 0.39716148483549923 \n",
            "Epoch:  1\n",
            "1642/2343.75 loss: 0.39706144631997303 \n",
            "Epoch:  1\n",
            "1643/2343.75 loss: 0.3970673518513676 \n",
            "Epoch:  1\n",
            "1644/2343.75 loss: 0.39704742966995415 \n",
            "Epoch:  1\n",
            "1645/2343.75 loss: 0.39699740810172673 \n",
            "Epoch:  1\n",
            "1646/2343.75 loss: 0.3969398582875548 \n",
            "Epoch:  1\n",
            "1647/2343.75 loss: 0.39686638944176506 \n",
            "Epoch:  1\n",
            "1648/2343.75 loss: 0.39687286989662846 \n",
            "Epoch:  1\n",
            "1649/2343.75 loss: 0.3969400085012118 \n",
            "Epoch:  1\n",
            "1650/2343.75 loss: 0.3969514893174966 \n",
            "Epoch:  1\n",
            "1651/2343.75 loss: 0.3970129871357584 \n",
            "Epoch:  1\n",
            "1652/2343.75 loss: 0.39697302854464983 \n",
            "Epoch:  1\n",
            "1653/2343.75 loss: 0.39690415754896263 \n",
            "Epoch:  1\n",
            "1654/2343.75 loss: 0.39696462908000024 \n",
            "Epoch:  1\n",
            "1655/2343.75 loss: 0.3969198390211604 \n",
            "Epoch:  1\n",
            "1656/2343.75 loss: 0.39685714040704717 \n",
            "Epoch:  1\n",
            "1657/2343.75 loss: 0.3968740310093031 \n",
            "Epoch:  1\n",
            "1658/2343.75 loss: 0.39687706407938755 \n",
            "Epoch:  1\n",
            "1659/2343.75 loss: 0.39683316352676196 \n",
            "Epoch:  1\n",
            "1660/2343.75 loss: 0.39676782627876517 \n",
            "Epoch:  1\n",
            "1661/2343.75 loss: 0.3967261423147255 \n",
            "Epoch:  1\n",
            "1662/2343.75 loss: 0.3967236657226509 \n",
            "Epoch:  1\n",
            "1663/2343.75 loss: 0.3967256129701407 \n",
            "Epoch:  1\n",
            "1664/2343.75 loss: 0.39667683872911663 \n",
            "Epoch:  1\n",
            "1665/2343.75 loss: 0.3966989727670691 \n",
            "Epoch:  1\n",
            "1666/2343.75 loss: 0.39665147518246346 \n",
            "Epoch:  1\n",
            "1667/2343.75 loss: 0.3966594992978253 \n",
            "Epoch:  1\n",
            "1668/2343.75 loss: 0.39667293335628623 \n",
            "Epoch:  1\n",
            "1669/2343.75 loss: 0.3966234247752292 \n",
            "Epoch:  1\n",
            "1670/2343.75 loss: 0.3966084979006375 \n",
            "Epoch:  1\n",
            "1671/2343.75 loss: 0.39652810148609596 \n",
            "Epoch:  1\n",
            "1672/2343.75 loss: 0.3964110332214398 \n",
            "Epoch:  1\n",
            "1673/2343.75 loss: 0.39638534831851185 \n",
            "Epoch:  1\n",
            "1674/2343.75 loss: 0.396358038106961 \n",
            "Epoch:  1\n",
            "1675/2343.75 loss: 0.3963687466924259 \n",
            "Epoch:  1\n",
            "1676/2343.75 loss: 0.3964909236293928 \n",
            "Epoch:  1\n",
            "1677/2343.75 loss: 0.39640643431653 \n",
            "Epoch:  1\n",
            "1678/2343.75 loss: 0.3963590111936253 \n",
            "Epoch:  1\n",
            "1679/2343.75 loss: 0.3963134300584594 \n",
            "Epoch:  1\n",
            "1680/2343.75 loss: 0.39622347448674927 \n",
            "Epoch:  1\n",
            "1681/2343.75 loss: 0.3962083073968553 \n",
            "Epoch:  1\n",
            "1682/2343.75 loss: 0.3961274538430442 \n",
            "Epoch:  1\n",
            "1683/2343.75 loss: 0.3961201857094102 \n",
            "Epoch:  1\n",
            "1684/2343.75 loss: 0.39601920561903664 \n",
            "Epoch:  1\n",
            "1685/2343.75 loss: 0.3959710049494871 \n",
            "Epoch:  1\n",
            "1686/2343.75 loss: 0.39591951372359163 \n",
            "Epoch:  1\n",
            "1687/2343.75 loss: 0.3958881211895231 \n",
            "Epoch:  1\n",
            "1688/2343.75 loss: 0.39587610898249387 \n",
            "Epoch:  1\n",
            "1689/2343.75 loss: 0.3957914146445912 \n",
            "Epoch:  1\n",
            "1690/2343.75 loss: 0.39579064076923887 \n",
            "Epoch:  1\n",
            "1691/2343.75 loss: 0.39573102891797435 \n",
            "Epoch:  1\n",
            "1692/2343.75 loss: 0.39574308277091824 \n",
            "Epoch:  1\n",
            "1693/2343.75 loss: 0.3957853313214382 \n",
            "Epoch:  1\n",
            "1694/2343.75 loss: 0.3957830067053657 \n",
            "Epoch:  1\n",
            "1695/2343.75 loss: 0.39567213504627907 \n",
            "Epoch:  1\n",
            "1696/2343.75 loss: 0.3956456096490552 \n",
            "Epoch:  1\n",
            "1697/2343.75 loss: 0.39561310004184047 \n",
            "Epoch:  1\n",
            "1698/2343.75 loss: 0.3955174924574437 \n",
            "Epoch:  1\n",
            "1699/2343.75 loss: 0.3955363351895529 \n",
            "Epoch:  1\n",
            "1700/2343.75 loss: 0.39554560425385243 \n",
            "Epoch:  1\n",
            "1701/2343.75 loss: 0.39554646986806996 \n",
            "Epoch:  1\n",
            "1702/2343.75 loss: 0.3954913724417275 \n",
            "Epoch:  1\n",
            "1703/2343.75 loss: 0.3953965545802469 \n",
            "Epoch:  1\n",
            "1704/2343.75 loss: 0.3954144228501054 \n",
            "Epoch:  1\n",
            "1705/2343.75 loss: 0.3955124100497711 \n",
            "Epoch:  1\n",
            "1706/2343.75 loss: 0.3955098410569651 \n",
            "Epoch:  1\n",
            "1707/2343.75 loss: 0.3955002323194326 \n",
            "Epoch:  1\n",
            "1708/2343.75 loss: 0.39546292623523527 \n",
            "Epoch:  1\n",
            "1709/2343.75 loss: 0.39542583097317063 \n",
            "Epoch:  1\n",
            "1710/2343.75 loss: 0.39545848268051026 \n",
            "Epoch:  1\n",
            "1711/2343.75 loss: 0.39545358330115815 \n",
            "Epoch:  1\n",
            "1712/2343.75 loss: 0.3954518445301529 \n",
            "Epoch:  1\n",
            "1713/2343.75 loss: 0.39549605813542493 \n",
            "Epoch:  1\n",
            "1714/2343.75 loss: 0.39549051383320166 \n",
            "Epoch:  1\n",
            "1715/2343.75 loss: 0.3955282595590417 \n",
            "Epoch:  1\n",
            "1716/2343.75 loss: 0.3954946455232751 \n",
            "Epoch:  1\n",
            "1717/2343.75 loss: 0.3955068814376319 \n",
            "Epoch:  1\n",
            "1718/2343.75 loss: 0.3954859135922753 \n",
            "Epoch:  1\n",
            "1719/2343.75 loss: 0.395482428813743 \n",
            "Epoch:  1\n",
            "1720/2343.75 loss: 0.39543084554343916 \n",
            "Epoch:  1\n",
            "1721/2343.75 loss: 0.39537344136546854 \n",
            "Epoch:  1\n",
            "1722/2343.75 loss: 0.39534645861861184 \n",
            "Epoch:  1\n",
            "1723/2343.75 loss: 0.3952860250083725 \n",
            "Epoch:  1\n",
            "1724/2343.75 loss: 0.39524847876334535 \n",
            "Epoch:  1\n",
            "1725/2343.75 loss: 0.3952783347125816 \n",
            "Epoch:  1\n",
            "1726/2343.75 loss: 0.3953063298096599 \n",
            "Epoch:  1\n",
            "1727/2343.75 loss: 0.3952690246256275 \n",
            "Epoch:  1\n",
            "1728/2343.75 loss: 0.3952049820098386 \n",
            "Epoch:  1\n",
            "1729/2343.75 loss: 0.39512999472073734 \n",
            "Epoch:  1\n",
            "1730/2343.75 loss: 0.39510745287871235 \n",
            "Epoch:  1\n",
            "1731/2343.75 loss: 0.39505203581881854 \n",
            "Epoch:  1\n",
            "1732/2343.75 loss: 0.3950356568773565 \n",
            "Epoch:  1\n",
            "1733/2343.75 loss: 0.39500522610947336 \n",
            "Epoch:  1\n",
            "1734/2343.75 loss: 0.3949763018133317 \n",
            "Epoch:  1\n",
            "1735/2343.75 loss: 0.39490570870446995 \n",
            "Epoch:  1\n",
            "1736/2343.75 loss: 0.3948992051797553 \n",
            "Epoch:  1\n",
            "1737/2343.75 loss: 0.3947956414745373 \n",
            "Epoch:  1\n",
            "1738/2343.75 loss: 0.3946977301086757 \n",
            "Epoch:  1\n",
            "1739/2343.75 loss: 0.39470584325749297 \n",
            "Epoch:  1\n",
            "1740/2343.75 loss: 0.39472830177449003 \n",
            "Epoch:  1\n",
            "1741/2343.75 loss: 0.39468791235738726 \n",
            "Epoch:  1\n",
            "1742/2343.75 loss: 0.3946455238676591 \n",
            "Epoch:  1\n",
            "1743/2343.75 loss: 0.39463653801641335 \n",
            "Epoch:  1\n",
            "1744/2343.75 loss: 0.3946463805727426 \n",
            "Epoch:  1\n",
            "1745/2343.75 loss: 0.394608621437525 \n",
            "Epoch:  1\n",
            "1746/2343.75 loss: 0.3946079141116511 \n",
            "Epoch:  1\n",
            "1747/2343.75 loss: 0.3945248783214414 \n",
            "Epoch:  1\n",
            "1748/2343.75 loss: 0.39446691009165424 \n",
            "Epoch:  1\n",
            "1749/2343.75 loss: 0.3945345298392432 \n",
            "Epoch:  1\n",
            "1750/2343.75 loss: 0.39458126370938146 \n",
            "Epoch:  1\n",
            "1751/2343.75 loss: 0.39454319450544983 \n",
            "Epoch:  1\n",
            "1752/2343.75 loss: 0.39453285869638105 \n",
            "Epoch:  1\n",
            "1753/2343.75 loss: 0.39447854024314555 \n",
            "Epoch:  1\n",
            "1754/2343.75 loss: 0.3944356949410887 \n",
            "Epoch:  1\n",
            "1755/2343.75 loss: 0.3944450449156055 \n",
            "Epoch:  1\n",
            "1756/2343.75 loss: 0.39444574123427617 \n",
            "Epoch:  1\n",
            "1757/2343.75 loss: 0.39438916659734896 \n",
            "Epoch:  1\n",
            "1758/2343.75 loss: 0.3943655289256349 \n",
            "Epoch:  1\n",
            "1759/2343.75 loss: 0.39441762442954564 \n",
            "Epoch:  1\n",
            "1760/2343.75 loss: 0.39434905554136723 \n",
            "Epoch:  1\n",
            "1761/2343.75 loss: 0.3943490550750771 \n",
            "Epoch:  1\n",
            "1762/2343.75 loss: 0.39431715082594837 \n",
            "Epoch:  1\n",
            "1763/2343.75 loss: 0.39429242657222985 \n",
            "Epoch:  1\n",
            "1764/2343.75 loss: 0.39426182768500223 \n",
            "Epoch:  1\n",
            "1765/2343.75 loss: 0.39422058777320423 \n",
            "Epoch:  1\n",
            "1766/2343.75 loss: 0.39411390510803573 \n",
            "Epoch:  1\n",
            "1767/2343.75 loss: 0.3941280393949731 \n",
            "Epoch:  1\n",
            "1768/2343.75 loss: 0.39406609060132214 \n",
            "Epoch:  1\n",
            "1769/2343.75 loss: 0.39401291160933716 \n",
            "Epoch:  1\n",
            "1770/2343.75 loss: 0.394096027384499 \n",
            "Epoch:  1\n",
            "1771/2343.75 loss: 0.39406432532631785 \n",
            "Epoch:  1\n",
            "1772/2343.75 loss: 0.39407875597712283 \n",
            "Epoch:  1\n",
            "1773/2343.75 loss: 0.3940761901581032 \n",
            "Epoch:  1\n",
            "1774/2343.75 loss: 0.3941173876842982 \n",
            "Epoch:  1\n",
            "1775/2343.75 loss: 0.3940974395755712 \n",
            "Epoch:  1\n",
            "1776/2343.75 loss: 0.3941370906726524 \n",
            "Epoch:  1\n",
            "1777/2343.75 loss: 0.3940955124401537 \n",
            "Epoch:  1\n",
            "1778/2343.75 loss: 0.39413532184174116 \n",
            "Epoch:  1\n",
            "1779/2343.75 loss: 0.39418681650684123 \n",
            "Epoch:  1\n",
            "1780/2343.75 loss: 0.39415663221389774 \n",
            "Epoch:  1\n",
            "1781/2343.75 loss: 0.3942090445663257 \n",
            "Epoch:  1\n",
            "1782/2343.75 loss: 0.3941815337035606 \n",
            "Epoch:  1\n",
            "1783/2343.75 loss: 0.39412376505109764 \n",
            "Epoch:  1\n",
            "1784/2343.75 loss: 0.3940885738999236 \n",
            "Epoch:  1\n",
            "1785/2343.75 loss: 0.39407927295128414 \n",
            "Epoch:  1\n",
            "1786/2343.75 loss: 0.39407657776455274 \n",
            "Epoch:  1\n",
            "1787/2343.75 loss: 0.3940868401854097 \n",
            "Epoch:  1\n",
            "1788/2343.75 loss: 0.39406947284320537 \n",
            "Epoch:  1\n",
            "1789/2343.75 loss: 0.3940100343367241 \n",
            "Epoch:  1\n",
            "1790/2343.75 loss: 0.39393563427983685 \n",
            "Epoch:  1\n",
            "1791/2343.75 loss: 0.39385264397632064 \n",
            "Epoch:  1\n",
            "1792/2343.75 loss: 0.39383748739675245 \n",
            "Epoch:  1\n",
            "1793/2343.75 loss: 0.39378545034589574 \n",
            "Epoch:  1\n",
            "1794/2343.75 loss: 0.39381698199799464 \n",
            "Epoch:  1\n",
            "1795/2343.75 loss: 0.3937757692335045 \n",
            "Epoch:  1\n",
            "1796/2343.75 loss: 0.39383187434808636 \n",
            "Epoch:  1\n",
            "1797/2343.75 loss: 0.3938177936815049 \n",
            "Epoch:  1\n",
            "1798/2343.75 loss: 0.39372755897714407 \n",
            "Epoch:  1\n",
            "1799/2343.75 loss: 0.39366069053610164 \n",
            "Epoch:  1\n",
            "1800/2343.75 loss: 0.3936102381850004 \n",
            "Epoch:  1\n",
            "1801/2343.75 loss: 0.3936081658656007 \n",
            "Epoch:  1\n",
            "1802/2343.75 loss: 0.39359956979751587 \n",
            "Epoch:  1\n",
            "1803/2343.75 loss: 0.39357105723505803 \n",
            "Epoch:  1\n",
            "1804/2343.75 loss: 0.39355657087468704 \n",
            "Epoch:  1\n",
            "1805/2343.75 loss: 0.3935211499862101 \n",
            "Epoch:  1\n",
            "1806/2343.75 loss: 0.3935084411071425 \n",
            "Epoch:  1\n",
            "1807/2343.75 loss: 0.39353080995512746 \n",
            "Epoch:  1\n",
            "1808/2343.75 loss: 0.39353784653546076 \n",
            "Epoch:  1\n",
            "1809/2343.75 loss: 0.39358503783934684 \n",
            "Epoch:  1\n",
            "1810/2343.75 loss: 0.3935480334691617 \n",
            "Epoch:  1\n",
            "1811/2343.75 loss: 0.39345790930174573 \n",
            "Epoch:  1\n",
            "1812/2343.75 loss: 0.3933890441620606 \n",
            "Epoch:  1\n",
            "1813/2343.75 loss: 0.3933961721262438 \n",
            "Epoch:  1\n",
            "1814/2343.75 loss: 0.39339466432401954 \n",
            "Epoch:  1\n",
            "1815/2343.75 loss: 0.39336651254297617 \n",
            "Epoch:  1\n",
            "1816/2343.75 loss: 0.3932835984079144 \n",
            "Epoch:  1\n",
            "1817/2343.75 loss: 0.3932454007031238 \n",
            "Epoch:  1\n",
            "1818/2343.75 loss: 0.393209888605445 \n",
            "Epoch:  1\n",
            "1819/2343.75 loss: 0.3931953916182885 \n",
            "Epoch:  1\n",
            "1820/2343.75 loss: 0.3930882006343546 \n",
            "Epoch:  1\n",
            "1821/2343.75 loss: 0.3931481364202552 \n",
            "Epoch:  1\n",
            "1822/2343.75 loss: 0.3931341744260479 \n",
            "Epoch:  1\n",
            "1823/2343.75 loss: 0.3931193221523835 \n",
            "Epoch:  1\n",
            "1824/2343.75 loss: 0.39310260900079386 \n",
            "Epoch:  1\n",
            "1825/2343.75 loss: 0.39308007941188583 \n",
            "Epoch:  1\n",
            "1826/2343.75 loss: 0.39305100030891216 \n",
            "Epoch:  1\n",
            "1827/2343.75 loss: 0.39302593311478323 \n",
            "Epoch:  1\n",
            "1828/2343.75 loss: 0.3930115156352292 \n",
            "Epoch:  1\n",
            "1829/2343.75 loss: 0.3929980141380446 \n",
            "Epoch:  1\n",
            "1830/2343.75 loss: 0.3929551481582115 \n",
            "Epoch:  1\n",
            "1831/2343.75 loss: 0.393018013769232 \n",
            "Epoch:  1\n",
            "1832/2343.75 loss: 0.3930761704858508 \n",
            "Epoch:  1\n",
            "1833/2343.75 loss: 0.39302106516896834 \n",
            "Epoch:  1\n",
            "1834/2343.75 loss: 0.39303100305617017 \n",
            "Epoch:  1\n",
            "1835/2343.75 loss: 0.39302103672671684 \n",
            "Epoch:  1\n",
            "1836/2343.75 loss: 0.39294274632348014 \n",
            "Epoch:  1\n",
            "1837/2343.75 loss: 0.39288436923828685 \n",
            "Epoch:  1\n",
            "1838/2343.75 loss: 0.3928183856997041 \n",
            "Epoch:  1\n",
            "1839/2343.75 loss: 0.3928133890518676 \n",
            "Epoch:  1\n",
            "1840/2343.75 loss: 0.3928514750090583 \n",
            "Epoch:  1\n",
            "1841/2343.75 loss: 0.3928986433630528 \n",
            "Epoch:  1\n",
            "1842/2343.75 loss: 0.392868106593532 \n",
            "Epoch:  1\n",
            "1843/2343.75 loss: 0.3928956544741891 \n",
            "Epoch:  1\n",
            "1844/2343.75 loss: 0.39288977092197597 \n",
            "Epoch:  1\n",
            "1845/2343.75 loss: 0.3929040908716611 \n",
            "Epoch:  1\n",
            "1846/2343.75 loss: 0.39286187191105043 \n",
            "Epoch:  1\n",
            "1847/2343.75 loss: 0.3928122590920884 \n",
            "Epoch:  1\n",
            "1848/2343.75 loss: 0.3927462893830821 \n",
            "Epoch:  1\n",
            "1849/2343.75 loss: 0.3927577804552542 \n",
            "Epoch:  1\n",
            "1850/2343.75 loss: 0.3927027715469682 \n",
            "Epoch:  1\n",
            "1851/2343.75 loss: 0.39264724255573674 \n",
            "Epoch:  1\n",
            "1852/2343.75 loss: 0.39255682937499325 \n",
            "Epoch:  1\n",
            "1853/2343.75 loss: 0.3925627386508893 \n",
            "Epoch:  1\n",
            "1854/2343.75 loss: 0.3924510242402393 \n",
            "Epoch:  1\n",
            "1855/2343.75 loss: 0.3923922888212034 \n",
            "Epoch:  1\n",
            "1856/2343.75 loss: 0.39234403426536774 \n",
            "Epoch:  1\n",
            "1857/2343.75 loss: 0.3923741551547184 \n",
            "Epoch:  1\n",
            "1858/2343.75 loss: 0.39231544281764336 \n",
            "Epoch:  1\n",
            "1859/2343.75 loss: 0.3922941312113757 \n",
            "Epoch:  1\n",
            "1860/2343.75 loss: 0.39230673370631647 \n",
            "Epoch:  1\n",
            "1861/2343.75 loss: 0.39228906299994915 \n",
            "Epoch:  1\n",
            "1862/2343.75 loss: 0.39231889060250036 \n",
            "Epoch:  1\n",
            "1863/2343.75 loss: 0.3923381189435976 \n",
            "Epoch:  1\n",
            "1864/2343.75 loss: 0.3923075268239822 \n",
            "Epoch:  1\n",
            "1865/2343.75 loss: 0.39231075211512956 \n",
            "Epoch:  1\n",
            "1866/2343.75 loss: 0.39226339891528983 \n",
            "Epoch:  1\n",
            "1867/2343.75 loss: 0.3921905464115822 \n",
            "Epoch:  1\n",
            "1868/2343.75 loss: 0.3921741183952829 \n",
            "Epoch:  1\n",
            "1869/2343.75 loss: 0.39215720070396515 \n",
            "Epoch:  1\n",
            "1870/2343.75 loss: 0.3921063806700362 \n",
            "Epoch:  1\n",
            "1871/2343.75 loss: 0.3920704899355769 \n",
            "Epoch:  1\n",
            "1872/2343.75 loss: 0.39207817082632945 \n",
            "Epoch:  1\n",
            "1873/2343.75 loss: 0.39209805949806786 \n",
            "Epoch:  1\n",
            "1874/2343.75 loss: 0.39208826549053194 \n",
            "Epoch:  1\n",
            "1875/2343.75 loss: 0.3920973220280112 \n",
            "Epoch:  1\n",
            "1876/2343.75 loss: 0.39206992521620077 \n",
            "Epoch:  1\n",
            "1877/2343.75 loss: 0.39205191504999726 \n",
            "Epoch:  1\n",
            "1878/2343.75 loss: 0.3919835369865459 \n",
            "Epoch:  1\n",
            "1879/2343.75 loss: 0.39195155299248846 \n",
            "Epoch:  1\n",
            "1880/2343.75 loss: 0.39190475389179175 \n",
            "Epoch:  1\n",
            "1881/2343.75 loss: 0.3918605574053355 \n",
            "Epoch:  1\n",
            "1882/2343.75 loss: 0.39183856360119707 \n",
            "Epoch:  1\n",
            "1883/2343.75 loss: 0.3918356671059334 \n",
            "Epoch:  1\n",
            "1884/2343.75 loss: 0.39176496620677825 \n",
            "Epoch:  1\n",
            "1885/2343.75 loss: 0.3917003088370993 \n",
            "Epoch:  1\n",
            "1886/2343.75 loss: 0.39162247141139117 \n",
            "Epoch:  1\n",
            "1887/2343.75 loss: 0.3916444153351299 \n",
            "Epoch:  1\n",
            "1888/2343.75 loss: 0.39164643971739527 \n",
            "Epoch:  1\n",
            "1889/2343.75 loss: 0.39163321475818674 \n",
            "Epoch:  1\n",
            "1890/2343.75 loss: 0.3915320600721333 \n",
            "Epoch:  1\n",
            "1891/2343.75 loss: 0.3914631064626808 \n",
            "Epoch:  1\n",
            "1892/2343.75 loss: 0.3914610653863782 \n",
            "Epoch:  1\n",
            "1893/2343.75 loss: 0.3914047992830543 \n",
            "Epoch:  1\n",
            "1894/2343.75 loss: 0.39139435377001447 \n",
            "Epoch:  1\n",
            "1895/2343.75 loss: 0.39136554798276363 \n",
            "Epoch:  1\n",
            "1896/2343.75 loss: 0.39130364788885674 \n",
            "Epoch:  1\n",
            "1897/2343.75 loss: 0.39125427634340443 \n",
            "Epoch:  1\n",
            "1898/2343.75 loss: 0.39123938194603086 \n",
            "Epoch:  1\n",
            "1899/2343.75 loss: 0.3911896336000217 \n",
            "Epoch:  1\n",
            "1900/2343.75 loss: 0.391087507061868 \n",
            "Epoch:  1\n",
            "1901/2343.75 loss: 0.3910584741131991 \n",
            "Epoch:  1\n",
            "1902/2343.75 loss: 0.3910008742113334 \n",
            "Epoch:  1\n",
            "1903/2343.75 loss: 0.39091719565678296 \n",
            "Epoch:  1\n",
            "1904/2343.75 loss: 0.39083361428553665 \n",
            "Epoch:  1\n",
            "1905/2343.75 loss: 0.39081878153312116 \n",
            "Epoch:  1\n",
            "1906/2343.75 loss: 0.39079250144470906 \n",
            "Epoch:  1\n",
            "1907/2343.75 loss: 0.3907746544392854 \n",
            "Epoch:  1\n",
            "1908/2343.75 loss: 0.39071079620270904 \n",
            "Epoch:  1\n",
            "1909/2343.75 loss: 0.3906748154556564 \n",
            "Epoch:  1\n",
            "1910/2343.75 loss: 0.3905788369199981 \n",
            "Epoch:  1\n",
            "1911/2343.75 loss: 0.3905266058713073 \n",
            "Epoch:  1\n",
            "1912/2343.75 loss: 0.39044223464850164 \n",
            "Epoch:  1\n",
            "1913/2343.75 loss: 0.39043541322681224 \n",
            "Epoch:  1\n",
            "1914/2343.75 loss: 0.3904558034385153 \n",
            "Epoch:  1\n",
            "1915/2343.75 loss: 0.39041431417731504 \n",
            "Epoch:  1\n",
            "1916/2343.75 loss: 0.3904532088888892 \n",
            "Epoch:  1\n",
            "1917/2343.75 loss: 0.3904043066706722 \n",
            "Epoch:  1\n",
            "1918/2343.75 loss: 0.3903546689884808 \n",
            "Epoch:  1\n",
            "1919/2343.75 loss: 0.39029130931012335 \n",
            "Epoch:  1\n",
            "1920/2343.75 loss: 0.39021824853361925 \n",
            "Epoch:  1\n",
            "1921/2343.75 loss: 0.39024934126112637 \n",
            "Epoch:  1\n",
            "1922/2343.75 loss: 0.3902511832711601 \n",
            "Epoch:  1\n",
            "1923/2343.75 loss: 0.39016681729105307 \n",
            "Epoch:  1\n",
            "1924/2343.75 loss: 0.39015885307417286 \n",
            "Epoch:  1\n",
            "1925/2343.75 loss: 0.3901451985551932 \n",
            "Epoch:  1\n",
            "1926/2343.75 loss: 0.39011385683482586 \n",
            "Epoch:  1\n",
            "1927/2343.75 loss: 0.3901371709090422 \n",
            "Epoch:  1\n",
            "1928/2343.75 loss: 0.3901348643549494 \n",
            "Epoch:  1\n",
            "1929/2343.75 loss: 0.3901059427332384 \n",
            "Epoch:  1\n",
            "1930/2343.75 loss: 0.3901139741839676 \n",
            "Epoch:  1\n",
            "1931/2343.75 loss: 0.3901266543036539 \n",
            "Epoch:  1\n",
            "1932/2343.75 loss: 0.3901289220045591 \n",
            "Epoch:  1\n",
            "1933/2343.75 loss: 0.3901910617207914 \n",
            "Epoch:  1\n",
            "1934/2343.75 loss: 0.3902203971332358 \n",
            "Epoch:  1\n",
            "1935/2343.75 loss: 0.39023521751134604 \n",
            "Epoch:  1\n",
            "1936/2343.75 loss: 0.39018857521406447 \n",
            "Epoch:  1\n",
            "1937/2343.75 loss: 0.3901653043979585 \n",
            "Epoch:  1\n",
            "1938/2343.75 loss: 0.39012752247416155 \n",
            "Epoch:  1\n",
            "1939/2343.75 loss: 0.3900924824178219 \n",
            "Epoch:  1\n",
            "1940/2343.75 loss: 0.3900354401175353 \n",
            "Epoch:  1\n",
            "1941/2343.75 loss: 0.3899990882874149 \n",
            "Epoch:  1\n",
            "1942/2343.75 loss: 0.3900171319924859 \n",
            "Epoch:  1\n",
            "1943/2343.75 loss: 0.3900266345590353 \n",
            "Epoch:  1\n",
            "1944/2343.75 loss: 0.3899786346752417 \n",
            "Epoch:  1\n",
            "1945/2343.75 loss: 0.3899534373541285 \n",
            "Epoch:  1\n",
            "1946/2343.75 loss: 0.38995155954611993 \n",
            "Epoch:  1\n",
            "1947/2343.75 loss: 0.3899094302903333 \n",
            "Epoch:  1\n",
            "1948/2343.75 loss: 0.3899199624147826 \n",
            "Epoch:  1\n",
            "1949/2343.75 loss: 0.3899035058800991 \n",
            "Epoch:  1\n",
            "1950/2343.75 loss: 0.38995578890424826 \n",
            "Epoch:  1\n",
            "1951/2343.75 loss: 0.38998203540648346 \n",
            "Epoch:  1\n",
            "1952/2343.75 loss: 0.3899370911132966 \n",
            "Epoch:  1\n",
            "1953/2343.75 loss: 0.38991382217059484 \n",
            "Epoch:  1\n",
            "1954/2343.75 loss: 0.3898522583648677 \n",
            "Epoch:  1\n",
            "1955/2343.75 loss: 0.389893573676455 \n",
            "Epoch:  1\n",
            "1956/2343.75 loss: 0.38990638335413874 \n",
            "Epoch:  1\n",
            "1957/2343.75 loss: 0.38991128811584186 \n",
            "Epoch:  1\n",
            "1958/2343.75 loss: 0.3898631991431566 \n",
            "Epoch:  1\n",
            "1959/2343.75 loss: 0.38986619842447795 \n",
            "Epoch:  1\n",
            "1960/2343.75 loss: 0.38985827734645445 \n",
            "Epoch:  1\n",
            "1961/2343.75 loss: 0.3897938530789603 \n",
            "Epoch:  1\n",
            "1962/2343.75 loss: 0.38977805524684916 \n",
            "Epoch:  1\n",
            "1963/2343.75 loss: 0.3897643142176864 \n",
            "Epoch:  1\n",
            "1964/2343.75 loss: 0.389790864383598 \n",
            "Epoch:  1\n",
            "1965/2343.75 loss: 0.38973910305440607 \n",
            "Epoch:  1\n",
            "1966/2343.75 loss: 0.3896818363505932 \n",
            "Epoch:  1\n",
            "1967/2343.75 loss: 0.38965529549442895 \n",
            "Epoch:  1\n",
            "1968/2343.75 loss: 0.3896994196245548 \n",
            "Epoch:  1\n",
            "1969/2343.75 loss: 0.38972076716913184 \n",
            "Epoch:  1\n",
            "1970/2343.75 loss: 0.389717730931193 \n",
            "Epoch:  1\n",
            "1971/2343.75 loss: 0.3896200682204708 \n",
            "Epoch:  1\n",
            "1972/2343.75 loss: 0.38953001274563487 \n",
            "Epoch:  1\n",
            "1973/2343.75 loss: 0.3894748582137634 \n",
            "Epoch:  1\n",
            "1974/2343.75 loss: 0.3894152236211149 \n",
            "Epoch:  1\n",
            "1975/2343.75 loss: 0.3893886725776471 \n",
            "Epoch:  1\n",
            "1976/2343.75 loss: 0.3893646745818639 \n",
            "Epoch:  1\n",
            "1977/2343.75 loss: 0.3893468259815861 \n",
            "Epoch:  1\n",
            "1978/2343.75 loss: 0.38928901698174895 \n",
            "Epoch:  1\n",
            "1979/2343.75 loss: 0.38924341646559313 \n",
            "Epoch:  1\n",
            "1980/2343.75 loss: 0.3892060084769607 \n",
            "Epoch:  1\n",
            "1981/2343.75 loss: 0.38916535034219385 \n",
            "Epoch:  1\n",
            "1982/2343.75 loss: 0.3891774276877315 \n",
            "Epoch:  1\n",
            "1983/2343.75 loss: 0.38911890083052697 \n",
            "Epoch:  1\n",
            "1984/2343.75 loss: 0.3891336467689471 \n",
            "Epoch:  1\n",
            "1985/2343.75 loss: 0.3890708773983089 \n",
            "Epoch:  1\n",
            "1986/2343.75 loss: 0.3890218691020139 \n",
            "Epoch:  1\n",
            "1987/2343.75 loss: 0.38906968757510185 \n",
            "Epoch:  1\n",
            "1988/2343.75 loss: 0.389041623760852 \n",
            "Epoch:  1\n",
            "1989/2343.75 loss: 0.38901561682098473 \n",
            "Epoch:  1\n",
            "1990/2343.75 loss: 0.38901898144388364 \n",
            "Epoch:  1\n",
            "1991/2343.75 loss: 0.3890018171021139 \n",
            "Epoch:  1\n",
            "1992/2343.75 loss: 0.3889556764524544 \n",
            "Epoch:  1\n",
            "1993/2343.75 loss: 0.38890562077971136 \n",
            "Epoch:  1\n",
            "1994/2343.75 loss: 0.38885027141051187 \n",
            "Epoch:  1\n",
            "1995/2343.75 loss: 0.388859454258709 \n",
            "Epoch:  1\n",
            "1996/2343.75 loss: 0.3888251990915598 \n",
            "Epoch:  1\n",
            "1997/2343.75 loss: 0.3887855100246879 \n",
            "Epoch:  1\n",
            "1998/2343.75 loss: 0.3887745755398017 \n",
            "Epoch:  1\n",
            "1999/2343.75 loss: 0.3887117181047797 \n",
            "Epoch:  1\n",
            "2000/2343.75 loss: 0.3887437427642642 \n",
            "Epoch:  1\n",
            "2001/2343.75 loss: 0.3887194350629777 \n",
            "Epoch:  1\n",
            "2002/2343.75 loss: 0.3886995027125388 \n",
            "Epoch:  1\n",
            "2003/2343.75 loss: 0.3887062833352598 \n",
            "Epoch:  1\n",
            "2004/2343.75 loss: 0.38868398543753824 \n",
            "Epoch:  1\n",
            "2005/2343.75 loss: 0.3886278527877029 \n",
            "Epoch:  1\n",
            "2006/2343.75 loss: 0.3885966030129997 \n",
            "Epoch:  1\n",
            "2007/2343.75 loss: 0.3885994179033307 \n",
            "Epoch:  1\n",
            "2008/2343.75 loss: 0.38855801964797093 \n",
            "Epoch:  1\n",
            "2009/2343.75 loss: 0.388556715156605 \n",
            "Epoch:  1\n",
            "2010/2343.75 loss: 0.38854300363006905 \n",
            "Epoch:  1\n",
            "2011/2343.75 loss: 0.3884461503142628 \n",
            "Epoch:  1\n",
            "2012/2343.75 loss: 0.38839103889240295 \n",
            "Epoch:  1\n",
            "2013/2343.75 loss: 0.38841306062298714 \n",
            "Epoch:  1\n",
            "2014/2343.75 loss: 0.38843789922984007 \n",
            "Epoch:  1\n",
            "2015/2343.75 loss: 0.3884127152314971 \n",
            "Epoch:  1\n",
            "2016/2343.75 loss: 0.38844195803203496 \n",
            "Epoch:  1\n",
            "2017/2343.75 loss: 0.38841909518681383 \n",
            "Epoch:  1\n",
            "2018/2343.75 loss: 0.38839996805753135 \n",
            "Epoch:  1\n",
            "2019/2343.75 loss: 0.3883589924857168 \n",
            "Epoch:  1\n",
            "2020/2343.75 loss: 0.38837012007117333 \n",
            "Epoch:  1\n",
            "2021/2343.75 loss: 0.38837348876850775 \n",
            "Epoch:  1\n",
            "2022/2343.75 loss: 0.38831828190616846 \n",
            "Epoch:  1\n",
            "2023/2343.75 loss: 0.38832493788993405 \n",
            "Epoch:  1\n",
            "2024/2343.75 loss: 0.3883218929502699 \n",
            "Epoch:  1\n",
            "2025/2343.75 loss: 0.3882834302649446 \n",
            "Epoch:  1\n",
            "2026/2343.75 loss: 0.38823314805776543 \n",
            "Epoch:  1\n",
            "2027/2343.75 loss: 0.388212163229078 \n",
            "Epoch:  1\n",
            "2028/2343.75 loss: 0.3881666016525797 \n",
            "Epoch:  1\n",
            "2029/2343.75 loss: 0.38813344348827605 \n",
            "Epoch:  1\n",
            "2030/2343.75 loss: 0.38807776059674726 \n",
            "Epoch:  1\n",
            "2031/2343.75 loss: 0.3880983055459233 \n",
            "Epoch:  1\n",
            "2032/2343.75 loss: 0.38803874319020715 \n",
            "Epoch:  1\n",
            "2033/2343.75 loss: 0.38801191045354133 \n",
            "Epoch:  1\n",
            "2034/2343.75 loss: 0.38798001068815846 \n",
            "Epoch:  1\n",
            "2035/2343.75 loss: 0.3879818879287463 \n",
            "Epoch:  1\n",
            "2036/2343.75 loss: 0.38799078575992446 \n",
            "Epoch:  1\n",
            "2037/2343.75 loss: 0.38796420975920026 \n",
            "Epoch:  1\n",
            "2038/2343.75 loss: 0.3879686088290736 \n",
            "Epoch:  1\n",
            "2039/2343.75 loss: 0.38791531727594486 \n",
            "Epoch:  1\n",
            "2040/2343.75 loss: 0.38785058486292256 \n",
            "Epoch:  1\n",
            "2041/2343.75 loss: 0.387878170618231 \n",
            "Epoch:  1\n",
            "2042/2343.75 loss: 0.3878146499847584 \n",
            "Epoch:  1\n",
            "2043/2343.75 loss: 0.3878067967913855 \n",
            "Epoch:  1\n",
            "2044/2343.75 loss: 0.38783924862341657 \n",
            "Epoch:  1\n",
            "2045/2343.75 loss: 0.3878131405116293 \n",
            "Epoch:  1\n",
            "2046/2343.75 loss: 0.38787652319911403 \n",
            "Epoch:  1\n",
            "2047/2343.75 loss: 0.3878554028342478 \n",
            "Epoch:  1\n",
            "2048/2343.75 loss: 0.38779544939408367 \n",
            "Epoch:  1\n",
            "2049/2343.75 loss: 0.38776276290416717 \n",
            "Epoch:  1\n",
            "2050/2343.75 loss: 0.38773894792414476 \n",
            "Epoch:  1\n",
            "2051/2343.75 loss: 0.3876613686278782 \n",
            "Epoch:  1\n",
            "2052/2343.75 loss: 0.3876452902236684 \n",
            "Epoch:  1\n",
            "2053/2343.75 loss: 0.3876422735796235 \n",
            "Epoch:  1\n",
            "2054/2343.75 loss: 0.38760881959086785 \n",
            "Epoch:  1\n",
            "2055/2343.75 loss: 0.3875824633681357 \n",
            "Epoch:  1\n",
            "2056/2343.75 loss: 0.38750519506057374 \n",
            "Epoch:  1\n",
            "2057/2343.75 loss: 0.3874992498633813 \n",
            "Epoch:  1\n",
            "2058/2343.75 loss: 0.3874475996707932 \n",
            "Epoch:  1\n",
            "2059/2343.75 loss: 0.38744055885568407 \n",
            "Epoch:  1\n",
            "2060/2343.75 loss: 0.3874326752682209 \n",
            "Epoch:  1\n",
            "2061/2343.75 loss: 0.3874462019908671 \n",
            "Epoch:  1\n",
            "2062/2343.75 loss: 0.3874314707702534 \n",
            "Epoch:  1\n",
            "2063/2343.75 loss: 0.38738842459409967 \n",
            "Epoch:  1\n",
            "2064/2343.75 loss: 0.38732006106047595 \n",
            "Epoch:  1\n",
            "2065/2343.75 loss: 0.387320076601376 \n",
            "Epoch:  1\n",
            "2066/2343.75 loss: 0.3873104897670603 \n",
            "Epoch:  1\n",
            "2067/2343.75 loss: 0.38728819655639984 \n",
            "Epoch:  1\n",
            "2068/2343.75 loss: 0.3872678556653715 \n",
            "Epoch:  1\n",
            "2069/2343.75 loss: 0.3872660719902043 \n",
            "Epoch:  1\n",
            "2070/2343.75 loss: 0.3872344923776339 \n",
            "Epoch:  1\n",
            "2071/2343.75 loss: 0.3872451898372196 \n",
            "Epoch:  1\n",
            "2072/2343.75 loss: 0.3872060762171117 \n",
            "Epoch:  1\n",
            "2073/2343.75 loss: 0.38716155245339745 \n",
            "Epoch:  1\n",
            "2074/2343.75 loss: 0.38710886431745734 \n",
            "Epoch:  1\n",
            "2075/2343.75 loss: 0.3870910356370355 \n",
            "Epoch:  1\n",
            "2076/2343.75 loss: 0.38702725931521387 \n",
            "Epoch:  1\n",
            "2077/2343.75 loss: 0.38710122865277835 \n",
            "Epoch:  1\n",
            "2078/2343.75 loss: 0.3870706077648964 \n",
            "Epoch:  1\n",
            "2079/2343.75 loss: 0.3869967121057786 \n",
            "Epoch:  1\n",
            "2080/2343.75 loss: 0.38696116569923245 \n",
            "Epoch:  1\n",
            "2081/2343.75 loss: 0.3869538108650935 \n",
            "Epoch:  1\n",
            "2082/2343.75 loss: 0.38690223600277235 \n",
            "Epoch:  1\n",
            "2083/2343.75 loss: 0.38683472526565393 \n",
            "Epoch:  1\n",
            "2084/2343.75 loss: 0.3867826551985112 \n",
            "Epoch:  1\n",
            "2085/2343.75 loss: 0.3867404274035277 \n",
            "Epoch:  1\n",
            "2086/2343.75 loss: 0.3866720503819977 \n",
            "Epoch:  1\n",
            "2087/2343.75 loss: 0.3866623253615081 \n",
            "Epoch:  1\n",
            "2088/2343.75 loss: 0.38664508522869356 \n",
            "Epoch:  1\n",
            "2089/2343.75 loss: 0.38663663062039744 \n",
            "Epoch:  1\n",
            "2090/2343.75 loss: 0.38665556316880845 \n",
            "Epoch:  1\n",
            "2091/2343.75 loss: 0.3866626294418799 \n",
            "Epoch:  1\n",
            "2092/2343.75 loss: 0.3866940673335682 \n",
            "Epoch:  1\n",
            "2093/2343.75 loss: 0.38663140095151255 \n",
            "Epoch:  1\n",
            "2094/2343.75 loss: 0.3865805716176022 \n",
            "Epoch:  1\n",
            "2095/2343.75 loss: 0.38656898211858426 \n",
            "Epoch:  1\n",
            "2096/2343.75 loss: 0.3866277345814589 \n",
            "Epoch:  1\n",
            "2097/2343.75 loss: 0.3865777957328623 \n",
            "Epoch:  1\n",
            "2098/2343.75 loss: 0.3865678868886117 \n",
            "Epoch:  1\n",
            "2099/2343.75 loss: 0.3865389200264499 \n",
            "Epoch:  1\n",
            "2100/2343.75 loss: 0.3865200903928036 \n",
            "Epoch:  1\n",
            "2101/2343.75 loss: 0.38648779476885564 \n",
            "Epoch:  1\n",
            "2102/2343.75 loss: 0.38645439243889174 \n",
            "Epoch:  1\n",
            "2103/2343.75 loss: 0.3864544755053271 \n",
            "Epoch:  1\n",
            "2104/2343.75 loss: 0.3864498884193688 \n",
            "Epoch:  1\n",
            "2105/2343.75 loss: 0.38642652159431157 \n",
            "Epoch:  1\n",
            "2106/2343.75 loss: 0.38637942236268696 \n",
            "Epoch:  1\n",
            "2107/2343.75 loss: 0.3863404841574026 \n",
            "Epoch:  1\n",
            "2108/2343.75 loss: 0.3863217545140688 \n",
            "Epoch:  1\n",
            "2109/2343.75 loss: 0.38628183262184335 \n",
            "Epoch:  1\n",
            "2110/2343.75 loss: 0.3862200941888713 \n",
            "Epoch:  1\n",
            "2111/2343.75 loss: 0.38623543304504093 \n",
            "Epoch:  1\n",
            "2112/2343.75 loss: 0.3862128188428978 \n",
            "Epoch:  1\n",
            "2113/2343.75 loss: 0.38618703821608785 \n",
            "Epoch:  1\n",
            "2114/2343.75 loss: 0.3861343554500711 \n",
            "Epoch:  1\n",
            "2115/2343.75 loss: 0.3860587286278746 \n",
            "Epoch:  1\n",
            "2116/2343.75 loss: 0.3861173557035755 \n",
            "Epoch:  1\n",
            "2117/2343.75 loss: 0.38615594171093137 \n",
            "Epoch:  1\n",
            "2118/2343.75 loss: 0.38611226477899774 \n",
            "Epoch:  1\n",
            "2119/2343.75 loss: 0.3860642707432216 \n",
            "Epoch:  1\n",
            "2120/2343.75 loss: 0.3860013892014372 \n",
            "Epoch:  1\n",
            "2121/2343.75 loss: 0.3859589396362368 \n",
            "Epoch:  1\n",
            "2122/2343.75 loss: 0.3859347211854165 \n",
            "Epoch:  1\n",
            "2123/2343.75 loss: 0.3858650543721719 \n",
            "Epoch:  1\n",
            "2124/2343.75 loss: 0.38584064403001 \n",
            "Epoch:  1\n",
            "2125/2343.75 loss: 0.3857861926546101 \n",
            "Epoch:  1\n",
            "2126/2343.75 loss: 0.3857420226484607 \n",
            "Epoch:  1\n",
            "2127/2343.75 loss: 0.3857953358035451 \n",
            "Epoch:  1\n",
            "2128/2343.75 loss: 0.38577439063112745 \n",
            "Epoch:  1\n",
            "2129/2343.75 loss: 0.3857816875610553 \n",
            "Epoch:  1\n",
            "2130/2343.75 loss: 0.38577063890730817 \n",
            "Epoch:  1\n",
            "2131/2343.75 loss: 0.3857097525296359 \n",
            "Epoch:  1\n",
            "2132/2343.75 loss: 0.38572466625638596 \n",
            "Epoch:  1\n",
            "2133/2343.75 loss: 0.38570000275257266 \n",
            "Epoch:  1\n",
            "2134/2343.75 loss: 0.385694819584105 \n",
            "Epoch:  1\n",
            "2135/2343.75 loss: 0.3857237058116144 \n",
            "Epoch:  1\n",
            "2136/2343.75 loss: 0.3856629956100497 \n",
            "Epoch:  1\n",
            "2137/2343.75 loss: 0.3856391454894371 \n",
            "Epoch:  1\n",
            "2138/2343.75 loss: 0.3855868425044349 \n",
            "Epoch:  1\n",
            "2139/2343.75 loss: 0.3855556273502167 \n",
            "Epoch:  1\n",
            "2140/2343.75 loss: 0.38555837683257854 \n",
            "Epoch:  1\n",
            "2141/2343.75 loss: 0.38554300255829993 \n",
            "Epoch:  1\n",
            "2142/2343.75 loss: 0.38554254551802736 \n",
            "Epoch:  1\n",
            "2143/2343.75 loss: 0.3855231480408849 \n",
            "Epoch:  1\n",
            "2144/2343.75 loss: 0.3854667859824943 \n",
            "Epoch:  1\n",
            "2145/2343.75 loss: 0.3854500536163722 \n",
            "Epoch:  1\n",
            "2146/2343.75 loss: 0.3854296671952766 \n",
            "Epoch:  1\n",
            "2147/2343.75 loss: 0.3853955573334707 \n",
            "Epoch:  1\n",
            "2148/2343.75 loss: 0.38537137256133497 \n",
            "Epoch:  1\n",
            "2149/2343.75 loss: 0.38537797623595527 \n",
            "Epoch:  1\n",
            "2150/2343.75 loss: 0.3853833214910238 \n",
            "Epoch:  1\n",
            "2151/2343.75 loss: 0.38533520893121076 \n",
            "Epoch:  1\n",
            "2152/2343.75 loss: 0.38526575282541364 \n",
            "Epoch:  1\n",
            "2153/2343.75 loss: 0.38526651352477936 \n",
            "Epoch:  1\n",
            "2154/2343.75 loss: 0.38521964296249117 \n",
            "Epoch:  1\n",
            "2155/2343.75 loss: 0.3852823415429008 \n",
            "Epoch:  1\n",
            "2156/2343.75 loss: 0.385305037080385 \n",
            "Epoch:  1\n",
            "2157/2343.75 loss: 0.3852577138971796 \n",
            "Epoch:  1\n",
            "2158/2343.75 loss: 0.38527271074931785 \n",
            "Epoch:  1\n",
            "2159/2343.75 loss: 0.38524525134513776 \n",
            "Epoch:  1\n",
            "2160/2343.75 loss: 0.3852275194240797 \n",
            "Epoch:  1\n",
            "2161/2343.75 loss: 0.3852572621472669 \n",
            "Epoch:  1\n",
            "2162/2343.75 loss: 0.38527064849340525 \n",
            "Epoch:  1\n",
            "2163/2343.75 loss: 0.3852439054155416 \n",
            "Epoch:  1\n",
            "2164/2343.75 loss: 0.38522106543286566 \n",
            "Epoch:  1\n",
            "2165/2343.75 loss: 0.38524884888807714 \n",
            "Epoch:  1\n",
            "2166/2343.75 loss: 0.3852019328770134 \n",
            "Epoch:  1\n",
            "2167/2343.75 loss: 0.38516513022466997 \n",
            "Epoch:  1\n",
            "2168/2343.75 loss: 0.38516368660277184 \n",
            "Epoch:  1\n",
            "2169/2343.75 loss: 0.38514034670481484 \n",
            "Epoch:  1\n",
            "2170/2343.75 loss: 0.38511025615273287 \n",
            "Epoch:  1\n",
            "2171/2343.75 loss: 0.3850560247500189 \n",
            "Epoch:  1\n",
            "2172/2343.75 loss: 0.38504438220192855 \n",
            "Epoch:  1\n",
            "2173/2343.75 loss: 0.3850363965704009 \n",
            "Epoch:  1\n",
            "2174/2343.75 loss: 0.38501556812346666 \n",
            "Epoch:  1\n",
            "2175/2343.75 loss: 0.38498263261691834 \n",
            "Epoch:  1\n",
            "2176/2343.75 loss: 0.3849395248571776 \n",
            "Epoch:  1\n",
            "2177/2343.75 loss: 0.3849358321590857 \n",
            "Epoch:  1\n",
            "2178/2343.75 loss: 0.38489094437426963 \n",
            "Epoch:  1\n",
            "2179/2343.75 loss: 0.3848607979304747 \n",
            "Epoch:  1\n",
            "2180/2343.75 loss: 0.38486284966563916 \n",
            "Epoch:  1\n",
            "2181/2343.75 loss: 0.3848831276968463 \n",
            "Epoch:  1\n",
            "2182/2343.75 loss: 0.3848412693079273 \n",
            "Epoch:  1\n",
            "2183/2343.75 loss: 0.38481741715836654 \n",
            "Epoch:  1\n",
            "2184/2343.75 loss: 0.38480914703632113 \n",
            "Epoch:  1\n",
            "2185/2343.75 loss: 0.3847916603620076 \n",
            "Epoch:  1\n",
            "2186/2343.75 loss: 0.3847604444583475 \n",
            "Epoch:  1\n",
            "2187/2343.75 loss: 0.38478351658860555 \n",
            "Epoch:  1\n",
            "2188/2343.75 loss: 0.38473666308540466 \n",
            "Epoch:  1\n",
            "2189/2343.75 loss: 0.3847407273710046 \n",
            "Epoch:  1\n",
            "2190/2343.75 loss: 0.3847594311021333 \n",
            "Epoch:  1\n",
            "2191/2343.75 loss: 0.3847115121741038 \n",
            "Epoch:  1\n",
            "2192/2343.75 loss: 0.3846893528866213 \n",
            "Epoch:  1\n",
            "2193/2343.75 loss: 0.384717664498846 \n",
            "Epoch:  1\n",
            "2194/2343.75 loss: 0.384696426322357 \n",
            "Epoch:  1\n",
            "2195/2343.75 loss: 0.3847113425193697 \n",
            "Epoch:  1\n",
            "2196/2343.75 loss: 0.3846860020050527 \n",
            "Epoch:  1\n",
            "2197/2343.75 loss: 0.38464158253440867 \n",
            "Epoch:  1\n",
            "2198/2343.75 loss: 0.3846271036903443 \n",
            "Epoch:  1\n",
            "2199/2343.75 loss: 0.38464527183635666 \n",
            "Epoch:  1\n",
            "2200/2343.75 loss: 0.38463624551560543 \n",
            "Epoch:  1\n",
            "2201/2343.75 loss: 0.38459691640114807 \n",
            "Epoch:  1\n",
            "2202/2343.75 loss: 0.38463349120891804 \n",
            "Epoch:  1\n",
            "2203/2343.75 loss: 0.38458666108724426 \n",
            "Epoch:  1\n",
            "2204/2343.75 loss: 0.3845764925082525 \n",
            "Epoch:  1\n",
            "2205/2343.75 loss: 0.3845271002416814 \n",
            "Epoch:  1\n",
            "2206/2343.75 loss: 0.38449101864373353 \n",
            "Epoch:  1\n",
            "2207/2343.75 loss: 0.3845089723329073 \n",
            "Epoch:  1\n",
            "2208/2343.75 loss: 0.38444280645301193 \n",
            "Epoch:  1\n",
            "2209/2343.75 loss: 0.3844899373019443 \n",
            "Epoch:  1\n",
            "2210/2343.75 loss: 0.3844902394166741 \n",
            "Epoch:  1\n",
            "2211/2343.75 loss: 0.384483268467632 \n",
            "Epoch:  1\n",
            "2212/2343.75 loss: 0.3844284290743262 \n",
            "Epoch:  1\n",
            "2213/2343.75 loss: 0.38446677640390053 \n",
            "Epoch:  1\n",
            "2214/2343.75 loss: 0.384423702746697 \n",
            "Epoch:  1\n",
            "2215/2343.75 loss: 0.38441808335483074 \n",
            "Epoch:  1\n",
            "2216/2343.75 loss: 0.38436590078638433 \n",
            "Epoch:  1\n",
            "2217/2343.75 loss: 0.3844084973145542 \n",
            "Epoch:  1\n",
            "2218/2343.75 loss: 0.384388360939836 \n",
            "Epoch:  1\n",
            "2219/2343.75 loss: 0.38444190130711675 \n",
            "Epoch:  1\n",
            "2220/2343.75 loss: 0.3844841584959099 \n",
            "Epoch:  1\n",
            "2221/2343.75 loss: 0.384496898391191 \n",
            "Epoch:  1\n",
            "2222/2343.75 loss: 0.38448741009816684 \n",
            "Epoch:  1\n",
            "2223/2343.75 loss: 0.38445111177191676 \n",
            "Epoch:  1\n",
            "2224/2343.75 loss: 0.38441675592674296 \n",
            "Epoch:  1\n",
            "2225/2343.75 loss: 0.38440706819188775 \n",
            "Epoch:  1\n",
            "2226/2343.75 loss: 0.3843865521121678 \n",
            "Epoch:  1\n",
            "2227/2343.75 loss: 0.38430642832197665 \n",
            "Epoch:  1\n",
            "2228/2343.75 loss: 0.38434950435118803 \n",
            "Epoch:  1\n",
            "2229/2343.75 loss: 0.3843559895328877 \n",
            "Epoch:  1\n",
            "2230/2343.75 loss: 0.3843168517109633 \n",
            "Epoch:  1\n",
            "2231/2343.75 loss: 0.38427706589541766 \n",
            "Epoch:  1\n",
            "2232/2343.75 loss: 0.3842683965202623 \n",
            "Epoch:  1\n",
            "2233/2343.75 loss: 0.38430253715123464 \n",
            "Epoch:  1\n",
            "2234/2343.75 loss: 0.38429921160741676 \n",
            "Epoch:  1\n",
            "2235/2343.75 loss: 0.3842714913322695 \n",
            "Epoch:  1\n",
            "2236/2343.75 loss: 0.38422382801811134 \n",
            "Epoch:  1\n",
            "2237/2343.75 loss: 0.38417446482144263 \n",
            "Epoch:  1\n",
            "2238/2343.75 loss: 0.3841908317144133 \n",
            "Epoch:  1\n",
            "2239/2343.75 loss: 0.38418960251313233 \n",
            "Epoch:  1\n",
            "2240/2343.75 loss: 0.384174761609646 \n",
            "Epoch:  1\n",
            "2241/2343.75 loss: 0.38415100502579685 \n",
            "Epoch:  1\n",
            "2242/2343.75 loss: 0.3841207405511577 \n",
            "Epoch:  1\n",
            "2243/2343.75 loss: 0.38408983000114216 \n",
            "Epoch:  1\n",
            "2244/2343.75 loss: 0.38405780471645645 \n",
            "Epoch:  1\n",
            "2245/2343.75 loss: 0.38402416185539745 \n",
            "Epoch:  1\n",
            "2246/2343.75 loss: 0.38401833084433884 \n",
            "Epoch:  1\n",
            "2247/2343.75 loss: 0.3839970682399659 \n",
            "Epoch:  1\n",
            "2248/2343.75 loss: 0.3839283067685013 \n",
            "Epoch:  1\n",
            "2249/2343.75 loss: 0.38387105611960093 \n",
            "Epoch:  1\n",
            "2250/2343.75 loss: 0.3838314726346337 \n",
            "Epoch:  1\n",
            "2251/2343.75 loss: 0.38378961373709447 \n",
            "Epoch:  1\n",
            "2252/2343.75 loss: 0.3837594200228248 \n",
            "Epoch:  1\n",
            "2253/2343.75 loss: 0.38371929871422483 \n",
            "Epoch:  1\n",
            "2254/2343.75 loss: 0.3837314525887601 \n",
            "Epoch:  1\n",
            "2255/2343.75 loss: 0.38368850198726284 \n",
            "Epoch:  1\n",
            "2256/2343.75 loss: 0.3836746422469114 \n",
            "Epoch:  1\n",
            "2257/2343.75 loss: 0.3837041591811328 \n",
            "Epoch:  1\n",
            "2258/2343.75 loss: 0.38371489755397037 \n",
            "Epoch:  1\n",
            "2259/2343.75 loss: 0.38365574051848556 \n",
            "Epoch:  1\n",
            "2260/2343.75 loss: 0.38358841499017965 \n",
            "Epoch:  1\n",
            "2261/2343.75 loss: 0.38359027965921516 \n",
            "Epoch:  1\n",
            "2262/2343.75 loss: 0.3835622784486135 \n",
            "Epoch:  1\n",
            "2263/2343.75 loss: 0.3835390921971718 \n",
            "Epoch:  1\n",
            "2264/2343.75 loss: 0.3836162149445637 \n",
            "Epoch:  1\n",
            "2265/2343.75 loss: 0.383636074941293 \n",
            "Epoch:  1\n",
            "2266/2343.75 loss: 0.3836067310437054 \n",
            "Epoch:  1\n",
            "2267/2343.75 loss: 0.3835538188997505 \n",
            "Epoch:  1\n",
            "2268/2343.75 loss: 0.3835165805559738 \n",
            "Epoch:  1\n",
            "2269/2343.75 loss: 0.3834721320460546 \n",
            "Epoch:  1\n",
            "2270/2343.75 loss: 0.3834576548550004 \n",
            "Epoch:  1\n",
            "2271/2343.75 loss: 0.38349304090477 \n",
            "Epoch:  1\n",
            "2272/2343.75 loss: 0.3834994783747107 \n",
            "Epoch:  1\n",
            "2273/2343.75 loss: 0.3835180284964902 \n",
            "Epoch:  1\n",
            "2274/2343.75 loss: 0.38346437428678787 \n",
            "Epoch:  1\n",
            "2275/2343.75 loss: 0.38340767175090124 \n",
            "Epoch:  1\n",
            "2276/2343.75 loss: 0.3833745268218916 \n",
            "Epoch:  1\n",
            "2277/2343.75 loss: 0.38338381676714706 \n",
            "Epoch:  1\n",
            "2278/2343.75 loss: 0.3834284713141172 \n",
            "Epoch:  1\n",
            "2279/2343.75 loss: 0.383439494741329 \n",
            "Epoch:  1\n",
            "2280/2343.75 loss: 0.3834361714041061 \n",
            "Epoch:  1\n",
            "2281/2343.75 loss: 0.3834016516961412 \n",
            "Epoch:  1\n",
            "2282/2343.75 loss: 0.38338934066548347 \n",
            "Epoch:  1\n",
            "2283/2343.75 loss: 0.3833290891438044 \n",
            "Epoch:  1\n",
            "2284/2343.75 loss: 0.3833093981708948 \n",
            "Epoch:  1\n",
            "2285/2343.75 loss: 0.3832767688109493 \n",
            "Epoch:  1\n",
            "2286/2343.75 loss: 0.38327957933445295 \n",
            "Epoch:  1\n",
            "2287/2343.75 loss: 0.3832813467817394 \n",
            "Epoch:  1\n",
            "2288/2343.75 loss: 0.3831945571473927 \n",
            "Epoch:  1\n",
            "2289/2343.75 loss: 0.38313979546185667 \n",
            "Epoch:  1\n",
            "2290/2343.75 loss: 0.383146166872999 \n",
            "Epoch:  1\n",
            "2291/2343.75 loss: 0.38307857447113663 \n",
            "Epoch:  1\n",
            "2292/2343.75 loss: 0.3831014751712152 \n",
            "Epoch:  1\n",
            "2293/2343.75 loss: 0.3830810862722455 \n",
            "Epoch:  1\n",
            "2294/2343.75 loss: 0.38301489620037327 \n",
            "Epoch:  1\n",
            "2295/2343.75 loss: 0.3830177900332711 \n",
            "Epoch:  1\n",
            "2296/2343.75 loss: 0.38298480238637356 \n",
            "Epoch:  1\n",
            "2297/2343.75 loss: 0.38293225971821393 \n",
            "Epoch:  1\n",
            "2298/2343.75 loss: 0.3828490513039444 \n",
            "Epoch:  1\n",
            "2299/2343.75 loss: 0.38277392550007155 \n",
            "Epoch:  1\n",
            "2300/2343.75 loss: 0.38272713719244267 \n",
            "Epoch:  1\n",
            "2301/2343.75 loss: 0.3826602322836734 \n",
            "Epoch:  1\n",
            "2302/2343.75 loss: 0.3826795711731631 \n",
            "Epoch:  1\n",
            "2303/2343.75 loss: 0.38268490257259047 \n",
            "Epoch:  1\n",
            "2304/2343.75 loss: 0.38267115236880206 \n",
            "Epoch:  1\n",
            "2305/2343.75 loss: 0.38265728315941067 \n",
            "Epoch:  1\n",
            "2306/2343.75 loss: 0.3826347532781833 \n",
            "Epoch:  1\n",
            "2307/2343.75 loss: 0.38265440088779096 \n",
            "Epoch:  1\n",
            "2308/2343.75 loss: 0.38259303966684743 \n",
            "Epoch:  1\n",
            "2309/2343.75 loss: 0.382543714034867 \n",
            "Epoch:  1\n",
            "2310/2343.75 loss: 0.38251134542379994 \n",
            "Epoch:  1\n",
            "2311/2343.75 loss: 0.3824795257093894 \n",
            "Epoch:  1\n",
            "2312/2343.75 loss: 0.382488635476152 \n",
            "Epoch:  1\n",
            "2313/2343.75 loss: 0.382472773236299 \n",
            "Epoch:  1\n",
            "2314/2343.75 loss: 0.3824432417396335 \n",
            "Epoch:  1\n",
            "2315/2343.75 loss: 0.38242830062648586 \n",
            "Epoch:  1\n",
            "2316/2343.75 loss: 0.38237805093446064 \n",
            "Epoch:  1\n",
            "2317/2343.75 loss: 0.38237154816146346 \n",
            "Epoch:  1\n",
            "2318/2343.75 loss: 0.3823555239948953 \n",
            "Epoch:  1\n",
            "2319/2343.75 loss: 0.3823459326385938 \n",
            "Epoch:  1\n",
            "2320/2343.75 loss: 0.3823123787213173 \n",
            "Epoch:  1\n",
            "2321/2343.75 loss: 0.3823160072539923 \n",
            "Epoch:  1\n",
            "2322/2343.75 loss: 0.3822633083901732 \n",
            "Epoch:  1\n",
            "2323/2343.75 loss: 0.3822344803987087 \n",
            "Epoch:  1\n",
            "2324/2343.75 loss: 0.3822058839118609 \n",
            "Epoch:  1\n",
            "2325/2343.75 loss: 0.3821853763733767 \n",
            "Epoch:  1\n",
            "2326/2343.75 loss: 0.3821651962280376 \n",
            "Epoch:  1\n",
            "2327/2343.75 loss: 0.38215167127242405 \n",
            "Epoch:  1\n",
            "2328/2343.75 loss: 0.38215257578036366 \n",
            "Epoch:  1\n",
            "2329/2343.75 loss: 0.38215412688741357 \n",
            "Epoch:  1\n",
            "2330/2343.75 loss: 0.3821302959012249 \n",
            "Epoch:  1\n",
            "2331/2343.75 loss: 0.38205011787714105 \n",
            "Epoch:  1\n",
            "2332/2343.75 loss: 0.38199316921328147 \n",
            "Epoch:  1\n",
            "2333/2343.75 loss: 0.3820219215743033 \n",
            "Epoch:  1\n",
            "2334/2343.75 loss: 0.38202674469294334 \n",
            "Epoch:  1\n",
            "2335/2343.75 loss: 0.38196137629781074 \n",
            "Epoch:  1\n",
            "2336/2343.75 loss: 0.38192748850642694 \n",
            "Epoch:  1\n",
            "2337/2343.75 loss: 0.3819361713079941 \n",
            "Epoch:  1\n",
            "2338/2343.75 loss: 0.3819637110328613 \n",
            "Epoch:  1\n",
            "2339/2343.75 loss: 0.3820095518906402 \n",
            "Epoch:  1\n",
            "2340/2343.75 loss: 0.38195446624378016 \n",
            "Epoch:  1\n",
            "2341/2343.75 loss: 0.3819369548067998 \n",
            "Epoch:  1\n",
            "2342/2343.75 loss: 0.38191861998587145 \n",
            "Epoch:  1\n",
            "2343/2343.75 loss: 0.38186182672914387 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d63f465bcbb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0;31m# 배치에서 데이터 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-50efa32ac9de>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, input_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mhidden_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (bat, len, hid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mhidden_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (bat, hid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m         )\n\u001b[1;32m    874\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    505\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m                 )\n\u001b[1;32m    509\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         )\n\u001b[1;32m    428\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         )\n\u001b[1;32m    365\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 15.90 GiB total capacity; 14.63 GiB already allocated; 19.06 MiB free; 14.97 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Hh_8sUIFIn"
      },
      "source": [
        "##### **Labeling**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx65feCbCVRr",
        "outputId": "786da140-9342-41cd-f279-4ef3be9ed561"
      },
      "source": [
        "print(test_inputs[0].unsqueeze(0))\r\n",
        "print(test_masks[0].unsqueeze(0))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  101,  9670, 89523, 47058,  9607, 61439, 42428, 58303, 48345,   119,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTYyt9G4Cp7W"
      },
      "source": [
        "# 문장 테스트\r\n",
        "def test_sentences(inputs, masks):\r\n",
        "    \r\n",
        "    # 평가모드로 변경\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # 데이터를 GPU에 넣음\r\n",
        "    b_input_ids = inputs.to(device)\r\n",
        "    b_input_mask = masks.to(device)\r\n",
        "\r\n",
        "    # 그래디언트 계산 안함\r\n",
        "    with torch.no_grad():     \r\n",
        "        # Forward 수행\r\n",
        "        outputs = model(b_input_ids, b_input_mask)\r\n",
        "    # 로스 구함\r\n",
        "    logits = outputs[0]\r\n",
        "\r\n",
        "    # CPU로 데이터 이동\r\n",
        "    #logits = logits.detach().cpu().numpy()\r\n",
        "    \r\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt7w5v0ZCsRE"
      },
      "source": [
        "pred_label=[]\r\n",
        "for i in range(len(test_inputs)):\r\n",
        "  input_id = test_inputs[i].unsqueeze(0)\r\n",
        "  input_mask =  test_masks[i].unsqueeze(0)\r\n",
        "  logits = test_sentences(input_id,input_mask)\r\n",
        "  #pred_flat = np.argmax(logits).flatten()\r\n",
        "  _, max_idx = torch.max(logits, dim=-1) # tensor\r\n",
        "  pred_emotion = max_idx.tolist()\r\n",
        "  pred_label.append(pred_emotion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAwvLhpKCvOH",
        "outputId": "7ac71b82-2393-4d20-916f-10cce2bd0835"
      },
      "source": [
        "df = pd.DataFrame(df_test, columns=['Id'])\r\n",
        "df['Predicted'] = pred_label\r\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          Id  Predicted\n",
            "0          0          1\n",
            "1          1          0\n",
            "2          2          0\n",
            "3          3          1\n",
            "4          4          0\n",
            "...      ...        ...\n",
            "11182  11182          1\n",
            "11183  11183          0\n",
            "11184  11184          1\n",
            "11185  11185          1\n",
            "11186  11186          0\n",
            "\n",
            "[11187 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez31waPKCxz_"
      },
      "source": [
        "df.to_csv(\"test_naver_sentiment.csv\", sep=\",\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3riAqyoPwhb"
      },
      "source": [
        "##### **Proposal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWEWuZF31iGY"
      },
      "source": [
        "- There is a class imbalance problem. (Use weighted cross-entropy etc.)\n",
        "\n",
        "- Our model takes a single sentence. (Make it grasp its context as well.)\n",
        "\n",
        "- Our model does not consider speaker information. (Make it consider the info.)\n",
        "\n",
        "- Batch size is set as 1. (Increase the batch size.)"
      ]
    }
  ]
}